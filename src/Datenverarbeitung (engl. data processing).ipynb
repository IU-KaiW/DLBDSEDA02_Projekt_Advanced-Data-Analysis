{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37691fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c60a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1209796400.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install cartopy\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Pyton Package Installation\n",
    "\n",
    "## virtual environment setup\n",
    "%pip install conda\n",
    "\n",
    "## nlp libraries\n",
    "%pip install --upgrade gensim\n",
    "%pip install top2vec\n",
    "\n",
    "## machine learning libraries\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "\n",
    "## visualization libraries\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install pyldavis\n",
    "%pip install cartopy\n",
    "\n",
    "## datahandling libraries\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69929d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in /opt/anaconda3/lib/python3.12/site-packages (0.17.4)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (0.8.41)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (0.5.11)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (5.22.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (1.8.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (5.2.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (4.66.4)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /opt/anaconda3/lib/python3.12/site-packages (from bertopic) (0.42.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=0.4.1->bertopic) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.3.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.10.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/anaconda3/lib/python3.12/site-packages (from umap-learn>=0.5.0->bertopic) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from umap-learn>=0.5.0->bertopic) (0.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (0.27.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (69.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.7.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (0.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping pyldavis-y as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Verwendete Bibliotheken\n",
    "%pip install bertopic\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP-Pipeline\n",
    "import nltk \n",
    "import sklearn\n",
    "import gensim\n",
    "# import spaCy as sp\n",
    "import re\n",
    "\n",
    "# ML\n",
    "import transformers\n",
    "import torch \n",
    "\n",
    "## Embeddings\n",
    "\n",
    "import top2vec as t2v \n",
    "\n",
    "## Visualisierung\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud as wc\n",
    "import pyLDAvis\n",
    "import cartopy as ctp\n",
    "\n",
    "# Sortieren\n",
    "import venv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217597b",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42862b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2026.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: transformers 4.57.5\n",
      "Uninstalling transformers-4.57.5:\n",
      "  Successfully uninstalled transformers-4.57.5\n",
      "Found existing installation: torch 2.9.1\n",
      "Uninstalling torch-2.9.1:\n",
      "  Successfully uninstalled torch-2.9.1\n",
      "Found existing installation: sentencepiece 0.2.1\n",
      "Uninstalling sentencepiece-0.2.1:\n",
      "  Successfully uninstalled sentencepiece-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "%pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
    "%pip uninstall transformers torch sentencepiece -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2d05d",
   "metadata": {},
   "source": [
    "# Datenvorverarbeiten (engl. data pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e167cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  I used to love Comcast. Until all these consta...\n",
      "1  I'm so over Comcast! The worst internet provid...\n",
      "2  If I could give them a negative star or no sta...\n",
      "3  I've had the worst experiences so far since in...\n",
      "4  Check your contract when you sign up for Comca...\n",
      "5  Thank God. I am changing to Dish. They gave me...\n",
      "6  I Have been a long time customer and only have...\n",
      "7  There is a malfunction on the DVR manager whic...\n",
      "8  Charges overwhelming. Comcast service rep was ...\n",
      "9  I have had cable, DISH, and U-verse, etc. in t...\n",
      "(5627, 1)\n"
     ]
    }
   ],
   "source": [
    "# Landen des bereinigten Datensatzes\n",
    "df = pd.read_csv('/Users/kaiweber/Documents/GitHub/DLBDSEDA02_Projekt_Advanced-Data-Analysis/src/complaints_data_cleaned.csv', usecols=[\"text\"])\n",
    "print(df.head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d851caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeitet: 5627 Beschwerden\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f0a8b_row0_col0, #T_f0a8b_row0_col1, #T_f0a8b_row1_col0, #T_f0a8b_row1_col1, #T_f0a8b_row2_col0, #T_f0a8b_row2_col1, #T_f0a8b_row3_col0, #T_f0a8b_row3_col1, #T_f0a8b_row4_col0, #T_f0a8b_row4_col1, #T_f0a8b_row5_col0, #T_f0a8b_row5_col1, #T_f0a8b_row6_col0, #T_f0a8b_row6_col1, #T_f0a8b_row7_col0, #T_f0a8b_row7_col1, #T_f0a8b_row8_col0, #T_f0a8b_row8_col1, #T_f0a8b_row9_col0, #T_f0a8b_row9_col1 {\n",
       "  text-align: left;\n",
       "  width: 1000px;\n",
       "  max-width: 1500px;\n",
       "  font-size: 12px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f0a8b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f0a8b_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_f0a8b_level0_col1\" class=\"col_heading level0 col1\" >cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f0a8b_row0_col0\" class=\"data row0 col0\" >I used to love Comcast. Until all these constant updates. My internet and cable crash a lot at night, and sometimes during the day, some channels don't even work and on demand sometimes don't play either. I wish they will do something about it. Because just a few mins ago, the internet have crashed for about 20 mins for no reason. I'm tired of it and thinking about switching to Wow or something. Please do not get Xfinity.</td>\n",
       "      <td id=\"T_f0a8b_row0_col1\" class=\"data row0 col1\" >['love', 'comcast', 'constant', 'update', 'internet', 'cable', 'crash', 'lot', 'night', 'day', 'channel', 'work', 'demand', 'play', 'wish', 'min', 'ago', 'internet', 'crash', 'min', 'reason', 'tired', 'think', 'switch', 'wow', 'xfinity']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f0a8b_row1_col0\" class=\"data row1 col0\" >I'm so over Comcast! The worst internet provider. I'm taking online classes and multiple times was late with my assignments because of the power interruptions in my area that lead to poor quality internet service. Definitely switching to Verizon. I'd rather pay $10 extra then dealing w/ Comcast and non stopping internet problems.</td>\n",
       "      <td id=\"T_f0a8b_row1_col1\" class=\"data row1 col1\" >['comcast', 'bad', 'internet', 'provider', 'take', 'online', 'class', 'multiple', 'time', 'late', 'assignment', 'power', 'interruption', 'area', 'lead', 'poor', 'quality', 'internet', 'service', 'definitely', 'switch', 'verizon', 'pay', 'extra', 'deal', 'comcast', 'non', 'stop', 'internet', 'problem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f0a8b_row2_col0\" class=\"data row2 col0\" >If I could give them a negative star or no stars on this review I would. I have never worked with any industry with as bad of customer service as Comcast. It is not a matter of money because I make well enough above and beyond to afford their services but they are a legitimate ripoff. I think they are the biggest scam of since the mortgage industry's major meltdown and I hope I move somewhere where Comcast does not exist. The disregard to want to help or do the right thing is honestly astounding. If you have to call, which you do FOR ALL ISSUES - billing, connection/service, adding or removing service, errors, it does not matter you will be transferred minimum of 4 times. Everyone says the same thing and passes the issues to the next person and no one resolves the problem.They offer promotional packages in small timeframes and can never access them again so they then upgrade you without you wishing and change your billing. It has been 5 months and I have been overcharged $40 a month since I started with them. The blatant rudeness that must make you qualified to do this job is the type of quality service that gets you this review. So... Dear Comcast, you suck. Sincerely, a customer who cannot wait to never use your service again.</td>\n",
       "      <td id=\"T_f0a8b_row2_col1\" class=\"data row2 col1\" >['negative', 'star', 'star', 'review', 'work', 'industry', 'bad', 'customer', 'service', 'comcast', 'matter', 'money', 'afford', 'service', 'legitimate', 'ripoff', 'think', 'big', 'scam', 'mortgage', 'industry', 'major', 'meltdown', 'hope', 'comcast', 'exist', 'disregard', 'want', 'help', 'right', 'thing', 'honestly', 'astounding', 'issues', 'billing', 'connection', 'service', 'add', 'remove', 'service', 'error', 'matter', 'transfer', 'minimum', 'time', 'say', 'thing', 'pass', 'issue', 'person', 'resolve', 'problem', 'offer', 'promotional', 'package', 'small', 'timeframe', 'access', 'upgrade', 'wish', 'change', 'billing', 'month', 'overcharge', 'month', 'start', 'blatant', 'rudeness', 'qualified', 'job', 'type', 'quality', 'service', 'get', 'review', 'dear', 'comcast', 'suck', 'sincerely', 'customer', 'wait', 'use', 'service']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f0a8b_row3_col0\" class=\"data row3 col0\" >I've had the worst experiences so far since install on 10/4/16. Nothing but problems. Two no shows on scheduled service appointments, extreme difficulty in adding boxes to the second floor. What is so difficult about adding boxes to an existing account? No thank you, I'm not starting a second account for the second floor of the same house! A separate bundle package? All I wanted was just to add a few boxes. Apparently this is not possible. Well then, I guess it's not possible to remain a customer!</td>\n",
       "      <td id=\"T_f0a8b_row3_col1\" class=\"data row3 col1\" >['bad', 'experience', 'far', 'install', 'problem', 'show', 'schedule', 'service', 'appointment', 'extreme', 'difficulty', 'add', 'box', 'floor', 'difficult', 'add', 'box', 'exist', 'account', 'thank', 'start', 'account', 'floor', 'house', 'separate', 'bundle', 'package', 'want', 'add', 'box', 'apparently', 'possible', 'guess', 'possible', 'remain', 'customer']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f0a8b_row4_col0\" class=\"data row4 col0\" >Check your contract when you sign up for Comcast as their advertised offers do not match the contract they issue. I signed up for $49.99 150Mbps internet for 2 years, however my contract has $19.99 for 25Mbps internet for 2 years. They say there is an add on in place for $30 which boost it to Blast! Pro, however this isn't part of the contract, which means that Comcast can increase the price whenever they want within the 2 years. This means I haven't received the advertised rate. Comcast has so far refused to issue corrected contract, or issue in writing that the $30 will remain at that price for 2 years. I just have to trust them. So watch out, Comcast is doing the usual illegal practices, I'm guessing to catch people out and hope they don't notice and end up paying more than they should.</td>\n",
       "      <td id=\"T_f0a8b_row4_col1\" class=\"data row4 col1\" >['check', 'contract', 'sign', 'comcast', 'advertised', 'offer', 'match', 'contract', 'issue', 'sign', '150mbps', 'internet', 'year', 'contract', '25mbps', 'internet', 'year', 'add', 'place', 'boost', 'blast', 'pro', 'contract', 'mean', 'comcast', 'increase', 'price', 'want', 'year', 'mean', 'receive', 'advertised', 'rate', 'comcast', 'far', 'refuse', 'issue', 'correct', 'contract', 'issue', 'write', 'remain', 'price', 'year', 'trust', 'watch', 'comcast', 'usual', 'illegal', 'practice', 'guess', 'catch', 'people', 'hope', 'notice', 'end', 'pay']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f0a8b_row5_col0\" class=\"data row5 col0\" >Thank God. I am changing to Dish. They gave me awesome pricing and super people to deal with. You can actually understand what they are saying. I'm so excited to finally be able to return this equipment although still haven't received the home security yet as promised 4 times. Go to h*ll Comcast. You have made me miserable and cause me to miss many hours of work with your promises.</td>\n",
       "      <td id=\"T_f0a8b_row5_col1\" class=\"data row5 col1\" >['thank', 'god', 'change', 'dish', 'give', 'awesome', 'pricing', 'super', 'people', 'deal', 'actually', 'understand', 'say', 'excited', 'finally', 'able', 'return', 'equipment', 'receive', 'home', 'security', 'promise', 'time', 'h*ll', 'comcast', 'miserable', 'cause', 'miss', 'hour', 'work', 'promise']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f0a8b_row6_col0\" class=\"data row6 col0\" >I Have been a long time customer and only have Xfinity as my ISP for a while now. While I was in the local Walmart on November 4, 2016, there were customer representatives from Xfinity running promotions for and in the Salt Lake City area. Spoke with a representative and was able to get and signed a contract for Pro Blast at $50.00 a month with no contract or early termination fees. I received an email from Xfinity stating the changes that would be made to my account. It stated that not only would it be under contract for 24 months but there would be early termination fees. This is not what I had originally signed up for and it specifically states this on the contract that I signed. Contacted Xfinity customer service and was told since they cannot see the contract over the phone that I would need to go to Xfinity store in person. Went to Xfinity store on November 8, 2016 and was told that it would be under contract and there was no way around it. Because of this I have cancelled the upgrade and went back to my original plan. It's plain and simple. When a contract is signed it should be honored for what is stated on it. Xfinity is dishonest and not trustworthy. Therefore I will be looking and changing my ISP as soon as possible to another company. Xfinity does not deserve a paycheck from me or anyone else that I know.</td>\n",
       "      <td id=\"T_f0a8b_row6_col1\" class=\"data row6 col1\" >['long', 'time', 'customer', 'xfinity', 'isp', 'local', 'walmart', 'november', 'customer', 'representative', 'xfinity', 'run', 'promotion', 'salt', 'lake', 'city', 'area', 'speak', 'representative', 'able', 'sign', 'contract', 'pro', 'blast', 'month', 'contract', 'early', 'termination', 'fee', 'receive', 'email', 'xfinity', 'state', 'change', 'account', 'state', 'contract', 'month', 'early', 'termination', 'fee', 'originally', 'sign', 'specifically', 'state', 'contract', 'sign', 'contacted', 'xfinity', 'customer', 'service', 'tell', 'contract', 'phone', 'need', 'xfinity', 'store', 'person', 'go', 'xfinity', 'store', 'november', 'tell', 'contract', 'way', 'cancel', 'upgrade', 'go', 'original', 'plan', 'plain', 'simple', 'contract', 'sign', 'honor', 'state', 'xfinity', 'dishonest', 'trustworthy', 'look', 'change', 'isp', 'soon', 'possible', 'company', 'xfinity', 'deserve', 'paycheck', 'know']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f0a8b_row7_col0\" class=\"data row7 col0\" >There is a malfunction on the DVR manager which is preventing us from adding more recordings. Customer service is fairly certain that the problem is from the signal from their system to ours, but protocol demands that they access our home before investigating that option. Since we work, that cannot be done until next Saturday. Customer service tech agreed that this seems illogical since logic would dictate that one would investigate the most probably malfunction first, but insists they must follow protocol. This is extremely frustrating. After 35 years as a customer of Comcast & their predecessors, I am investigating alternatives.</td>\n",
       "      <td id=\"T_f0a8b_row7_col1\" class=\"data row7 col1\" >['malfunction', 'dvr', 'manager', 'prevent', 'add', 'recording', 'customer', 'service', 'fairly', 'certain', 'problem', 'signal', 'system', 'protocol', 'demand', 'access', 'home', 'investigate', 'option', 'work', 'saturday', 'customer', 'service', 'tech', 'agree', 'illogical', 'logic', 'dictate', 'investigate', 'probably', 'malfunction', 'insist', 'follow', 'protocol', 'extremely', 'frustrating', 'year', 'customer', 'comcast', 'predecessor', 'investigate', 'alternative']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f0a8b_row8_col0\" class=\"data row8 col0\" >Charges overwhelming. Comcast service rep was so ignorant and rude when I call to resolve my issue with my bill. I emailed Tom ** his rep was rude to me. None of the representative was helpful. They all just pass me on to other people. I am cutting my service with Comcast.</td>\n",
       "      <td id=\"T_f0a8b_row8_col1\" class=\"data row8 col1\" >['charge', 'overwhelming', 'comcast', 'service', 'rep', 'ignorant', 'rude', 'resolve', 'issue', 'bill', 'email', 'tom', 'rep', 'rude', 'representative', 'helpful', 'pass', 'people', 'cut', 'service', 'comcast']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f0a8b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f0a8b_row9_col0\" class=\"data row9 col0\" >I have had cable, DISH, and U-verse, etc. in the past. All are eh... but you know what? Comcast takes the cake. I have never been driven to take time out of my day just to gripe online for all to see. But consumers, stay away! So my first terrible experience with Comcast is that they took 5 phones and 2 months to come out and bury the lines they had to lay in my front yard to get the cable needed into my house. Finally got someone when my special needs neighbor tripped and fell!Now 3 months into my contract, I have had my internet, phone, and TV go out for HOURS at a time. I would spend 3 hours on with a tech when it will come back up after the technician resets the router manually for the 3rd or 4th time. I have had it, I work from home occasionally and this is a huge inconvenience! The hardware is faulty, I understand that sometimes you get a lemon... but 3 months! 3 months! I have had it. Worst company ever. Crappy equipment and terrible customer service, and worse is the technicians they hire! Not a clue! Comcast should send a technician out here to switch out this equipment before I set a bonfire to it.</td>\n",
       "      <td id=\"T_f0a8b_row9_col1\" class=\"data row9 col1\" >['cable', 'dish', 'verse', 'etc', 'past', 'know', 'comcast', 'take', 'cake', 'drive', 'time', 'day', 'gripe', 'online', 'consumer', 'stay', 'away', 'terrible', 'experience', 'comcast', 'take', 'phone', 'month', 'come', 'bury', 'line', 'lay', 'yard', 'cable', 'need', 'house', 'finally', 'get', 'special', 'need', 'neighbor', 'trip', 'fell!now', 'month', 'contract', 'internet', 'phone', 'hour', 'time', 'spend', 'hour', 'tech', 'come', 'technician', 'reset', 'router', 'manually', 'time', 'work', 'home', 'occasionally', 'huge', 'inconvenience', 'hardware', 'faulty', 'understand', 'lemon', 'month', 'month', 'bad', 'company', 'crappy', 'equipment', 'terrible', 'customer', 'service', 'bad', 'technician', 'hire', 'clue', 'comcast', 'send', 'technician', 'switch', 'equipment', 'set', 'bonfire']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x375880680>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Textbereinigung (engl. text cleaning)\n",
    "##  Rauschentfernung (engl. noise reduction)\n",
    "### Wortbereinigung (engl. word cleaning)\n",
    "\n",
    "# Datensatz laden (Rohtext)\n",
    "df = pd.read_csv('/Users/kaiweber/Documents/GitHub/DLBDSEDA02_Projekt_Advanced-Data-Analysis/src/complaints_data_cleaned.csv', \n",
    "                 usecols=[\"text\"],nrows=5627)                                # Zeilenbegrenzung\n",
    "\n",
    "# Linguistische Verarbeitung (engl. linguistic processing)\n",
    "## Rechtschreibfehlerkorrektur (engl. spelling correction)\n",
    "\n",
    "## Textbereinigung, Tokenisierung, Lemmatisierung\n",
    "\n",
    "## spaCy Pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")                                          # englisches Modell (small version)\n",
    "\n",
    "## Textbereinigung, Tokenisierung, Grundformreduktion (Lemmatisierung)\n",
    "df[\"cleaned\"] = [[token.lemma_.lower()\n",
    "                  for token in doc\n",
    "                  # Filterblock      \n",
    "                  if (not token.is_stop and                                 # Standard:     Stopwort-Filter\n",
    "                      not token.is_punct and                                # Standard:     Satzzeichen-Filter\n",
    "                      not token.like_num and                                # Standard:     Nummern-Filter (einfache Zahlen)\n",
    "                      len(token.text) > 2 and                               # Individuell:  Wörter mit min. 2 Zeichen\n",
    "                      not any(char in token.text for char in ':/-–—') and   # Individuell:  Filter für Datums-/Zeit-Token\n",
    "                      token.is_ascii and                                    # Individuell:  Emoijs-Filter (True = AN / False = Nur Emoijs)\n",
    "                      token.pos_ != \"PRON\" and                              # Individuell:  Pronomen-Filter\n",
    "                      token.text.lower() not in [\"meh\", \"ugh\"])]            # Individuell:  Wortfilter\n",
    "                 for doc in nlp.pipe(df[\"text\"], batch_size=50)]            # Batch-Verarbeitung (50 Texte parallel)\n",
    "\n",
    "print(f\"Verarbeitet: {len(df)} Beschwerden\")\n",
    "\n",
    "# Ausgabe des Prozesses\n",
    "df.head(10)[[\"text\", \"cleaned\"]].style.set_properties(\n",
    "    **{'text-align': 'left', 'width': '1000px', 'max-width': '1500px', 'font-size': '12px'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526427e1",
   "metadata": {},
   "source": [
    "# Vokabularerstellung (engl. vocabulary construction)\n",
    "Mapping der gefilterten Wörter (Token) zu IDs.\n",
    "\n",
    "Wortfrequenzschwellen (ist in SpaCy Pipeline)\n",
    "ggf. nur Nomen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0ed37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vokabulargröße: 11975 Token (Wörter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>comcast</td>\n",
       "      <td>15282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9498</th>\n",
       "      <td>service</td>\n",
       "      <td>15181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>tell</td>\n",
       "      <td>7876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>call</td>\n",
       "      <td>7790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>time</td>\n",
       "      <td>6242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>customer</td>\n",
       "      <td>5736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>bill</td>\n",
       "      <td>5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>internet</td>\n",
       "      <td>5257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9309</th>\n",
       "      <td>say</td>\n",
       "      <td>5219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>month</td>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>phone</td>\n",
       "      <td>4893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>pay</td>\n",
       "      <td>4778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>cable</td>\n",
       "      <td>4710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>day</td>\n",
       "      <td>4114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>come</td>\n",
       "      <td>3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658</th>\n",
       "      <td>go</td>\n",
       "      <td>3639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>work</td>\n",
       "      <td>3637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>charge</td>\n",
       "      <td>3540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>problem</td>\n",
       "      <td>3379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>get</td>\n",
       "      <td>3197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>box</td>\n",
       "      <td>2968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>account</td>\n",
       "      <td>2805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>ask</td>\n",
       "      <td>2745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7128</th>\n",
       "      <td>new</td>\n",
       "      <td>2634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>year</td>\n",
       "      <td>2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>hour</td>\n",
       "      <td>2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8577</th>\n",
       "      <td>receive</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>company</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10986</th>\n",
       "      <td>try</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7081</th>\n",
       "      <td>need</td>\n",
       "      <td>2317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10521</th>\n",
       "      <td>tech</td>\n",
       "      <td>2306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9946</th>\n",
       "      <td>speak</td>\n",
       "      <td>2255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11606</th>\n",
       "      <td>want</td>\n",
       "      <td>2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9456</th>\n",
       "      <td>send</td>\n",
       "      <td>2138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10533</th>\n",
       "      <td>technician</td>\n",
       "      <td>2134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>home</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>issue</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>know</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>number</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>week</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>supervisor</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8813</th>\n",
       "      <td>rep</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>modem</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>credit</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11584</th>\n",
       "      <td>wait</td>\n",
       "      <td>1717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>cancel</td>\n",
       "      <td>1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>equipment</td>\n",
       "      <td>1651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>fix</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>like</td>\n",
       "      <td>1587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10463</th>\n",
       "      <td>take</td>\n",
       "      <td>1555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  frequency\n",
       "2176      comcast      15282\n",
       "9498      service      15181\n",
       "10576        tell       7876\n",
       "1735         call       7790\n",
       "10755        time       6242\n",
       "2731     customer       5736\n",
       "1348         bill       5625\n",
       "5667     internet       5257\n",
       "9309          say       5219\n",
       "6905        month       4962\n",
       "7836        phone       4893\n",
       "7697          pay       4778\n",
       "1713        cable       4710\n",
       "2805          day       4114\n",
       "2190         come       3832\n",
       "4658           go       3639\n",
       "11833        work       3637\n",
       "1922       charge       3540\n",
       "8209      problem       3379\n",
       "4607          get       3197\n",
       "1513          box       2968\n",
       "428       account       2805\n",
       "973           ask       2745\n",
       "7128          new       2634\n",
       "11928        year       2590\n",
       "5165         hour       2538\n",
       "8577      receive       2513\n",
       "2239      company       2492\n",
       "10986         try       2378\n",
       "7081         need       2317\n",
       "10521        tech       2306\n",
       "9946        speak       2255\n",
       "11606        want       2153\n",
       "9456         send       2138\n",
       "10533  technician       2134\n",
       "5095         home       2087\n",
       "5760        issue       2068\n",
       "6010         know       2028\n",
       "7249       number       1984\n",
       "11673        week       1982\n",
       "10340  supervisor       1931\n",
       "8813          rep       1928\n",
       "6858        modem       1892\n",
       "2634       credit       1832\n",
       "11584        wait       1717\n",
       "1763       cancel       1710\n",
       "3808    equipment       1651\n",
       "4259          fix       1600\n",
       "6245         like       1587\n",
       "10463        take       1555"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vokabularerstellung (engl. vocabulary construction)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Umwandung des Dataframes in Liste (Vectorizer braucht Strings)\n",
    "df[\"text_cleaned\"] = [\" \".join(tokens) for tokens in df[\"cleaned\"]]\n",
    "\n",
    "vectorizer = CountVectorizer()                                                  # Vectorizer mit fit_transform: Vokabular + Matrix in einem\n",
    "X = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "## Vokabularerstellung\n",
    "vocabulary = vectorizer.get_feature_names_out()                                 # Vokabular extrahieren\n",
    "print(f\"Vokabulargröße: {len(vocabulary)} Token (Wörter)\")                      # Ausgabe der Vokabulargrö0e\n",
    "\n",
    "word_counts = X.sum(axis=0).A1                                                  # Häufigkeiten (Summe pro Spalte)\n",
    "vocab_df = pd.DataFrame({\n",
    "    'word': vocabulary,\n",
    "    'frequency': word_counts\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "vocab_df.head(50)                                                               # Erste Spalte ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4e67c",
   "metadata": {},
   "source": [
    "# Textbereinigung (engl. text cleaning)\n",
    "\n",
    "##  Rauschentfernung (engl. noise reduction)\n",
    "### Wortbereinigung (engl. word cleaning)\n",
    "#### Stoppworte (engl. stopwords)\n",
    "- individuelle Stopworte (Beschwerde)\n",
    "### Zeichenbereinigung (engl. character cleaning)\n",
    "#### Satzzeichen (engl. punctuation marks)\n",
    "#### Leerzeichen (engl. white space)\n",
    "#### Sonderzeichen (engl. special character)\n",
    "### Nummernbereinigung (engl. numbers cleaning)\n",
    "#### Nummern (engl. removing numbers)\n",
    "\n",
    "\n",
    "##  Standardisierung (engl. standardisation)\n",
    "### Normalisierung (engl. normalisation)\n",
    "#### Kasusumwandlung (engl. case conversion)\n",
    "#### Formatnormalisierungen (engl. format normalisations)\n",
    "### Rechtschreibfehlerkorrektur (engl. spelling correction)\n",
    "auf eine Rechtschreikorrektur wird verzichtet, da contextualSpellcheck sicht nicht installieren ließ (Mac M1), T5 zu lange dauerte 10min + und PyEnchant schlechte Korrekturen lieferte\n",
    "\n",
    "# lexikalische Verarbeitung (engl. lexical processing)\n",
    "##  Tokenisierung (engl. tokenization)\n",
    "##  Grundformreduktion (engl. inflection reduction)\n",
    "### Lemmatisierung (engl. lemmatization)\n",
    "NLTK(WordNetLemmatizer) / SpaCy\n",
    "##  Vokabularerstellung/Wortschatzaufbau (engl. vocabulary construction)\n",
    "Scikit-learn (CountVectorizer)\n",
    "fit_transform lernt das Vokabular / dict und gibt eine Matrix zurück\n",
    "# syntaktische Verarbeitung (engl. syntactic processing)\n",
    "# semantische Verarbeitung (engl. context processing)\n",
    "## Semantisches Parsen (engl. semantic parsing)\n",
    "### Eigennamenerkennung (engl. Named Entity Recognition - NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9b8c5",
   "metadata": {},
   "source": [
    "# syntaktische Verarbeitung (engl. syntactic processing)\n",
    "auf ein syntaktische Parsen wrd zunächst verzichtet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038eb8e",
   "metadata": {},
   "source": [
    "# semantische Verarbeitung (engl. context processing)\n",
    "ggf. eine NER durchführen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c217",
   "metadata": {},
   "source": [
    "# Vektorisierung (engl. vectorization)\n",
    "## häufigkeitsbasierter Vektor (engl. frequency vectors)\n",
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Form: (5627, 11975)\n",
      "\n",
      " Beschwerde  ID-Vokabular Token (Wort)  TF-IDF Score\n",
      "          1          1807        crash      0.566538\n",
      "          1           352          min      0.379436\n",
      "          1           793     constant      0.223336\n",
      "          1           713          wow      0.221154\n",
      "          1           762         love      0.220104\n",
      "          1           582        tired      0.206122\n",
      "          1           451       update      0.197124\n",
      "          1           403         wish      0.186337\n",
      "          1           270        night      0.173111\n",
      "          1           291          lot      0.169078\n",
      "          1           232       demand      0.168771\n",
      "          1           187         play      0.159005\n",
      "          1           200       reason      0.155522\n",
      "          1             7     internet      0.151246\n",
      "          1           112      xfinity      0.142833\n",
      "          1           119       switch      0.134631\n",
      "          1            55      channel      0.133232\n",
      "          1           115          ago      0.131436\n",
      "          1            77        think      0.121827\n",
      "          1            16         work      0.089130\n",
      "          1            13          day      0.083317\n",
      "          1            12        cable      0.081176\n",
      "          1             0      comcast      0.050103\n",
      "          2          2295   assignment      0.338515\n",
      "          2           625 interruption      0.262047\n",
      "          2             7     internet      0.261377\n",
      "          2           820         lead      0.259958\n",
      "          2           729   definitely      0.251250\n",
      "          2           499        class      0.233766\n",
      "          2           465        power      0.230693\n"
     ]
    }
   ],
   "source": [
    "# Vektorisierung (engl. vectorization)\n",
    "## häufigkeitsbasierter Vektor (engl. frequency vectors)\n",
    "### TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Vokabular im korrekten Format: {wort: index}\n",
    "vocabulary = {word: idx for idx, word in enumerate(vocab_df['word'])}\n",
    "\n",
    "# Inverse Mapping: {index: wort}\n",
    "idx_to_word = {idx: word for word, idx in vocabulary.items()}\n",
    "\n",
    "# TF-IDF mit dem erstellten Vokabular\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Texte für TF-IDF vorbereiten (mit den bereinigten Token)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "print(f\"TF-IDF Matrix Form: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Sparse-Matrix\n",
    "result_data = []\n",
    "cx = tfidf_matrix.tocoo()\n",
    "for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "    result_data.append({\n",
    "        'Beschwerde'    : i+1,                                                                                      # Beschwerdeindex angepasst\n",
    "        'ID-Vokabular'  : j,\n",
    "        'Token (Wort)'  : idx_to_word[j], \n",
    "        'TF-IDF Score'  : v\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(result_data).sort_values(['Beschwerde', 'TF-IDF Score'], ascending=[True, False])\n",
    "print(result_df.head(30).to_string(index=False))                                                                # pandas-Spalte ausblenden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55766c9",
   "metadata": {},
   "source": [
    "## Worteinbettung (engl. word embeddings)\n",
    "### Glove (global Vectors for word vectorization)\n",
    "GloVe steht für „Global Vectors for word vectorization“ (Globale Vektoren für die Wortdarstellung). Es handelt sich hierbei um eine weitere Vektorisierungsmethode, die häufig im NLP verwendet wird, um semantische und syntaktische Informationen in einem Vektorraum darzustellen. Während Word2Vec ein prädiktives Modell ist, ist GloVe ein unüberwachter Ansatz, der auf der Anzahl der Wörter basiert. Sie wurde entwickelt, weil Pennington et al. (2014) zu dem Schluss kamen, dass der Skip-Gram-Ansatz in Word2Vec die statistischen Informationen in Bezug auf das gemeinsame Vorkommen (Kookkurenz) von Wörtern nicht vollständig berücksichtigt. Deshalb haben sie den Skip-Gram-Ansatz mit den Vorteilen der Matrixfaktorisierung kombiniert. Das GloVe-Modell verwendet eine Kookkurenzmatrix, die Informationen über den Wortkontext enthält. Es hat sich gezeigt, dass das entwickelte Modell verwandte Modelle übertrifft, insbesondere bei der Erkennung von benannten Entitäten und Ähnlichkeitsaufgaben (Pennington et al., 2014).\n",
    "\n",
    "(iu. DLBDSEAIS01_D , 2023, p. 70)\n",
    "\n",
    "Bias im Datensatz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Word2Vec-Training...\n",
      "✓ Word2Vec-Modell trainiert!\n",
      "  Vokabular-Größe: 7085\n",
      "  Vektor-Dimension: 200\n",
      "✓ Embedding-Matrix erstellt: (11975, 200)\n",
      "✓ Dokumentenvektoren erstellt: (5627, 200)\n",
      "\n",
      "--- Ähnliche Wörter (Word2Vec) ---\n",
      "\n",
      "Ähnlich zu 'internet':\n",
      "  cutter: 0.615\n",
      "  wifi: 0.613\n",
      "  dsl: 0.581\n",
      "  blast: 0.581\n",
      "  landline: 0.578\n",
      "  1yr: 0.572\n",
      "  6mbs: 0.570\n",
      "  mbps: 0.557\n",
      "  advertised: 0.538\n",
      "  100mbps: 0.528\n",
      "\n",
      "Ähnlich zu 'comcast':\n",
      "  service: 0.464\n",
      "  arlington: 0.462\n",
      "  company: 0.455\n",
      "  shreveport: 0.453\n",
      "  inclination: 0.452\n",
      "  satisfied: 0.451\n",
      "  lawyers: 0.448\n",
      "  fraudulently: 0.443\n",
      "  24h: 0.439\n",
      "  wth: 0.428\n",
      "\n",
      "Ähnlich zu 'time':\n",
      "  hour: 0.682\n",
      "  countless: 0.633\n",
      "  dozen: 0.623\n",
      "  hrs: 0.587\n",
      "  numerous: 0.577\n",
      "  phone: 0.575\n",
      "  half: 0.572\n",
      "  multiple: 0.562\n",
      "  occasion: 0.557\n",
      "  course: 0.550\n"
     ]
    }
   ],
   "source": [
    "## Worteinbettung mit Word2Vec (engl. word embeddings)\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"Starte Word2Vec-Training...\")\n",
    "\n",
    "# 1. Vorbereitung: Tokenisierte Daten aus df[\"cleaned\"]\n",
    "sentences = df[\"cleaned\"].tolist()  # Liste von Listen mit Token\n",
    "\n",
    "# 2. Word2Vec-Modell trainieren\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=200,           # Dimensionalität der Wort-Vektoren\n",
    "    window=5,                  # Kontextfenster (±5 Wörter)\n",
    "    min_count=2,               # Wörter die weniger als 2x vorkommen ignorieren\n",
    "    workers=4,                 # Parallel-Verarbeitung\n",
    "    sg=0,                      # 0=CBOW, 1=Skip-gram\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"✓ Word2Vec-Modell trainiert!\")\n",
    "print(f\"  Vokabular-Größe: {len(w2v_model.wv)}\")\n",
    "print(f\"  Vektor-Dimension: {w2v_model.vector_size}\")\n",
    "\n",
    "# 3. Embedding-Matrix für das gesamte Vokabular erstellen\n",
    "embedding_matrix = np.zeros((len(vocab_df), 200))\n",
    "\n",
    "for idx, word in enumerate(vocab_df['word'].values):\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    else:\n",
    "        # Wort nicht im Modell: klein random Vektor\n",
    "        embedding_matrix[idx] = np.random.randn(200) * 0.01\n",
    "\n",
    "print(f\"✓ Embedding-Matrix erstellt: {embedding_matrix.shape}\")\n",
    "\n",
    "# 4. Dokumentenvektoren erstellen (Durchschnitt aller Wortvektoren)\n",
    "def get_document_vector(tokens, model, vector_size=200):\n",
    "    \"\"\"Berechnet den Durchschnittvektor eines Dokuments\"\"\"\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Dokumentenvektoren für alle Beschwerden\n",
    "document_embeddings = np.array([\n",
    "    get_document_vector(tokens, w2v_model, 200) \n",
    "    for tokens in df[\"cleaned\"]\n",
    "])\n",
    "\n",
    "print(f\"✓ Dokumentenvektoren erstellt: {document_embeddings.shape}\")\n",
    "\n",
    "# 5. Beispiele: Ähnliche Wörter finden\n",
    "print(\"\\n--- Ähnliche Wörter (Word2Vec) ---\")\n",
    "test_words = ['internet', 'comcast', 'time']\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        similar = w2v_model.wv.most_similar(word, topn=10)\n",
    "        print(f\"\\nÄhnlich zu '{word}':\")\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"  {similar_word}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{word}' nicht im Modell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5834439",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be303bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LATENT DIRICHLET ALLOCATION (LDA)\n",
      "================================================================================\n",
      "\n",
      "1. Überprüfe Datenverfügbarkeit...\n",
      "   ✓ 'cleaned' Spalte gefunden\n",
      "   ✓ 5627 Dokumente geladen\n",
      "   - Nicht-leere Dokumente: 5627\n",
      "   - Leere Dokumente: 0\n",
      "   - Beispiel-Tokens (erste 10): ['love', 'comcast', 'constant', 'update', 'internet', 'cable', 'crash', 'lot', 'night', 'day']\n",
      "   → Nach Filterung: 5627 nicht-leere Dokumente\n",
      "\n",
      "2. Erstelle Dictionary aus Token-Liste...\n",
      "   Wörter VOR Filter: 12628\n",
      "\n",
      "   Top 10 häufigste Wörter:\n",
      "     'comcast': 15215x\n",
      "     'service': 15169x\n",
      "     'tell': 7876x\n",
      "     'call': 7783x\n",
      "     'time': 6235x\n",
      "     'customer': 5732x\n",
      "     'bill': 5619x\n",
      "     'internet': 5247x\n",
      "     'say': 5219x\n",
      "     'month': 4956x\n",
      "\n",
      "   Wende MINIMALE Filter an:\n",
      "   - no_below=1 (Wort kommt mindestens 1x vor)\n",
      "   - no_above=0.99 (Wort kommt in max. 99% der Dokumente vor)\n",
      "   Wörter NACH Filter: 12628 (von 12628)\n",
      "\n",
      "3. Erstelle Corpus (Bag-of-Words)...\n",
      "   Corpus Größe: 5627 Dokumente\n",
      "   Nicht-leere Dokumente im Corpus: 5627\n",
      "   Durchschn. Terme pro Dokument: 56.7\n",
      "   Min/Max Terme pro Dokument: 3/333\n",
      "\n",
      "   Sample Corpus-Dokumente (erste 3):\n",
      "     Doc 0: 23 Terme - [('ago', 1), ('cable', 1), ('channel', 1), ('comcast', 1), ('constant', 1)]\n",
      "     Doc 1: 27 Terme - [('comcast', 2), ('internet', 3), ('switch', 1), ('area', 1), ('assignment', 1)]\n",
      "     Doc 2: 68 Terme - [('comcast', 3), ('think', 1), ('wish', 1), ('work', 1), ('bad', 1)]\n",
      "\n",
      "4. Trainiere LDA Modell...\n",
      "   Trainings-Daten: 5627 Dokumente mit 12628 Wörtern im Dictionary\n",
      "   Parameter:\n",
      "   - Topics: 10\n",
      "   - Passes: 20\n",
      "   - Iterations: 400\n",
      "   (Dies kann 5-15 Minuten dauern...)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ LDA MODELL ERFOLGREICH TRAINIERT!\n",
      "================================================================================\n",
      "\n",
      "Modell-Statistiken:\n",
      "  - Dictionary Größe: 12628 Wörter\n",
      "  - Corpus Größe: 5627 Dokumente\n",
      "  - Anzahl Topics: 10\n",
      "\n",
      "================================================================================\n",
      "TOP WORDS PER TOPIC (mit Gewichtungen)\n",
      "================================================================================\n",
      "\n",
      "Topic 0:\n",
      "  service                   0.0340\n",
      "  comcast                   0.0301\n",
      "  company                   0.0254\n",
      "  time                      0.0202\n",
      "  bad                       0.0183\n",
      "  customer                  0.0167\n",
      "  business                  0.0139\n",
      "  area                      0.0111\n",
      "  internet                  0.0107\n",
      "  like                      0.0106\n",
      "\n",
      "Topic 1:\n",
      "  call                      0.0369\n",
      "  comcast                   0.0266\n",
      "  tell                      0.0263\n",
      "  service                   0.0261\n",
      "  come                      0.0219\n",
      "  day                       0.0212\n",
      "  time                      0.0200\n",
      "  phone                     0.0198\n",
      "  say                       0.0184\n",
      "  work                      0.0146\n",
      "\n",
      "Topic 2:\n",
      "  box                       0.0328\n",
      "  comcast                   0.0270\n",
      "  channel                   0.0204\n",
      "  cable                     0.0194\n",
      "  pay                       0.0150\n",
      "  tell                      0.0137\n",
      "  customer                  0.0137\n",
      "  service                   0.0134\n",
      "  go                        0.0106\n",
      "  like                      0.0097\n",
      "\n",
      "Topic 3:\n",
      "  service                   0.0353\n",
      "  comcast                   0.0254\n",
      "  customer                  0.0237\n",
      "  time                      0.0212\n",
      "  phone                     0.0195\n",
      "  problem                   0.0162\n",
      "  hour                      0.0146\n",
      "  email                     0.0139\n",
      "  issue                     0.0134\n",
      "  try                       0.0120\n",
      "\n",
      "Topic 4:\n",
      "  service                   0.0414\n",
      "  month                     0.0409\n",
      "  internet                  0.0310\n",
      "  comcast                   0.0297\n",
      "  bill                      0.0259\n",
      "  pay                       0.0206\n",
      "  year                      0.0185\n",
      "  customer                  0.0173\n",
      "  charge                    0.0163\n",
      "  price                     0.0162\n",
      "\n",
      "Topic 5:\n",
      "  comcast                   0.0554\n",
      "  service                   0.0550\n",
      "  cable                     0.0248\n",
      "  problem                   0.0231\n",
      "  internet                  0.0175\n",
      "  technician                0.0167\n",
      "  work                      0.0108\n",
      "  home                      0.0089\n",
      "  time                      0.0083\n",
      "  customer                  0.0082\n",
      "\n",
      "Topic 6:\n",
      "  vcr                       0.0170\n",
      "  parent                    0.0134\n",
      "  winter                    0.0088\n",
      "  dark                      0.0086\n",
      "  practice                  0.0082\n",
      "  locked                    0.0082\n",
      "  outraged                  0.0080\n",
      "  i'm                       0.0077\n",
      "  inquiry                   0.0074\n",
      "  gerald                    0.0073\n",
      "\n",
      "Topic 7:\n",
      "  say                       0.0292\n",
      "  modem                     0.0268\n",
      "  tech                      0.0225\n",
      "  tell                      0.0221\n",
      "  comcast                   0.0198\n",
      "  come                      0.0186\n",
      "  call                      0.0180\n",
      "  internet                  0.0166\n",
      "  work                      0.0166\n",
      "  box                       0.0164\n",
      "\n",
      "Topic 8:\n",
      "  november                  0.0579\n",
      "  a.m.                      0.0362\n",
      "  house                     0.0328\n",
      "  rewire                    0.0308\n",
      "  p.m.                      0.0302\n",
      "  signal                    0.0215\n",
      "  say                       0.0209\n",
      "  voice                     0.0202\n",
      "  goodlow                   0.0189\n",
      "  supervisor                0.0177\n",
      "\n",
      "Topic 9:\n",
      "  comcast                   0.0396\n",
      "  bill                      0.0391\n",
      "  service                   0.0383\n",
      "  tell                      0.0249\n",
      "  account                   0.0225\n",
      "  pay                       0.0213\n",
      "  call                      0.0202\n",
      "  credit                    0.0175\n",
      "  charge                    0.0168\n",
      "  receive                   0.0152\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LATENT DIRICHLET ALLOCATION (LDA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Überprüfe Datenverfügbarkeit...\")\n",
    "\n",
    "# Prüfe, ob 'cleaned' Spalte existiert\n",
    "if \"cleaned\" in df.columns:\n",
    "    print(\"   ✓ 'cleaned' Spalte gefunden\")\n",
    "    cleaned_tokens = df[\"cleaned\"].tolist()\n",
    "else:\n",
    "    print(\"   ✗ 'cleaned' Spalte nicht gefunden!\")\n",
    "    print(\"   Nutze 'text_cleaned' und konvertiere...\")\n",
    "    cleaned_tokens = [tokens.split() for tokens in df[\"text_cleaned\"]]\n",
    "\n",
    "print(f\"   ✓ {len(cleaned_tokens)} Dokumente geladen\")\n",
    "\n",
    "# Überprüfe auf leere Dokumente\n",
    "non_empty_count = sum(1 for doc in cleaned_tokens if len(doc) > 0)\n",
    "empty_count = len(cleaned_tokens) - non_empty_count\n",
    "print(f\"   - Nicht-leere Dokumente: {non_empty_count}\")\n",
    "print(f\"   - Leere Dokumente: {empty_count}\")\n",
    "\n",
    "# Debug: Zeige erste Tokens\n",
    "if non_empty_count > 0:\n",
    "    sample_doc = next((doc for doc in cleaned_tokens if len(doc) > 0), None)\n",
    "    if sample_doc:\n",
    "        print(f\"   - Beispiel-Tokens (erste 10): {sample_doc[:10]}\")\n",
    "\n",
    "if non_empty_count == 0:\n",
    "    print(\"   ✗ FEHLER: Alle Dokumente sind leer!\")\n",
    "else:\n",
    "    # Entferne leere Dokumente\n",
    "    cleaned_tokens = [doc for doc in cleaned_tokens if len(doc) > 0]\n",
    "    print(f\"   → Nach Filterung: {len(cleaned_tokens)} nicht-leere Dokumente\")\n",
    "    \n",
    "    print(f\"\\n2. Erstelle Dictionary aus Token-Liste...\")\n",
    "    dictionary = Dictionary(cleaned_tokens)\n",
    "    print(f\"   Wörter VOR Filter: {len(dictionary)}\")\n",
    "    \n",
    "    # Debug: Zeige häufigste Wörter VOR Filter\n",
    "    print(f\"\\n   Top 10 häufigste Wörter:\")\n",
    "    word_freq = {}\n",
    "    for doc in cleaned_tokens:\n",
    "        for word in doc:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for word, freq in sorted_words:\n",
    "        print(f\"     '{word}': {freq}x\")\n",
    "    \n",
    "    # MINIMALE FILTER: So mild wie möglich\n",
    "    print(f\"\\n   Wende MINIMALE Filter an:\")\n",
    "    print(f\"   - no_below=1 (Wort kommt mindestens 1x vor)\")\n",
    "    print(f\"   - no_above=0.99 (Wort kommt in max. 99% der Dokumente vor)\")\n",
    "    \n",
    "    initial_dict_size = len(dictionary)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.99, keep_n=100000)\n",
    "    print(f\"   Wörter NACH Filter: {len(dictionary)} (von {initial_dict_size})\")\n",
    "    \n",
    "    if len(dictionary) == 0:\n",
    "        print(f\"\\n   ⚠️ WARNUNG: Dictionary ist leer nach Filter!\")\n",
    "        print(f\"   → Verwende Dictionary OHNE Filter...\")\n",
    "        dictionary = Dictionary(cleaned_tokens)\n",
    "        print(f\"   Dictionary (kein Filter): {len(dictionary)} Wörter\")\n",
    "    \n",
    "    if len(dictionary) > 0:\n",
    "        print(f\"\\n3. Erstelle Corpus (Bag-of-Words)...\")\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in cleaned_tokens]\n",
    "        \n",
    "        # Debug: Corpus-Statistiken\n",
    "        corpus_lengths = [len(doc) for doc in corpus]\n",
    "        non_empty_corpus = sum(1 for doc in corpus if len(doc) > 0)\n",
    "        \n",
    "        print(f\"   Corpus Größe: {len(corpus)} Dokumente\")\n",
    "        print(f\"   Nicht-leere Dokumente im Corpus: {non_empty_corpus}\")\n",
    "        print(f\"   Durchschn. Terme pro Dokument: {sum(corpus_lengths)/len(corpus_lengths):.1f}\")\n",
    "        print(f\"   Min/Max Terme pro Dokument: {min(corpus_lengths)}/{max(corpus_lengths)}\")\n",
    "        \n",
    "        # Zeige Sample Corpus-Einträge\n",
    "        sample_corpus = [doc for doc in corpus if len(doc) > 0][:3]\n",
    "        print(f\"\\n   Sample Corpus-Dokumente (erste 3):\")\n",
    "        for i, doc in enumerate(sample_corpus):\n",
    "            terms = [(dictionary[term_id], freq) for term_id, freq in doc[:5]]\n",
    "            print(f\"     Doc {i}: {len(doc)} Terme - {terms}\")\n",
    "        \n",
    "        if non_empty_corpus > 0:\n",
    "            # Entferne Dokumente mit 0 Termen für LDA\n",
    "            filtered_corpus = [doc for doc in corpus if len(doc) > 0]\n",
    "            filtered_tokens = [tokens for tokens, doc in zip(cleaned_tokens, corpus) if len(doc) > 0]\n",
    "            \n",
    "            print(f\"\\n4. Trainiere LDA Modell...\")\n",
    "            print(f\"   Trainings-Daten: {len(filtered_corpus)} Dokumente mit {len(dictionary)} Wörtern im Dictionary\")\n",
    "            print(f\"   Parameter:\")\n",
    "            print(f\"   - Topics: 10\")\n",
    "            print(f\"   - Passes: 20\")\n",
    "            print(f\"   - Iterations: 400\")\n",
    "            print(f\"   (Dies kann 5-15 Minuten dauern...)\\n\")\n",
    "            \n",
    "            try:\n",
    "                model = LdaModel(\n",
    "                    corpus=filtered_corpus,\n",
    "                    id2word=dictionary.id2token,\n",
    "                    num_topics=10,\n",
    "                    random_state=42,\n",
    "                    chunksize=2000,\n",
    "                    passes=20,\n",
    "                    iterations=400,\n",
    "                    per_word_topics=True,\n",
    "                    minimum_probability=0.0,\n",
    "                    alpha='auto',\n",
    "                    eta='auto'\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"✓ LDA MODELL ERFOLGREICH TRAINIERT!\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"\\nModell-Statistiken:\")\n",
    "                print(f\"  - Dictionary Größe: {len(dictionary)} Wörter\")\n",
    "                print(f\"  - Corpus Größe: {len(filtered_corpus)} Dokumente\")\n",
    "                print(f\"  - Anzahl Topics: 10\")\n",
    "                \n",
    "                # Zeige Top Words pro Topic\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(\"TOP WORDS PER TOPIC (mit Gewichtungen)\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                for idx in range(10):\n",
    "                    terms = model.show_topic(idx, topn=10)\n",
    "                    print(f\"\\nTopic {idx}:\")\n",
    "                    for term, weight in terms:\n",
    "                        print(f\"  {term:25s} {weight:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"\\n✗ FEHLER beim LDA-Training:\")\n",
    "                print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"\\n✗ FEHLER: Alle Dokumente im Corpus sind leer!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ FEHLER: Dictionary ist leer und konnte nicht rekonstruiert werden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188216dc",
   "metadata": {},
   "source": [
    "# BerTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23031072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BERTOPIC - Topic Modeling\n",
      "================================================================================\n",
      "   ✓ 5627 Dokumente aus 'cleaned' erstellt\n",
      "\n",
      "✓ Daten vorbereitet:\n",
      "  Anzahl Dokumente (Beschwerden): 5627\n",
      "  Erste 10 Beschwerden:\n",
      "    1. love comcast constant update internet cable crash lot night day channel work dem...\n",
      "    2. comcast bad internet provider take online class multiple time late assignment po...\n",
      "    3. negative star star review work industry bad customer service comcast matter mone...\n",
      "    4. bad experience far install problem show schedule service appointment extreme dif...\n",
      "    5. check contract sign comcast advertised offer match contract issue sign 150mbps i...\n",
      "    6. thank god change dish give awesome pricing super people deal actually understand...\n",
      "    7. long time customer xfinity isp local walmart november customer representative xf...\n",
      "    8. malfunction dvr manager prevent add recording customer service fairly certain pr...\n",
      "    9. charge overwhelming comcast service rep ignorant rude resolve issue bill email t...\n",
      "    10. cable dish verse etc past know comcast take cake drive time day gripe online con...\n",
      "\n",
      "2. Starte BERTopic-Training...\n",
      "   (Dies kann einige Minuten dauern...)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58467ba9e0f54b9dbf9ab21012107703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ BERTopic abgeschlossen!\n",
      "================================================================================\n",
      "  Topics gefunden: 40\n",
      "  Dokumente verarbeitet: 5627\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BERTOPIC - Topic Modeling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Daten vorbereiten\n",
    "\n",
    "docs = [\" \".join(tokens) for tokens in df[\"cleaned\"]]               # Datenaufbereitung für BERTopic (bereinigte Beschwerden)\n",
    "print(f\"   ✓ {len(docs)} Dokumente aus 'cleaned' erstellt\")\n",
    "\n",
    "if docs is not None and len(docs) > 0:\n",
    "    print(f\"\\n✓ Daten vorbereitet:\")\n",
    "    print(f\"  Anzahl Dokumente (Beschwerden): {len(docs)}\")\n",
    "    print(f\"  Erste 10 Beschwerden:\")\n",
    "    for i, doc in enumerate(docs[:10]):\n",
    "        preview = doc[:80] + \"...\" if len(doc) > 80 else doc\n",
    "        print(f\"    {i+1}. {preview}\")\n",
    "\n",
    "    print(f\"\\n2. Starte BERTopic-Training...\")\n",
    "    print(\"   (Dies kann einige Minuten dauern...)\\n\")\n",
    "\n",
    "    topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"✓ BERTopic abgeschlossen!\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"  Topics gefunden: {len(set(topics)) - 1}\")  # -1 für Outlier\n",
    "    print(f\"  Dokumente verarbeitet: {len(topics)}\")\n",
    "else:\n",
    "    print(\"\\n✗ BERTopic konnte nicht trainiert werden (keine Daten)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3abb502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('move', 0.04481445961140756),\n",
       " ('new', 0.03266076321154964),\n",
       " ('address', 0.029590517008694098),\n",
       " ('transfer', 0.023903870762157265),\n",
       " ('service', 0.022463783873816066),\n",
       " ('apartment', 0.021374080065588223),\n",
       " ('comcast', 0.020974105574061413),\n",
       " ('account', 0.01799278978295429),\n",
       " ('call', 0.016336605605041337),\n",
       " ('day', 0.01574857058687185)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45029e52",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
