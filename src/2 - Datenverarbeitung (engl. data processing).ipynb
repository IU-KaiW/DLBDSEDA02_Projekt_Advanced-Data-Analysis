{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9217597b",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42862b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (0.23.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (4.67.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (82.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (26.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (3.1.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (5.11.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (82.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (26.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2026.1.4)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.11.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.0.4)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "%pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2d05d",
   "metadata": {},
   "source": [
    "# Datenvorverarbeitung (engl. data pre-processing)\n",
    "Die Datenvorverarbeitung gliedert sich in die Textvorverarbeitung (engl. text pre-processing) und die linguistische Verarbeitung (engl. linguistic processing) welche die Daten in eine modellgeeignete Form bringen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e167cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  I used to love Comcast. Until all these consta...\n",
      "1  I'm so over Comcast! The worst internet provid...\n",
      "2  If I could give them a negative star or no sta...\n",
      "3  I've had the worst experiences so far since in...\n",
      "4  Check your contract when you sign up for Comca...\n",
      "5  Thank God. I am changing to Dish. They gave me...\n",
      "6  I Have been a long time customer and only have...\n",
      "7  There is a malfunction on the DVR manager whic...\n",
      "8  Charges overwhelming. Comcast service rep was ...\n",
      "9  I have had cable, DISH, and U-verse, etc. in t...\n",
      "(5626, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Landen des bereinigten Datensatzes\n",
    "df = pd.read_csv('../datasets/complaints_data_cleaned.csv', usecols=[\"text\"])\n",
    "print(df.head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc22560",
   "metadata": {},
   "source": [
    "# Textvorverarbeitung (engl. text pre-processing)\n",
    "In der Textvorverarbeitung erfolgt die Textbereinigung (engl. text cleaning). Sie wird durch Rauschentfernung (engl. noise reduction) und Standardisierung (engl. standardisation) durchgeführt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d851caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeitet: 5627 Beschwerden\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0d6da_row0_col0, #T_0d6da_row0_col1, #T_0d6da_row1_col0, #T_0d6da_row1_col1, #T_0d6da_row2_col0, #T_0d6da_row2_col1, #T_0d6da_row3_col0, #T_0d6da_row3_col1, #T_0d6da_row4_col0, #T_0d6da_row4_col1, #T_0d6da_row5_col0, #T_0d6da_row5_col1, #T_0d6da_row6_col0, #T_0d6da_row6_col1, #T_0d6da_row7_col0, #T_0d6da_row7_col1, #T_0d6da_row8_col0, #T_0d6da_row8_col1, #T_0d6da_row9_col0, #T_0d6da_row9_col1 {\n",
       "  text-align: left;\n",
       "  width: 1000px;\n",
       "  max-width: 1500px;\n",
       "  font-size: 12px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0d6da\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0d6da_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_0d6da_level0_col1\" class=\"col_heading level0 col1\" >cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0d6da_row0_col0\" class=\"data row0 col0\" >I used to love Comcast. Until all these constant updates. My internet and cable crash a lot at night, and sometimes during the day, some channels don't even work and on demand sometimes don't play either. I wish they will do something about it. Because just a few mins ago, the internet have crashed for about 20 mins for no reason. I'm tired of it and thinking about switching to Wow or something. Please do not get Xfinity.</td>\n",
       "      <td id=\"T_0d6da_row0_col1\" class=\"data row0 col1\" >['love', 'comcast', 'constant', 'update', 'internet', 'cable', 'crash', 'lot', 'night', 'day', 'channel', 'work', 'demand', 'play', 'wish', 'min', 'ago', 'internet', 'crash', 'min', 'reason', 'tired', 'think', 'switch', 'wow', 'xfinity']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0d6da_row1_col0\" class=\"data row1 col0\" >I'm so over Comcast! The worst internet provider. I'm taking online classes and multiple times was late with my assignments because of the power interruptions in my area that lead to poor quality internet service. Definitely switching to Verizon. I'd rather pay $10 extra then dealing w/ Comcast and non stopping internet problems.</td>\n",
       "      <td id=\"T_0d6da_row1_col1\" class=\"data row1 col1\" >['comcast', 'bad', 'internet', 'provider', 'take', 'online', 'class', 'multiple', 'time', 'late', 'assignment', 'power', 'interruption', 'area', 'lead', 'poor', 'quality', 'internet', 'service', 'definitely', 'switch', 'verizon', 'pay', 'extra', 'deal', 'comcast', 'non', 'stop', 'internet', 'problem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0d6da_row2_col0\" class=\"data row2 col0\" >If I could give them a negative star or no stars on this review I would. I have never worked with any industry with as bad of customer service as Comcast. It is not a matter of money because I make well enough above and beyond to afford their services but they are a legitimate ripoff. I think they are the biggest scam of since the mortgage industry's major meltdown and I hope I move somewhere where Comcast does not exist. The disregard to want to help or do the right thing is honestly astounding. If you have to call, which you do FOR ALL ISSUES - billing, connection/service, adding or removing service, errors, it does not matter you will be transferred minimum of 4 times. Everyone says the same thing and passes the issues to the next person and no one resolves the problem.They offer promotional packages in small timeframes and can never access them again so they then upgrade you without you wishing and change your billing. It has been 5 months and I have been overcharged $40 a month since I started with them. The blatant rudeness that must make you qualified to do this job is the type of quality service that gets you this review. So... Dear Comcast, you suck. Sincerely, a customer who cannot wait to never use your service again.</td>\n",
       "      <td id=\"T_0d6da_row2_col1\" class=\"data row2 col1\" >['negative', 'star', 'star', 'review', 'work', 'industry', 'bad', 'customer', 'service', 'comcast', 'matter', 'money', 'afford', 'service', 'legitimate', 'ripoff', 'think', 'big', 'scam', 'mortgage', 'industry', 'major', 'meltdown', 'hope', 'comcast', 'exist', 'disregard', 'want', 'help', 'right', 'thing', 'honestly', 'astounding', 'issues', 'billing', 'connection', 'service', 'add', 'remove', 'service', 'error', 'matter', 'transfer', 'minimum', 'time', 'say', 'thing', 'pass', 'issue', 'person', 'resolve', 'problem', 'offer', 'promotional', 'package', 'small', 'timeframe', 'access', 'upgrade', 'wish', 'change', 'billing', 'month', 'overcharge', 'month', 'start', 'blatant', 'rudeness', 'qualified', 'job', 'type', 'quality', 'service', 'get', 'review', 'dear', 'comcast', 'suck', 'sincerely', 'customer', 'wait', 'use', 'service']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0d6da_row3_col0\" class=\"data row3 col0\" >I've had the worst experiences so far since install on 10/4/16. Nothing but problems. Two no shows on scheduled service appointments, extreme difficulty in adding boxes to the second floor. What is so difficult about adding boxes to an existing account? No thank you, I'm not starting a second account for the second floor of the same house! A separate bundle package? All I wanted was just to add a few boxes. Apparently this is not possible. Well then, I guess it's not possible to remain a customer!</td>\n",
       "      <td id=\"T_0d6da_row3_col1\" class=\"data row3 col1\" >['bad', 'experience', 'far', 'install', 'problem', 'show', 'schedule', 'service', 'appointment', 'extreme', 'difficulty', 'add', 'box', 'floor', 'difficult', 'add', 'box', 'exist', 'account', 'thank', 'start', 'account', 'floor', 'house', 'separate', 'bundle', 'package', 'want', 'add', 'box', 'apparently', 'possible', 'guess', 'possible', 'remain', 'customer']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0d6da_row4_col0\" class=\"data row4 col0\" >Check your contract when you sign up for Comcast as their advertised offers do not match the contract they issue. I signed up for $49.99 150Mbps internet for 2 years, however my contract has $19.99 for 25Mbps internet for 2 years. They say there is an add on in place for $30 which boost it to Blast! Pro, however this isn't part of the contract, which means that Comcast can increase the price whenever they want within the 2 years. This means I haven't received the advertised rate. Comcast has so far refused to issue corrected contract, or issue in writing that the $30 will remain at that price for 2 years. I just have to trust them. So watch out, Comcast is doing the usual illegal practices, I'm guessing to catch people out and hope they don't notice and end up paying more than they should.</td>\n",
       "      <td id=\"T_0d6da_row4_col1\" class=\"data row4 col1\" >['check', 'contract', 'sign', 'comcast', 'advertised', 'offer', 'match', 'contract', 'issue', 'sign', '150mbps', 'internet', 'year', 'contract', '25mbps', 'internet', 'year', 'add', 'place', 'boost', 'blast', 'pro', 'contract', 'mean', 'comcast', 'increase', 'price', 'want', 'year', 'mean', 'receive', 'advertised', 'rate', 'comcast', 'far', 'refuse', 'issue', 'correct', 'contract', 'issue', 'write', 'remain', 'price', 'year', 'trust', 'watch', 'comcast', 'usual', 'illegal', 'practice', 'guess', 'catch', 'people', 'hope', 'notice', 'end', 'pay']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0d6da_row5_col0\" class=\"data row5 col0\" >Thank God. I am changing to Dish. They gave me awesome pricing and super people to deal with. You can actually understand what they are saying. I'm so excited to finally be able to return this equipment although still haven't received the home security yet as promised 4 times. Go to h*ll Comcast. You have made me miserable and cause me to miss many hours of work with your promises.</td>\n",
       "      <td id=\"T_0d6da_row5_col1\" class=\"data row5 col1\" >['thank', 'god', 'change', 'dish', 'give', 'awesome', 'pricing', 'super', 'people', 'deal', 'actually', 'understand', 'say', 'excited', 'finally', 'able', 'return', 'equipment', 'receive', 'home', 'security', 'promise', 'time', 'h*ll', 'comcast', 'miserable', 'cause', 'miss', 'hour', 'work', 'promise']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_0d6da_row6_col0\" class=\"data row6 col0\" >I Have been a long time customer and only have Xfinity as my ISP for a while now. While I was in the local Walmart on November 4, 2016, there were customer representatives from Xfinity running promotions for and in the Salt Lake City area. Spoke with a representative and was able to get and signed a contract for Pro Blast at $50.00 a month with no contract or early termination fees. I received an email from Xfinity stating the changes that would be made to my account. It stated that not only would it be under contract for 24 months but there would be early termination fees. This is not what I had originally signed up for and it specifically states this on the contract that I signed. Contacted Xfinity customer service and was told since they cannot see the contract over the phone that I would need to go to Xfinity store in person. Went to Xfinity store on November 8, 2016 and was told that it would be under contract and there was no way around it. Because of this I have cancelled the upgrade and went back to my original plan. It's plain and simple. When a contract is signed it should be honored for what is stated on it. Xfinity is dishonest and not trustworthy. Therefore I will be looking and changing my ISP as soon as possible to another company. Xfinity does not deserve a paycheck from me or anyone else that I know.</td>\n",
       "      <td id=\"T_0d6da_row6_col1\" class=\"data row6 col1\" >['long', 'time', 'customer', 'xfinity', 'isp', 'local', 'walmart', 'november', 'customer', 'representative', 'xfinity', 'run', 'promotion', 'salt', 'lake', 'city', 'area', 'speak', 'representative', 'able', 'sign', 'contract', 'pro', 'blast', 'month', 'contract', 'early', 'termination', 'fee', 'receive', 'email', 'xfinity', 'state', 'change', 'account', 'state', 'contract', 'month', 'early', 'termination', 'fee', 'originally', 'sign', 'specifically', 'state', 'contract', 'sign', 'contacted', 'xfinity', 'customer', 'service', 'tell', 'contract', 'phone', 'need', 'xfinity', 'store', 'person', 'go', 'xfinity', 'store', 'november', 'tell', 'contract', 'way', 'cancel', 'upgrade', 'go', 'original', 'plan', 'plain', 'simple', 'contract', 'sign', 'honor', 'state', 'xfinity', 'dishonest', 'trustworthy', 'look', 'change', 'isp', 'soon', 'possible', 'company', 'xfinity', 'deserve', 'paycheck', 'know']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_0d6da_row7_col0\" class=\"data row7 col0\" >There is a malfunction on the DVR manager which is preventing us from adding more recordings. Customer service is fairly certain that the problem is from the signal from their system to ours, but protocol demands that they access our home before investigating that option. Since we work, that cannot be done until next Saturday. Customer service tech agreed that this seems illogical since logic would dictate that one would investigate the most probably malfunction first, but insists they must follow protocol. This is extremely frustrating. After 35 years as a customer of Comcast & their predecessors, I am investigating alternatives.</td>\n",
       "      <td id=\"T_0d6da_row7_col1\" class=\"data row7 col1\" >['malfunction', 'dvr', 'manager', 'prevent', 'add', 'recording', 'customer', 'service', 'fairly', 'certain', 'problem', 'signal', 'system', 'protocol', 'demand', 'access', 'home', 'investigate', 'option', 'work', 'saturday', 'customer', 'service', 'tech', 'agree', 'illogical', 'logic', 'dictate', 'investigate', 'probably', 'malfunction', 'insist', 'follow', 'protocol', 'extremely', 'frustrating', 'year', 'customer', 'comcast', 'predecessor', 'investigate', 'alternative']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_0d6da_row8_col0\" class=\"data row8 col0\" >Charges overwhelming. Comcast service rep was so ignorant and rude when I call to resolve my issue with my bill. I emailed Tom ** his rep was rude to me. None of the representative was helpful. They all just pass me on to other people. I am cutting my service with Comcast.</td>\n",
       "      <td id=\"T_0d6da_row8_col1\" class=\"data row8 col1\" >['charge', 'overwhelming', 'comcast', 'service', 'rep', 'ignorant', 'rude', 'resolve', 'issue', 'bill', 'email', 'tom', 'rep', 'rude', 'representative', 'helpful', 'pass', 'people', 'cut', 'service', 'comcast']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d6da_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_0d6da_row9_col0\" class=\"data row9 col0\" >I have had cable, DISH, and U-verse, etc. in the past. All are eh... but you know what? Comcast takes the cake. I have never been driven to take time out of my day just to gripe online for all to see. But consumers, stay away! So my first terrible experience with Comcast is that they took 5 phones and 2 months to come out and bury the lines they had to lay in my front yard to get the cable needed into my house. Finally got someone when my special needs neighbor tripped and fell!Now 3 months into my contract, I have had my internet, phone, and TV go out for HOURS at a time. I would spend 3 hours on with a tech when it will come back up after the technician resets the router manually for the 3rd or 4th time. I have had it, I work from home occasionally and this is a huge inconvenience! The hardware is faulty, I understand that sometimes you get a lemon... but 3 months! 3 months! I have had it. Worst company ever. Crappy equipment and terrible customer service, and worse is the technicians they hire! Not a clue! Comcast should send a technician out here to switch out this equipment before I set a bonfire to it.</td>\n",
       "      <td id=\"T_0d6da_row9_col1\" class=\"data row9 col1\" >['cable', 'dish', 'verse', 'etc', 'past', 'know', 'comcast', 'take', 'cake', 'drive', 'time', 'day', 'gripe', 'online', 'consumer', 'stay', 'away', 'terrible', 'experience', 'comcast', 'take', 'phone', 'month', 'come', 'bury', 'line', 'lay', 'yard', 'cable', 'need', 'house', 'finally', 'get', 'special', 'need', 'neighbor', 'trip', 'fell!now', 'month', 'contract', 'internet', 'phone', 'hour', 'time', 'spend', 'hour', 'tech', 'come', 'technician', 'reset', 'router', 'manually', 'time', 'work', 'home', 'occasionally', 'huge', 'inconvenience', 'hardware', 'faulty', 'understand', 'lemon', 'month', 'month', 'bad', 'company', 'crappy', 'equipment', 'terrible', 'customer', 'service', 'bad', 'technician', 'hire', 'clue', 'comcast', 'send', 'technician', 'switch', 'equipment', 'set', 'bonfire']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3541f9f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Textbereinigung (engl. text cleaning)\n",
    "import pandas as pd\n",
    "import spacy\n",
    "# Datensatz laden (Rohtext)\n",
    "df = pd.read_csv('../datasets/complaints_data.csv', usecols=[\"text\"], nrows=5627)  # Zeilenbegrenzung\n",
    "\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")                                                 # Fehlende Werte als leere Strings ersetzen\n",
    "\n",
    "## spaCy Pipeline (Textbereinigung, Tokenisierung, Lemmatisierung, )\n",
    "nlp = spacy.load(\"en_core_web_sm\")                                                 # englisches Modell (small version)\n",
    "\n",
    "df[\"cleaned\"] = [[token.lemma_.lower()\n",
    "                  for token in doc\n",
    "                  # Filterblock      \n",
    "                  if (not token.is_stop and                                 # Standard:     Stopwort-Filter (allgemein)\n",
    "                      not token.is_punct and                                # Standard:     Satzzeichen-Filter\n",
    "                      not token.like_num and                                # Standard:     Nummern-Filter (einfache Zahlen)\n",
    "                                                                            # Individuell:  Stopwort-Filter (individuell)\n",
    "                      len(token.text) > 2 and                               # Individuell:  Wörter mit min. 2 Zeichen\n",
    "                      not any(char in token.text for char in ':/-–—') and   # Individuell:  Filter für Datums-/Zeit-Token\n",
    "                      token.is_ascii and                                    # Individuell:  Emoijs-Filter\n",
    "                      token.pos_ != \"PRON\" and                              # Individuell:  Pronomen-Filter\n",
    "                      token.text.lower() not in [\"meh\", \"ugh\"])]            # Individuell:  Wortfilter (ggf. Beschwerde, comcast, muss getuned werden)\n",
    "                 for doc in nlp.pipe(df[\"text\"], batch_size=50)]            # Batch-Verarbeitung (50 Texte parallel)\n",
    "\n",
    "print(f\"Verarbeitet: {len(df)} Beschwerden\")\n",
    "\n",
    "## Rechtschreibfehlerkorrektur (engl. spelling correction) - nicht umgesetzt\n",
    "## Eigennamenerkennung (engl. Named Entity Recognition - NER) - nicht umgesetzt\n",
    "\n",
    "## syntaktische Verarbeitung (engl. syntactic processing)\n",
    "## semantische Verarbeitung (engl. context processing)\n",
    "### Semantisches Parsen (engl. semantic parsing)\n",
    "#### Eigennamenerkennung (engl. Named Entity Recognition - NER)\n",
    "\n",
    "# Ausgabe des Prozesses\n",
    "df.head(10)[[\"text\", \"cleaned\"]].style.set_properties(\n",
    "    **{'text-align': 'left', 'width': '1000px', 'max-width': '1500px', 'font-size': '12px'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526427e1",
   "metadata": {},
   "source": [
    "# Linguistische Verarbeitung (engl. linguistic processing)\n",
    "In der linguistischen Verarbeitung erfolgen lexikalische, syntaktische und semantische Verarbeitungsschritte, um Daten für die Datenvorbereitung (engl. data preparation) zu präparieren. Die Phase beinhaltet Schritte wie .... die Sprachdaten annotieren  und ..... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1259ab",
   "metadata": {},
   "source": [
    "## Vokabularerstellung (engl. vocabulary construction)\n",
    "Durch die Vokabularersterllung erfolgt ein Mapping der gefilterten Wörter (Token) zu IDs.\n",
    "\n",
    "Wortfrequenzschwellen (ist in SpaCy Pipeline)\n",
    "ggf. nur Nomen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d57aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vokabulargröße: 11947 Token (Wörter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>comcast</td>\n",
       "      <td>15230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9477</th>\n",
       "      <td>service</td>\n",
       "      <td>15149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10553</th>\n",
       "      <td>tell</td>\n",
       "      <td>7859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>call</td>\n",
       "      <td>7768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10730</th>\n",
       "      <td>time</td>\n",
       "      <td>6219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>customer</td>\n",
       "      <td>5726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>bill</td>\n",
       "      <td>5601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>internet</td>\n",
       "      <td>5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9289</th>\n",
       "      <td>say</td>\n",
       "      <td>5215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6893</th>\n",
       "      <td>month</td>\n",
       "      <td>4946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7822</th>\n",
       "      <td>phone</td>\n",
       "      <td>4883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7683</th>\n",
       "      <td>pay</td>\n",
       "      <td>4766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>cable</td>\n",
       "      <td>4657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>day</td>\n",
       "      <td>4108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>come</td>\n",
       "      <td>3829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>go</td>\n",
       "      <td>3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11804</th>\n",
       "      <td>work</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>charge</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>problem</td>\n",
       "      <td>3359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>get</td>\n",
       "      <td>3186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency\n",
       "ID                        \n",
       "2170    comcast      15230\n",
       "9477    service      15149\n",
       "10553      tell       7859\n",
       "1729       call       7768\n",
       "10730      time       6219\n",
       "2725   customer       5726\n",
       "1343       bill       5601\n",
       "5655   internet       5240\n",
       "9289        say       5215\n",
       "6893      month       4946\n",
       "7822      phone       4883\n",
       "7683        pay       4766\n",
       "1707      cable       4657\n",
       "2799        day       4108\n",
       "2184       come       3829\n",
       "4647         go       3633\n",
       "11804      work       3624\n",
       "1916     charge       3526\n",
       "8194    problem       3359\n",
       "4597        get       3186"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vokabularerstellung (engl. vocabulary construction)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Umwandung des Dataframes in Liste (Vectorizer braucht Strings)\n",
    "df[\"text_cleaned\"] = [\" \".join(tokens) for tokens in df[\"cleaned\"]]\n",
    "\n",
    "vectorizer = CountVectorizer()                                                  # Vectorizer mit fit_transform: Vokabular + Matrix in einem (fit_transform lernt das Vokabular)\n",
    "X = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "## Vokabularerstellung\n",
    "vocabulary = vectorizer.get_feature_names_out()                                 # Vokabular extrahieren\n",
    "print(f\"Vokabulargröße: {len(vocabulary)} Token (Wörter)\")                      # Ausgabe der Vokabulargrö0e\n",
    "\n",
    "word_counts = X.sum(axis=0).A1                                                  # Häufigkeiten (Summe pro Spalte)\n",
    "vocab_df = pd.DataFrame({\n",
    "    'word': vocabulary,\n",
    "    'frequency': word_counts\n",
    "}).sort_values('frequency', ascending=False)\n",
    "vocab_df.index.name = 'ID'                                                      # Beschriftung ID-Spalte\n",
    "vocab_df.head(20)                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c217",
   "metadata": {},
   "source": [
    "# Datenvorbereitung (engl. data preparation)\n",
    "Im Rahmen der Datenverarbeitung werden Merkmale (engl. features) erzeugt und ausgewählt. Dies erfolgt durch Merkmalsgenerierung (engl. feature generation/featurization) und/oder Merkmalsauswahl (engl. feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d2a6e",
   "metadata": {},
   "source": [
    "## Merkmalsgenerierung (engl. feature generation/featurization)\n",
    "Merkmalsgenerierung bezeichnet den Prozess, aus rohem oder vorverarbeitetem Text neue, informative Merkmale zu erzeugen. Unstrukturierte Daten werden dabei durch Merkmalskodierung (engl. feature encoding) in numerische oder kategorische Repräsentationen überführt, die Machine-Learning-Modelle nutzen können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2613238",
   "metadata": {},
   "source": [
    "### Vektorisierung (engl. vectorization)\n",
    "Als Vektorisierung wird die Merkmalskodierung von Textdaten bezeichnet, bei der Token (z.B. Wörter, Subwörter oder Zeichen) je nach Anwendungsfall auf Silben‑, Wort‑, Satz‑, Segment‑ oder Dokumenten‑Ebene in numerische Repräsentationen überführt werden, um lexikalische, syntaktische oder kontextuelle Aspekte eines Textes in numerischen Repräsentationen zu überführen die als Vektoren bezeichnet werden. Sie können im n‑dimensionalen Merkmalsraum (engl. feature space) dargestellt und zu Merkmalsmatrizen zusammengefasst werden, wobei zwischen Merkmalsvektoren (engl. feature vectors) und Merkmalseinbettungen (engl. feature embeddings) unterschieden wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0069410",
   "metadata": {},
   "source": [
    "### Merkmalsvektor (engl. feature vector)\n",
    "Durch Merkmalsvektoren kann Kontext nicht, bzw. nur lokal begrenzt durch n-Grammen erfasst werden.\n",
    "\n",
    "#### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "Bei TF-IDF handelt es sich um eine häufigkeitsbasierte Vektorisierungsmethode mit Informationsgewichtung durch manuelles Feature Engineering auf Einzeltoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a5b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Form: (5627, 11947)\n",
      "\n",
      " Beschwerde  ID-Vokabular Token (Wort)  TF-IDF Score\n",
      "          1          1778        crash      0.566252\n",
      "          1           351          min      0.379244\n",
      "          1           798     constant      0.223787\n",
      "          1           714          wow      0.221042\n",
      "          1           763         love      0.219993\n",
      "          1           583        tired      0.206018\n",
      "          1           450       update      0.197024\n",
      "          1           402         wish      0.186243\n",
      "          1           270        night      0.173193\n",
      "          1           293          lot      0.169301\n",
      "          1           231       demand      0.169146\n",
      "          1           187         play      0.159046\n",
      "          1           201       reason      0.155555\n",
      "          1             7     internet      0.151469\n",
      "          1           112      xfinity      0.142761\n",
      "          1           119       switch      0.134698\n",
      "          1            56      channel      0.133427\n",
      "          1           116          ago      0.131495\n",
      "          1            75        think      0.121815\n",
      "          1            16         work      0.089292\n",
      "          1            13          day      0.083374\n",
      "          1            12        cable      0.081478\n",
      "          1             0      comcast      0.050223\n",
      "          2          2221   assignment      0.338314\n",
      "          2           622 interruption      0.261891\n",
      "          2             7     internet      0.261739\n",
      "          2           820         lead      0.259803\n",
      "          2           729   definitely      0.251101\n",
      "          2           499        class      0.233627\n",
      "          2           465        power      0.230556\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Vokabular im korrekten Format: {wort: index}\n",
    "vocabulary = {word: idx for idx, word in enumerate(vocab_df['word'])}\n",
    "\n",
    "# Inverse Mapping: {index: wort}\n",
    "idx_to_word = {idx: word for word, idx in vocabulary.items()}\n",
    "\n",
    "# TF-IDF mit dem erstellten Vokabular\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Texte für TF-IDF vorbereiten (mit bereinigten Token)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "print(f\"TF-IDF Matrix Form: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Sparse-Matrix\n",
    "result_data = []\n",
    "cx = tfidf_matrix.tocoo()\n",
    "for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "    result_data.append({\n",
    "        'Beschwerde'    : i+1,                                                                                      # Beschwerdeindex angepasst\n",
    "        'ID-Vokabular'  : j,\n",
    "        'Token (Wort)'  : idx_to_word[j], \n",
    "        'TF-IDF Score'  : v\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(result_data).sort_values(['Beschwerde', 'TF-IDF Score'], ascending=[True, False])\n",
    "print(result_df.head(30).to_string(index=False))                                                                    # pandas-Spalte ausblenden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f795ce1",
   "metadata": {},
   "source": [
    "# Merkmalseinbettung (engl. feature embedding)\n",
    "Durch Merkmalseinbettungen kann Kontext global auf Token oder Tokensequenzebene erfasst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0abc6",
   "metadata": {},
   "source": [
    "# Modellentwicklung (engl. model development)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5834439",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be303bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Latent Dirichlet Allocation (LDA)\n",
      "================================================================================\n",
      "\n",
      "1. Überprüfe Datenverfügbarkeit...\n",
      "   ✓ 'cleaned' Spalte gefunden\n",
      "   ✓ 5627 Dokumente geladen\n",
      "   - Nicht-leere Dokumente: 5604\n",
      "   - Leere Dokumente: 23\n",
      "   - Beispiel-Tokens (erste 10): ['love', 'comcast', 'constant', 'update', 'internet', 'cable', 'crash', 'lot', 'night', 'day']\n",
      "   → Nach Filterung: 5604 nicht-leere Dokumente\n",
      "\n",
      "2. Erstelle Dictionary aus Token-Liste...\n",
      "   Wörter VOR Filter: 12598\n",
      "\n",
      "   Top 10 häufigste Wörter:\n",
      "     'comcast': 15163x\n",
      "     'service': 15137x\n",
      "     'tell': 7859x\n",
      "     'call': 7761x\n",
      "     'time': 6212x\n",
      "     'customer': 5722x\n",
      "     'bill': 5595x\n",
      "     'internet': 5230x\n",
      "     'say': 5215x\n",
      "     'month': 4940x\n",
      "\n",
      "   Wende MINIMALE Filter an:\n",
      "   - no_below=1 (Wort kommt mindestens 1x vor)\n",
      "   - no_above=0.99 (Wort kommt in max. 99% der Dokumente vor)\n",
      "   Wörter NACH Filter: 12598 (von 12598)\n",
      "\n",
      "3. Erstelle Corpus (Bag-of-Words)...\n",
      "   Corpus Größe: 5604 Dokumente\n",
      "   Nicht-leere Dokumente im Corpus: 5604\n",
      "   Durchschn. Terme pro Dokument: 56.7\n",
      "   Min/Max Terme pro Dokument: 3/333\n",
      "\n",
      "   Sample Corpus-Dokumente (erste 3):\n",
      "     Doc 0: 23 Terme - [('ago', 1), ('cable', 1), ('channel', 1), ('comcast', 1), ('constant', 1)]\n",
      "     Doc 1: 27 Terme - [('comcast', 2), ('internet', 3), ('switch', 1), ('area', 1), ('assignment', 1)]\n",
      "     Doc 2: 68 Terme - [('comcast', 3), ('think', 1), ('wish', 1), ('work', 1), ('bad', 1)]\n",
      "\n",
      "4. Trainiere LDA Modell...\n",
      "   Trainings-Daten: 5604 Dokumente mit 12598 Wörtern im Dictionary\n",
      "   Parameter:\n",
      "   - Topics: 10\n",
      "   - Passes: 20\n",
      "   - Iterations: 400\n",
      "   (Dies kann 5-15 Minuten dauern...)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ LDA MODELL ERFOLGREICH TRAINIERT!\n",
      "================================================================================\n",
      "\n",
      "Modell-Statistiken:\n",
      "  - Dictionary Größe: 12598 Wörter\n",
      "  - Corpus Größe: 5604 Dokumente\n",
      "  - Anzahl Topics: 10\n",
      "\n",
      "================================================================================\n",
      "TOP WORDS PER TOPIC (mit Gewichtungen)\n",
      "================================================================================\n",
      "\n",
      "Topic 0:\n",
      "  channel                   0.0268\n",
      "  watch                     0.0242\n",
      "  comcast                   0.0237\n",
      "  movie                     0.0189\n",
      "  cable                     0.0177\n",
      "  pay                       0.0159\n",
      "  like                      0.0146\n",
      "  program                   0.0136\n",
      "  company                   0.0120\n",
      "  time                      0.0117\n",
      "\n",
      "Topic 1:\n",
      "  service                   0.0415\n",
      "  comcast                   0.0388\n",
      "  bill                      0.0229\n",
      "  tell                      0.0211\n",
      "  call                      0.0164\n",
      "  pay                       0.0159\n",
      "  charge                    0.0139\n",
      "  customer                  0.0139\n",
      "  month                     0.0138\n",
      "  account                   0.0137\n",
      "\n",
      "Topic 2:\n",
      "  call                      0.0311\n",
      "  come                      0.0305\n",
      "  say                       0.0252\n",
      "  comcast                   0.0246\n",
      "  tell                      0.0233\n",
      "  work                      0.0213\n",
      "  tech                      0.0202\n",
      "  service                   0.0201\n",
      "  cable                     0.0195\n",
      "  day                       0.0183\n",
      "\n",
      "Topic 3:\n",
      "  datum                     0.0506\n",
      "  usage                     0.0430\n",
      "  data                      0.0258\n",
      "  deposit                   0.0243\n",
      "  cap                       0.0205\n",
      "  overage                   0.0123\n",
      "  limit                     0.0118\n",
      "  death                     0.0102\n",
      "  limitation                0.0087\n",
      "  pbs                       0.0084\n",
      "\n",
      "Topic 4:\n",
      "  november                  0.0924\n",
      "  house                     0.0349\n",
      "  signal                    0.0306\n",
      "  goodlow                   0.0280\n",
      "  voice                     0.0277\n",
      "  place                     0.0230\n",
      "  integral                  0.0220\n",
      "  channel                   0.0200\n",
      "  adrian                    0.0181\n",
      "  street                    0.0169\n",
      "\n",
      "Topic 5:\n",
      "  phone                     0.0314\n",
      "  number                    0.0269\n",
      "  call                      0.0237\n",
      "  comcast                   0.0204\n",
      "  time                      0.0187\n",
      "  customer                  0.0179\n",
      "  service                   0.0171\n",
      "  speak                     0.0164\n",
      "  hour                      0.0160\n",
      "  minute                    0.0145\n",
      "\n",
      "Topic 6:\n",
      "  service                   0.0466\n",
      "  internet                  0.0412\n",
      "  comcast                   0.0403\n",
      "  problem                   0.0282\n",
      "  modem                     0.0238\n",
      "  time                      0.0153\n",
      "  speed                     0.0148\n",
      "  work                      0.0144\n",
      "  customer                  0.0111\n",
      "  connection                0.0096\n",
      "\n",
      "Topic 7:\n",
      "  box                       0.2099\n",
      "  cable                     0.0630\n",
      "  digital                   0.0507\n",
      "  channel                   0.0449\n",
      "  dvr                       0.0348\n",
      "  card                      0.0168\n",
      "  basic                     0.0137\n",
      "  upgrade                   0.0134\n",
      "  free                      0.0122\n",
      "  additional                0.0115\n",
      "\n",
      "Topic 8:\n",
      "  game                      0.0771\n",
      "  snow                      0.0129\n",
      "  vice                      0.0095\n",
      "  gateway                   0.0092\n",
      "  boston                    0.0076\n",
      "  cathy                     0.0063\n",
      "  evasive                   0.0054\n",
      "  recording                 0.0054\n",
      "  nhl                       0.0051\n",
      "  name                      0.0050\n",
      "\n",
      "Topic 9:\n",
      "  month                     0.0309\n",
      "  comcast                   0.0266\n",
      "  service                   0.0231\n",
      "  internet                  0.0202\n",
      "  bill                      0.0184\n",
      "  year                      0.0181\n",
      "  pay                       0.0178\n",
      "  price                     0.0150\n",
      "  cable                     0.0147\n",
      "  tell                      0.0146\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Latent Dirichlet Allocation (LDA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Überprüfe Datenverfügbarkeit...\")\n",
    "\n",
    "# Prüfe, ob 'cleaned' Spalte existiert\n",
    "if \"cleaned\" in df.columns:\n",
    "    print(\"   ✓ 'cleaned' Spalte gefunden\")\n",
    "    cleaned_tokens = df[\"cleaned\"].tolist()\n",
    "else:\n",
    "    print(\"   ✗ 'cleaned' Spalte nicht gefunden!\")\n",
    "    print(\"   Nutze 'text_cleaned' und konvertiere...\")\n",
    "    cleaned_tokens = [tokens.split() for tokens in df[\"text_cleaned\"]]\n",
    "\n",
    "print(f\"   ✓ {len(cleaned_tokens)} Dokumente geladen\")\n",
    "\n",
    "# Überprüfe auf leere Dokumente\n",
    "non_empty_count = sum(1 for doc in cleaned_tokens if len(doc) > 0)\n",
    "empty_count = len(cleaned_tokens) - non_empty_count\n",
    "print(f\"   - Nicht-leere Dokumente: {non_empty_count}\")\n",
    "print(f\"   - Leere Dokumente: {empty_count}\")\n",
    "\n",
    "# Debug: Zeige erste Tokens\n",
    "if non_empty_count > 0:\n",
    "    sample_doc = next((doc for doc in cleaned_tokens if len(doc) > 0), None)\n",
    "    if sample_doc:\n",
    "        print(f\"   - Beispiel-Tokens (erste 10): {sample_doc[:10]}\")\n",
    "\n",
    "if non_empty_count == 0:\n",
    "    print(\"   ✗ FEHLER: Alle Dokumente sind leer!\")\n",
    "else:\n",
    "    # Entferne leere Dokumente\n",
    "    cleaned_tokens = [doc for doc in cleaned_tokens if len(doc) > 0]\n",
    "    print(f\"   → Nach Filterung: {len(cleaned_tokens)} nicht-leere Dokumente\")\n",
    "    \n",
    "    print(f\"\\n2. Erstelle Dictionary aus Token-Liste...\")\n",
    "    dictionary = Dictionary(cleaned_tokens)\n",
    "    print(f\"   Wörter VOR Filter: {len(dictionary)}\")\n",
    "    \n",
    "    # Debug: Zeige häufigste Wörter VOR Filter\n",
    "    print(f\"\\n   Top 10 häufigste Wörter:\")\n",
    "    word_freq = {}\n",
    "    for doc in cleaned_tokens:\n",
    "        for word in doc:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for word, freq in sorted_words:\n",
    "        print(f\"     '{word}': {freq}x\")\n",
    "    \n",
    "    # MINIMALE FILTER: So mild wie möglich\n",
    "    print(f\"\\n   Wende MINIMALE Filter an:\")\n",
    "    print(f\"   - no_below=1 (Wort kommt mindestens 1x vor)\")\n",
    "    print(f\"   - no_above=0.99 (Wort kommt in max. 99% der Dokumente vor)\")\n",
    "    \n",
    "    initial_dict_size = len(dictionary)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.99, keep_n=100000)\n",
    "    print(f\"   Wörter NACH Filter: {len(dictionary)} (von {initial_dict_size})\")\n",
    "    \n",
    "    if len(dictionary) == 0:\n",
    "        print(f\"\\n   ⚠️ WARNUNG: Dictionary ist leer nach Filter!\")\n",
    "        print(f\"   → Verwende Dictionary OHNE Filter...\")\n",
    "        dictionary = Dictionary(cleaned_tokens)\n",
    "        print(f\"   Dictionary (kein Filter): {len(dictionary)} Wörter\")\n",
    "    \n",
    "    if len(dictionary) > 0:\n",
    "        print(f\"\\n3. Erstelle Corpus (Bag-of-Words)...\")\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in cleaned_tokens]\n",
    "        \n",
    "        # Debug: Corpus-Statistiken\n",
    "        corpus_lengths = [len(doc) for doc in corpus]\n",
    "        non_empty_corpus = sum(1 for doc in corpus if len(doc) > 0)\n",
    "        \n",
    "        print(f\"   Corpus Größe: {len(corpus)} Dokumente\")\n",
    "        print(f\"   Nicht-leere Dokumente im Corpus: {non_empty_corpus}\")\n",
    "        print(f\"   Durchschn. Terme pro Dokument: {sum(corpus_lengths)/len(corpus_lengths):.1f}\")\n",
    "        print(f\"   Min/Max Terme pro Dokument: {min(corpus_lengths)}/{max(corpus_lengths)}\")\n",
    "        \n",
    "        # Zeige Sample Corpus-Einträge\n",
    "        sample_corpus = [doc for doc in corpus if len(doc) > 0][:3]\n",
    "        print(f\"\\n   Sample Corpus-Dokumente (erste 3):\")\n",
    "        for i, doc in enumerate(sample_corpus):\n",
    "            terms = [(dictionary[term_id], freq) for term_id, freq in doc[:5]]\n",
    "            print(f\"     Doc {i}: {len(doc)} Terme - {terms}\")\n",
    "        \n",
    "        if non_empty_corpus > 0:\n",
    "            # Entferne Dokumente mit 0 Termen für LDA\n",
    "            filtered_corpus = [doc for doc in corpus if len(doc) > 0]\n",
    "            filtered_tokens = [tokens for tokens, doc in zip(cleaned_tokens, corpus) if len(doc) > 0]\n",
    "            \n",
    "            print(f\"\\n4. Trainiere LDA Modell...\")\n",
    "            print(f\"   Trainings-Daten: {len(filtered_corpus)} Dokumente mit {len(dictionary)} Wörtern im Dictionary\")\n",
    "            print(f\"   Parameter:\")\n",
    "            print(f\"   - Topics: 10\")\n",
    "            print(f\"   - Passes: 20\")\n",
    "            print(f\"   - Iterations: 400\")\n",
    "            print(f\"   (Dies kann 5-15 Minuten dauern...)\\n\")\n",
    "            \n",
    "            try:\n",
    "                model = LdaModel(\n",
    "                    corpus=filtered_corpus,\n",
    "                    id2word=dictionary.id2token,\n",
    "                    num_topics=10,\n",
    "                    random_state=42,\n",
    "                    chunksize=2000,\n",
    "                    passes=20,\n",
    "                    iterations=400,\n",
    "                    per_word_topics=True,\n",
    "                    minimum_probability=0.0,\n",
    "                    alpha='auto',\n",
    "                    eta='auto'\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"✓ LDA MODELL ERFOLGREICH TRAINIERT!\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"\\nModell-Statistiken:\")\n",
    "                print(f\"  - Dictionary Größe: {len(dictionary)} Wörter\")\n",
    "                print(f\"  - Corpus Größe: {len(filtered_corpus)} Dokumente\")\n",
    "                print(f\"  - Anzahl Topics: 10\")\n",
    "                \n",
    "                # Zeige Top Words pro Topic\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(\"TOP WORDS PER TOPIC (mit Gewichtungen)\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                for idx in range(10):\n",
    "                    terms = model.show_topic(idx, topn=10)\n",
    "                    print(f\"\\nTopic {idx}:\")\n",
    "                    for term, weight in terms:\n",
    "                        print(f\"  {term:25s} {weight:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"\\n✗ FEHLER beim LDA-Training:\")\n",
    "                print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"\\n✗ FEHLER: Alle Dokumente im Corpus sind leer!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ FEHLER: Dictionary ist leer und konnte nicht rekonstruiert werden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188216dc",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23031072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BERTOPIC - Topic Modeling\n",
      "================================================================================\n",
      "   ✓ 5627 Dokumente aus 'cleaned' erstellt\n",
      "\n",
      "✓ Daten vorbereitet:\n",
      "  Anzahl Dokumente (Beschwerden): 5627\n",
      "  Erste 10 Beschwerden:\n",
      "    1. love comcast constant update internet cable crash lot night day channel work dem...\n",
      "    2. comcast bad internet provider take online class multiple time late assignment po...\n",
      "    3. negative star star review work industry bad customer service comcast matter mone...\n",
      "    4. bad experience far install problem show schedule service appointment extreme dif...\n",
      "    5. check contract sign comcast advertised offer match contract issue sign 150mbps i...\n",
      "    6. thank god change dish give awesome pricing super people deal actually understand...\n",
      "    7. long time customer xfinity isp local walmart november customer representative xf...\n",
      "    8. malfunction dvr manager prevent add recording customer service fairly certain pr...\n",
      "    9. charge overwhelming comcast service rep ignorant rude resolve issue bill email t...\n",
      "    10. cable dish verse etc past know comcast take cake drive time day gripe online con...\n",
      "\n",
      "2. Starte BERTopic-Training...\n",
      "   (Dies kann einige Minuten dauern...)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1434.68it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ BERTopic abgeschlossen!\n",
      "================================================================================\n",
      "  Topics gefunden: 1\n",
      "  Dokumente verarbeitet: 5627\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BERTOPIC - Topic Modeling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Daten vorbereiten\n",
    "\n",
    "docs = [\" \".join(tokens) for tokens in df[\"cleaned\"]]               # Datenaufbereitung für BERTopic (bereinigte Beschwerden)\n",
    "print(f\"   ✓ {len(docs)} Dokumente aus 'cleaned' erstellt\")\n",
    "\n",
    "if docs is not None and len(docs) > 0:\n",
    "    print(f\"\\n✓ Daten vorbereitet:\")\n",
    "    print(f\"  Anzahl Dokumente (Beschwerden): {len(docs)}\")\n",
    "    print(f\"  Erste 10 Beschwerden:\")\n",
    "    for i, doc in enumerate(docs[:10]):\n",
    "        preview = doc[:80] + \"...\" if len(doc) > 80 else doc\n",
    "        print(f\"    {i+1}. {preview}\")\n",
    "\n",
    "    print(f\"\\n2. Starte BERTopic-Training...\")\n",
    "    print(\"   (Dies kann einige Minuten dauern...)\\n\")\n",
    "\n",
    "    topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"✓ BERTopic abgeschlossen!\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"  Topics gefunden: {len(set(topics)) - 1}\")  # -1 für Outlier\n",
    "    print(f\"  Dokumente verarbeitet: {len(topics)}\")\n",
    "else:\n",
    "    print(\"\\n✗ BERTopic konnte nicht trainiert werden (keine Daten)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3abb502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05),\n",
       " ('', 1e-05)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896092c7",
   "metadata": {},
   "source": [
    "# Modellbewertung"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projekt_Advanced-Data-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
