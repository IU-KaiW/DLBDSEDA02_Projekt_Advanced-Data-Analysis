{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9217597b",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42862b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (0.23.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (4.67.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (82.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy) (26.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (3.1.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (5.11.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: typer>=0.23.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.23.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3.8.0,>=3.7.2 (from en-core-web-sm==3.7.1)\n",
      "  Downloading spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (82.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (26.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2026.1.4)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.11.0)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.0.4)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.3)\n",
      "Downloading spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
      "Downloading thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, langcodes, blis, thinc, spacy, en-core-web-sm\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.5\n",
      "\u001b[2K    Uninstalling numpy-2.3.5:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.5\n",
      "\u001b[2K  Attempting uninstall: blis━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: blis 1.3.3━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling blis-1.3.3:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled blis-1.3.3━━━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: thinc━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: thinc 8.3.10[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling thinc-8.3.10:━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled thinc-8.3.10━\u001b[0m \u001b[32m0/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: spacy[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [thinc]\n",
      "\u001b[2K    Found existing installation: spacy 3.8.11━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [thinc]\n",
      "\u001b[2K    Uninstalling spacy-3.8.11:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [thinc]\n",
      "\u001b[2K      Successfully uninstalled spacy-3.8.11━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [thinc]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [en-core-web-sm]m [en-core-web-sm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed blis-0.7.11 en-core-web-sm-3.7.1 langcodes-3.5.1 numpy-1.26.4 spacy-3.7.5 thinc-8.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "%pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2d05d",
   "metadata": {},
   "source": [
    "# Datenvorverarbeitung (engl. data pre-processing)\n",
    "Die Datenvorverarbeitung gliedert sich in die Textvorverarbeitung (engl. text pre-processing) und die linguistische Verarbeitung (engl. linguistic processing) welche die Daten in eine modellgeeignete Form bringen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e167cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  I used to love Comcast. Until all these consta...\n",
      "1  I'm so over Comcast! The worst internet provid...\n",
      "2  If I could give them a negative star or no sta...\n",
      "3  I've had the worst experiences so far since in...\n",
      "4  Check your contract when you sign up for Comca...\n",
      "5  Thank God. I am changing to Dish. They gave me...\n",
      "6  I Have been a long time customer and only have...\n",
      "7  There is a malfunction on the DVR manager whic...\n",
      "8  Charges overwhelming. Comcast service rep was ...\n",
      "9  I have had cable, DISH, and U-verse, etc. in t...\n",
      "(5626, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Landen des bereinigten Datensatzes\n",
    "df = pd.read_csv('../datasets/complaints_data_cleaned.csv', usecols=[\"text\"])\n",
    "print(df.head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc22560",
   "metadata": {},
   "source": [
    "# Textvorverarbeitung (engl. text pre-processing)\n",
    "In der Textvorverarbeitung erfolgt die Textbereinigung (engl. text cleaning). Sie wird durch Rauschentfernung (engl. noise reduction) und Standardisierung (engl. standardisation) durchgeführt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeitet: 5627 Beschwerden\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fa243_row0_col0, #T_fa243_row0_col1, #T_fa243_row1_col0, #T_fa243_row1_col1, #T_fa243_row2_col0, #T_fa243_row2_col1, #T_fa243_row3_col0, #T_fa243_row3_col1, #T_fa243_row4_col0, #T_fa243_row4_col1, #T_fa243_row5_col0, #T_fa243_row5_col1, #T_fa243_row6_col0, #T_fa243_row6_col1, #T_fa243_row7_col0, #T_fa243_row7_col1, #T_fa243_row8_col0, #T_fa243_row8_col1, #T_fa243_row9_col0, #T_fa243_row9_col1 {\n",
       "  text-align: left;\n",
       "  width: 1000px;\n",
       "  max-width: 1500px;\n",
       "  font-size: 12px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fa243\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fa243_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_fa243_level0_col1\" class=\"col_heading level0 col1\" >cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fa243_row0_col0\" class=\"data row0 col0\" >I used to love Comcast. Until all these constant updates. My internet and cable crash a lot at night, and sometimes during the day, some channels don't even work and on demand sometimes don't play either. I wish they will do something about it. Because just a few mins ago, the internet have crashed for about 20 mins for no reason. I'm tired of it and thinking about switching to Wow or something. Please do not get Xfinity.</td>\n",
       "      <td id=\"T_fa243_row0_col1\" class=\"data row0 col1\" >['love', 'comcast', 'constant', 'update', 'internet', 'cable', 'crash', 'lot', 'night', 'day', 'channel', 'work', 'demand', 'play', 'wish', 'min', 'ago', 'internet', 'crash', 'min', 'reason', 'tired', 'think', 'switch', 'wow', 'xfinity']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fa243_row1_col0\" class=\"data row1 col0\" >I'm so over Comcast! The worst internet provider. I'm taking online classes and multiple times was late with my assignments because of the power interruptions in my area that lead to poor quality internet service. Definitely switching to Verizon. I'd rather pay $10 extra then dealing w/ Comcast and non stopping internet problems.</td>\n",
       "      <td id=\"T_fa243_row1_col1\" class=\"data row1 col1\" >['comcast', 'bad', 'internet', 'provider', 'take', 'online', 'class', 'multiple', 'time', 'late', 'assignment', 'power', 'interruption', 'area', 'lead', 'poor', 'quality', 'internet', 'service', 'definitely', 'switch', 'verizon', 'pay', 'extra', 'deal', 'comcast', 'non', 'stop', 'internet', 'problem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fa243_row2_col0\" class=\"data row2 col0\" >If I could give them a negative star or no stars on this review I would. I have never worked with any industry with as bad of customer service as Comcast. It is not a matter of money because I make well enough above and beyond to afford their services but they are a legitimate ripoff. I think they are the biggest scam of since the mortgage industry's major meltdown and I hope I move somewhere where Comcast does not exist. The disregard to want to help or do the right thing is honestly astounding. If you have to call, which you do FOR ALL ISSUES - billing, connection/service, adding or removing service, errors, it does not matter you will be transferred minimum of 4 times. Everyone says the same thing and passes the issues to the next person and no one resolves the problem.They offer promotional packages in small timeframes and can never access them again so they then upgrade you without you wishing and change your billing. It has been 5 months and I have been overcharged $40 a month since I started with them. The blatant rudeness that must make you qualified to do this job is the type of quality service that gets you this review. So... Dear Comcast, you suck. Sincerely, a customer who cannot wait to never use your service again.</td>\n",
       "      <td id=\"T_fa243_row2_col1\" class=\"data row2 col1\" >['negative', 'star', 'star', 'review', 'work', 'industry', 'bad', 'customer', 'service', 'comcast', 'matter', 'money', 'afford', 'service', 'legitimate', 'ripoff', 'think', 'big', 'scam', 'mortgage', 'industry', 'major', 'meltdown', 'hope', 'comcast', 'exist', 'disregard', 'want', 'help', 'right', 'thing', 'honestly', 'astounding', 'issues', 'billing', 'connection', 'service', 'add', 'remove', 'service', 'error', 'matter', 'transfer', 'minimum', 'time', 'say', 'thing', 'pass', 'issue', 'person', 'resolve', 'problem', 'offer', 'promotional', 'package', 'small', 'timeframe', 'access', 'upgrade', 'wish', 'change', 'billing', 'month', 'overcharge', 'month', 'start', 'blatant', 'rudeness', 'qualified', 'job', 'type', 'quality', 'service', 'get', 'review', 'dear', 'comcast', 'suck', 'sincerely', 'customer', 'wait', 'use', 'service']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fa243_row3_col0\" class=\"data row3 col0\" >I've had the worst experiences so far since install on 10/4/16. Nothing but problems. Two no shows on scheduled service appointments, extreme difficulty in adding boxes to the second floor. What is so difficult about adding boxes to an existing account? No thank you, I'm not starting a second account for the second floor of the same house! A separate bundle package? All I wanted was just to add a few boxes. Apparently this is not possible. Well then, I guess it's not possible to remain a customer!</td>\n",
       "      <td id=\"T_fa243_row3_col1\" class=\"data row3 col1\" >['bad', 'experience', 'far', 'install', 'problem', 'show', 'schedule', 'service', 'appointment', 'extreme', 'difficulty', 'add', 'box', 'floor', 'difficult', 'add', 'box', 'exist', 'account', 'thank', 'start', 'account', 'floor', 'house', 'separate', 'bundle', 'package', 'want', 'add', 'box', 'apparently', 'possible', 'guess', 'possible', 'remain', 'customer']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fa243_row4_col0\" class=\"data row4 col0\" >Check your contract when you sign up for Comcast as their advertised offers do not match the contract they issue. I signed up for $49.99 150Mbps internet for 2 years, however my contract has $19.99 for 25Mbps internet for 2 years. They say there is an add on in place for $30 which boost it to Blast! Pro, however this isn't part of the contract, which means that Comcast can increase the price whenever they want within the 2 years. This means I haven't received the advertised rate. Comcast has so far refused to issue corrected contract, or issue in writing that the $30 will remain at that price for 2 years. I just have to trust them. So watch out, Comcast is doing the usual illegal practices, I'm guessing to catch people out and hope they don't notice and end up paying more than they should.</td>\n",
       "      <td id=\"T_fa243_row4_col1\" class=\"data row4 col1\" >['check', 'contract', 'sign', 'comcast', 'advertised', 'offer', 'match', 'contract', 'issue', 'sign', '150mbps', 'internet', 'year', 'contract', '25mbps', 'internet', 'year', 'add', 'place', 'boost', 'blast', 'pro', 'contract', 'mean', 'comcast', 'increase', 'price', 'want', 'year', 'mean', 'receive', 'advertised', 'rate', 'comcast', 'far', 'refuse', 'issue', 'correct', 'contract', 'issue', 'write', 'remain', 'price', 'year', 'trust', 'watch', 'comcast', 'usual', 'illegal', 'practice', 'guess', 'catch', 'people', 'hope', 'notice', 'end', 'pay']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_fa243_row5_col0\" class=\"data row5 col0\" >Thank God. I am changing to Dish. They gave me awesome pricing and super people to deal with. You can actually understand what they are saying. I'm so excited to finally be able to return this equipment although still haven't received the home security yet as promised 4 times. Go to h*ll Comcast. You have made me miserable and cause me to miss many hours of work with your promises.</td>\n",
       "      <td id=\"T_fa243_row5_col1\" class=\"data row5 col1\" >['thank', 'god', 'change', 'dish', 'give', 'awesome', 'pricing', 'super', 'people', 'deal', 'actually', 'understand', 'say', 'excited', 'finally', 'able', 'return', 'equipment', 'receive', 'home', 'security', 'promise', 'time', 'h*ll', 'comcast', 'miserable', 'cause', 'miss', 'hour', 'work', 'promise']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_fa243_row6_col0\" class=\"data row6 col0\" >I Have been a long time customer and only have Xfinity as my ISP for a while now. While I was in the local Walmart on November 4, 2016, there were customer representatives from Xfinity running promotions for and in the Salt Lake City area. Spoke with a representative and was able to get and signed a contract for Pro Blast at $50.00 a month with no contract or early termination fees. I received an email from Xfinity stating the changes that would be made to my account. It stated that not only would it be under contract for 24 months but there would be early termination fees. This is not what I had originally signed up for and it specifically states this on the contract that I signed. Contacted Xfinity customer service and was told since they cannot see the contract over the phone that I would need to go to Xfinity store in person. Went to Xfinity store on November 8, 2016 and was told that it would be under contract and there was no way around it. Because of this I have cancelled the upgrade and went back to my original plan. It's plain and simple. When a contract is signed it should be honored for what is stated on it. Xfinity is dishonest and not trustworthy. Therefore I will be looking and changing my ISP as soon as possible to another company. Xfinity does not deserve a paycheck from me or anyone else that I know.</td>\n",
       "      <td id=\"T_fa243_row6_col1\" class=\"data row6 col1\" >['long', 'time', 'customer', 'xfinity', 'isp', 'local', 'walmart', 'november', 'customer', 'representative', 'xfinity', 'run', 'promotion', 'salt', 'lake', 'city', 'area', 'speak', 'representative', 'able', 'sign', 'contract', 'pro', 'blast', 'month', 'contract', 'early', 'termination', 'fee', 'receive', 'email', 'xfinity', 'state', 'change', 'account', 'state', 'contract', 'month', 'early', 'termination', 'fee', 'originally', 'sign', 'specifically', 'state', 'contract', 'sign', 'contacted', 'xfinity', 'customer', 'service', 'tell', 'contract', 'phone', 'need', 'xfinity', 'store', 'person', 'go', 'xfinity', 'store', 'november', 'tell', 'contract', 'way', 'cancel', 'upgrade', 'go', 'original', 'plan', 'plain', 'simple', 'contract', 'sign', 'honor', 'state', 'xfinity', 'dishonest', 'trustworthy', 'look', 'change', 'isp', 'soon', 'possible', 'company', 'xfinity', 'deserve', 'paycheck', 'know']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_fa243_row7_col0\" class=\"data row7 col0\" >There is a malfunction on the DVR manager which is preventing us from adding more recordings. Customer service is fairly certain that the problem is from the signal from their system to ours, but protocol demands that they access our home before investigating that option. Since we work, that cannot be done until next Saturday. Customer service tech agreed that this seems illogical since logic would dictate that one would investigate the most probably malfunction first, but insists they must follow protocol. This is extremely frustrating. After 35 years as a customer of Comcast & their predecessors, I am investigating alternatives.</td>\n",
       "      <td id=\"T_fa243_row7_col1\" class=\"data row7 col1\" >['malfunction', 'dvr', 'manager', 'prevent', 'add', 'recording', 'customer', 'service', 'fairly', 'certain', 'problem', 'signal', 'system', 'protocol', 'demand', 'access', 'home', 'investigate', 'option', 'work', 'saturday', 'customer', 'service', 'tech', 'agree', 'illogical', 'logic', 'dictate', 'investigate', 'probably', 'malfunction', 'insist', 'follow', 'protocol', 'extremely', 'frustrating', 'year', 'customer', 'comcast', 'predecessor', 'investigate', 'alternative']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_fa243_row8_col0\" class=\"data row8 col0\" >Charges overwhelming. Comcast service rep was so ignorant and rude when I call to resolve my issue with my bill. I emailed Tom ** his rep was rude to me. None of the representative was helpful. They all just pass me on to other people. I am cutting my service with Comcast.</td>\n",
       "      <td id=\"T_fa243_row8_col1\" class=\"data row8 col1\" >['charge', 'overwhelming', 'comcast', 'service', 'rep', 'ignorant', 'rude', 'resolve', 'issue', 'bill', 'email', 'tom', 'rep', 'rude', 'representative', 'helpful', 'pass', 'people', 'cut', 'service', 'comcast']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa243_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_fa243_row9_col0\" class=\"data row9 col0\" >I have had cable, DISH, and U-verse, etc. in the past. All are eh... but you know what? Comcast takes the cake. I have never been driven to take time out of my day just to gripe online for all to see. But consumers, stay away! So my first terrible experience with Comcast is that they took 5 phones and 2 months to come out and bury the lines they had to lay in my front yard to get the cable needed into my house. Finally got someone when my special needs neighbor tripped and fell!Now 3 months into my contract, I have had my internet, phone, and TV go out for HOURS at a time. I would spend 3 hours on with a tech when it will come back up after the technician resets the router manually for the 3rd or 4th time. I have had it, I work from home occasionally and this is a huge inconvenience! The hardware is faulty, I understand that sometimes you get a lemon... but 3 months! 3 months! I have had it. Worst company ever. Crappy equipment and terrible customer service, and worse is the technicians they hire! Not a clue! Comcast should send a technician out here to switch out this equipment before I set a bonfire to it.</td>\n",
       "      <td id=\"T_fa243_row9_col1\" class=\"data row9 col1\" >['cable', 'dish', 'verse', 'etc', 'past', 'know', 'comcast', 'take', 'cake', 'drive', 'time', 'day', 'gripe', 'online', 'consumer', 'stay', 'away', 'terrible', 'experience', 'comcast', 'take', 'phone', 'month', 'come', 'bury', 'line', 'lay', 'yard', 'cable', 'need', 'house', 'finally', 'get', 'special', 'need', 'neighbor', 'trip', 'fell!now', 'month', 'contract', 'internet', 'phone', 'hour', 'time', 'spend', 'hour', 'tech', 'come', 'technician', 'reset', 'router', 'manually', 'time', 'work', 'home', 'occasionally', 'huge', 'inconvenience', 'hardware', 'faulty', 'understand', 'lemon', 'month', 'month', 'bad', 'company', 'crappy', 'equipment', 'terrible', 'customer', 'service', 'bad', 'technician', 'hire', 'clue', 'comcast', 'send', 'technician', 'switch', 'equipment', 'set', 'bonfire']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x34357b810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Textbereinigung (engl. text cleaning)\n",
    "import pandas as pd\n",
    "import spacy\n",
    "# Datensatz laden (Rohtext)\n",
    "df = pd.read_csv('../datasets/complaints_data.csv', usecols=[\"text\"], nrows=5627)  # Zeilenbegrenzung\n",
    "\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")                                                 # Fehlende Werte als leere Strings ersetzen\n",
    "\n",
    "## spaCy Pipeline (Textbereinigung, Tokenisierung, Lemmatisierung, )\n",
    "nlp = spacy.load(\"en_core_web_sm\")                                          # englisches Modell (small version)\n",
    "\n",
    "df[\"cleaned\"] = [[token.lemma_.lower()\n",
    "                  for token in doc\n",
    "                  # Filterblock      \n",
    "                  if (not token.is_stop and                                 # Standard:     Stopwort-Filter (allgemein)\n",
    "                      not token.is_punct and                                # Standard:     Satzzeichen-Filter\n",
    "                      not token.like_num and                                # Standard:     Nummern-Filter (einfache Zahlen)\n",
    "                                                                            # Individuell:  Stopwort-Filter (individuell)\n",
    "                      len(token.text) > 2 and                               # Individuell:  Wörter mit min. 2 Zeichen\n",
    "                      not any(char in token.text for char in ':/-–—') and   # Individuell:  Filter für Datums-/Zeit-Token\n",
    "                      token.is_ascii and                                    # Individuell:  Emoijs-Filter\n",
    "                      token.pos_ != \"PRON\" and                              # Individuell:  Pronomen-Filter\n",
    "                      token.text.lower() not in [\"meh\", \"ugh\"])]            # Individuell:  Wortfilter (ggf. Beschwerde, comcast, muss getuned werden)\n",
    "                 for doc in nlp.pipe(df[\"text\"], batch_size=50)]            # Batch-Verarbeitung (50 Texte parallel)\n",
    "\n",
    "print(f\"Verarbeitet: {len(df)} Beschwerden\")\n",
    "\n",
    "## Rechtschreibfehlerkorrektur (engl. spelling correction) - nicht umgesetzt\n",
    "## Eigennamenerkennung (engl. Named Entity Recognition - NER) - nicht umgesetzt\n",
    "\n",
    "# Ausgabe des Prozesses\n",
    "df.head(10)[[\"text\", \"cleaned\"]].style.set_properties(\n",
    "    **{'text-align': 'left', 'width': '1000px', 'max-width': '1500px', 'font-size': '12px'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526427e1",
   "metadata": {},
   "source": [
    "# Linguistische Verarbeitung (engl. linguistic processing)\n",
    "In der linguistischen Verarbeitung erfolgen lexikalische, syntaktische und semantische Verarbeitungsschritte, um Daten für die Datenvorbereitung (engl. data preparation) zu präparieren. Die Phase beinhaltet Schritte wie .... die Sprachdaten annotieren  und ..... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1259ab",
   "metadata": {},
   "source": [
    "## Vokabularerstellung (engl. vocabulary construction)\n",
    "Mapping der gefilterten Wörter (Token) zu IDs.\n",
    "\n",
    "Wortfrequenzschwellen (ist in SpaCy Pipeline)\n",
    "ggf. nur Nomen?\n",
    "\n",
    "## syntaktische Verarbeitung (engl. syntactic processing)\n",
    "## semantische Verarbeitung (engl. context processing)\n",
    "### Semantisches Parsen (engl. semantic parsing)\n",
    "#### Eigennamenerkennung (engl. Named Entity Recognition - NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vokabulargröße: 11947 Token (Wörter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>comcast</td>\n",
       "      <td>15230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9477</th>\n",
       "      <td>service</td>\n",
       "      <td>15149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10553</th>\n",
       "      <td>tell</td>\n",
       "      <td>7859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>call</td>\n",
       "      <td>7768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10730</th>\n",
       "      <td>time</td>\n",
       "      <td>6219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>customer</td>\n",
       "      <td>5726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>bill</td>\n",
       "      <td>5601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>internet</td>\n",
       "      <td>5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9289</th>\n",
       "      <td>say</td>\n",
       "      <td>5215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6893</th>\n",
       "      <td>month</td>\n",
       "      <td>4946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7822</th>\n",
       "      <td>phone</td>\n",
       "      <td>4883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7683</th>\n",
       "      <td>pay</td>\n",
       "      <td>4766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>cable</td>\n",
       "      <td>4657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>day</td>\n",
       "      <td>4108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>come</td>\n",
       "      <td>3829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>go</td>\n",
       "      <td>3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11804</th>\n",
       "      <td>work</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>charge</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>problem</td>\n",
       "      <td>3359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>get</td>\n",
       "      <td>3186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>box</td>\n",
       "      <td>2963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>account</td>\n",
       "      <td>2802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>ask</td>\n",
       "      <td>2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116</th>\n",
       "      <td>new</td>\n",
       "      <td>2630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11899</th>\n",
       "      <td>year</td>\n",
       "      <td>2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>hour</td>\n",
       "      <td>2536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>receive</td>\n",
       "      <td>2499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>company</td>\n",
       "      <td>2488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10959</th>\n",
       "      <td>try</td>\n",
       "      <td>2377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>need</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>tech</td>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>speak</td>\n",
       "      <td>2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>want</td>\n",
       "      <td>2150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9435</th>\n",
       "      <td>send</td>\n",
       "      <td>2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10510</th>\n",
       "      <td>technician</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>home</td>\n",
       "      <td>2085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5748</th>\n",
       "      <td>issue</td>\n",
       "      <td>2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>know</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>number</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11645</th>\n",
       "      <td>week</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>rep</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>supervisor</td>\n",
       "      <td>1924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6846</th>\n",
       "      <td>modem</td>\n",
       "      <td>1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>credit</td>\n",
       "      <td>1825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11555</th>\n",
       "      <td>wait</td>\n",
       "      <td>1714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>cancel</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>equipment</td>\n",
       "      <td>1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>fix</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6233</th>\n",
       "      <td>like</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10441</th>\n",
       "      <td>take</td>\n",
       "      <td>1550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  frequency\n",
       "ID                          \n",
       "2170      comcast      15230\n",
       "9477      service      15149\n",
       "10553        tell       7859\n",
       "1729         call       7768\n",
       "10730        time       6219\n",
       "2725     customer       5726\n",
       "1343         bill       5601\n",
       "5655     internet       5240\n",
       "9289          say       5215\n",
       "6893        month       4946\n",
       "7822        phone       4883\n",
       "7683          pay       4766\n",
       "1707        cable       4657\n",
       "2799          day       4108\n",
       "2184         come       3829\n",
       "4647           go       3633\n",
       "11804        work       3624\n",
       "1916       charge       3526\n",
       "8194      problem       3359\n",
       "4597          get       3186\n",
       "1508          box       2963\n",
       "426       account       2802\n",
       "970           ask       2740\n",
       "7116          new       2630\n",
       "11899        year       2582\n",
       "5153         hour       2536\n",
       "8560      receive       2499\n",
       "2233      company       2488\n",
       "10959         try       2377\n",
       "7069         need       2315\n",
       "10498        tech       2300\n",
       "9923        speak       2248\n",
       "11577        want       2150\n",
       "9435         send       2133\n",
       "10510  technician       2124\n",
       "5083         home       2085\n",
       "5748        issue       2055\n",
       "5998         know       2027\n",
       "7237       number       1984\n",
       "11645        week       1972\n",
       "8796          rep       1925\n",
       "10317  supervisor       1924\n",
       "6846        modem       1883\n",
       "2628       credit       1825\n",
       "11555        wait       1714\n",
       "1757       cancel       1704\n",
       "3799    equipment       1645\n",
       "4249          fix       1592\n",
       "6233         like       1585\n",
       "10441        take       1550"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vokabularerstellung (engl. vocabulary construction)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Umwandung des Dataframes in Liste (Vectorizer braucht Strings)\n",
    "df[\"text_cleaned\"] = [\" \".join(tokens) for tokens in df[\"cleaned\"]]\n",
    "\n",
    "vectorizer = CountVectorizer()                                                  # Vectorizer mit fit_transform: Vokabular + Matrix in einem (fit_transform lernt das Vokabular)\n",
    "X = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "## Vokabularerstellung\n",
    "vocabulary = vectorizer.get_feature_names_out()                                 # Vokabular extrahieren\n",
    "print(f\"Vokabulargröße: {len(vocabulary)} Token (Wörter)\")                      # Ausgabe der Vokabulargrö0e\n",
    "\n",
    "word_counts = X.sum(axis=0).A1                                                  # Häufigkeiten (Summe pro Spalte)\n",
    "vocab_df = pd.DataFrame({\n",
    "    'word': vocabulary,\n",
    "    'frequency': word_counts\n",
    "}).sort_values('frequency', ascending=False)\n",
    "vocab_df.index.name = 'ID'                                                      # Beschriftung ID-Spalte\n",
    "vocab_df.head(50)                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c217",
   "metadata": {},
   "source": [
    "# Datenvorbereitung (engl. data preparation)\n",
    "Im Rahmen der Datenverarbeitung werden Merkmale (engl. features) erzeugt und ausgewählt. Dies erfolgt durch Merkmalsgenerierung (engl. feature generation/featurization) und Merkmalsauswahl (engl. feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d2a6e",
   "metadata": {},
   "source": [
    "## Merkmalsgenerierung (engl. feature generation/featurization)\n",
    "Merkmalsgenerierung bezeichnet den Prozess, aus rohem oder vorverarbeitetem Text neue, informative Merkmale zu erzeugen. Unstrukturierte Daten werden dabei durch Merkmalskodierung (engl. feature encoding) in numerische oder kategorische Repräsentationen überführt, die Machine-Learning-Modelle nutzen können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2613238",
   "metadata": {},
   "source": [
    "# Vektorisierung (engl. vectorization)\n",
    "Als Vektorisierung wird die Merkmalskodierung (engl. feature encoding) von Textdaten bezeichnet. \n",
    "## häufigkeitsbasierter Vektor (engl. frequency vectors)\n",
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5b4b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Vokabular im korrekten Format: {wort: index}\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m {word: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Inverse Mapping: {index: wort}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m idx_to_word \u001b[38;5;241m=\u001b[39m {idx: word \u001b[38;5;28;01mfor\u001b[39;00m word, idx \u001b[38;5;129;01min\u001b[39;00m vocabulary\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Vektorisierung (engl. vectorization)\n",
    "## häufigkeitsbasierter Vektor (engl. frequency vectors)\n",
    "### TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Vokabular im korrekten Format: {wort: index}\n",
    "vocabulary = {word: idx for idx, word in enumerate(vocab_df['word'])}\n",
    "\n",
    "# Inverse Mapping: {index: wort}\n",
    "idx_to_word = {idx: word for word, idx in vocabulary.items()}\n",
    "\n",
    "# TF-IDF mit dem erstellten Vokabular\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Texte für TF-IDF vorbereiten (mit den bereinigten Token)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "print(f\"TF-IDF Matrix Form: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Sparse-Matrix\n",
    "result_data = []\n",
    "cx = tfidf_matrix.tocoo()\n",
    "for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "    result_data.append({\n",
    "        'Beschwerde'    : i+1,                                                                                      # Beschwerdeindex angepasst\n",
    "        'ID-Vokabular'  : j,\n",
    "        'Token (Wort)'  : idx_to_word[j], \n",
    "        'TF-IDF Score'  : v\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(result_data).sort_values(['Beschwerde', 'TF-IDF Score'], ascending=[True, False])\n",
    "print(result_df.head(30).to_string(index=False))                                                                    # pandas-Spalte ausblenden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55766c9",
   "metadata": {},
   "source": [
    "## Worteinbettung (engl. word embeddings)\n",
    "### Glove (global Vectors for word vectorization)\n",
    "GloVe steht für „Global Vectors for word vectorization“ (Globale Vektoren für die Wortdarstellung). Es handelt sich hierbei um eine weitere Vektorisierungsmethode, die häufig im NLP verwendet wird, um semantische und syntaktische Informationen in einem Vektorraum darzustellen. Während Word2Vec ein prädiktives Modell ist, ist GloVe ein unüberwachter Ansatz, der auf der Anzahl der Wörter basiert. Sie wurde entwickelt, weil Pennington et al. (2014) zu dem Schluss kamen, dass der Skip-Gram-Ansatz in Word2Vec die statistischen Informationen in Bezug auf das gemeinsame Vorkommen (Kookkurenz) von Wörtern nicht vollständig berücksichtigt. Deshalb haben sie den Skip-Gram-Ansatz mit den Vorteilen der Matrixfaktorisierung kombiniert. Das GloVe-Modell verwendet eine Kookkurenzmatrix, die Informationen über den Wortkontext enthält. Es hat sich gezeigt, dass das entwickelte Modell verwandte Modelle übertrifft, insbesondere bei der Erkennung von benannten Entitäten und Ähnlichkeitsaufgaben (Pennington et al., 2014).\n",
    "\n",
    "(iu. DLBDSEAIS01_D , 2023, p. 70)\n",
    "\n",
    "### Worteinbettung mit Word2Vec (engl. word embeddings)\n",
    "\n",
    "Bias im Datensatz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Word2Vec-Training...\n",
      "✓ Word2Vec-Modell trainiert!\n",
      "  Vokabular-Größe: 7085\n",
      "  Vektor-Dimension: 200\n",
      "✓ Embedding-Matrix erstellt: (11975, 200)\n",
      "✓ Dokumentenvektoren erstellt: (5627, 200)\n",
      "\n",
      "--- Ähnliche Wörter (Word2Vec) ---\n",
      "\n",
      "Ähnlich zu 'internet':\n",
      "  cutter: 0.615\n",
      "  wifi: 0.613\n",
      "  dsl: 0.581\n",
      "  blast: 0.581\n",
      "  landline: 0.578\n",
      "  1yr: 0.572\n",
      "  6mbs: 0.570\n",
      "  mbps: 0.557\n",
      "  advertised: 0.538\n",
      "  100mbps: 0.528\n",
      "\n",
      "Ähnlich zu 'comcast':\n",
      "  service: 0.464\n",
      "  arlington: 0.462\n",
      "  company: 0.455\n",
      "  shreveport: 0.453\n",
      "  inclination: 0.452\n",
      "  satisfied: 0.451\n",
      "  lawyers: 0.448\n",
      "  fraudulently: 0.443\n",
      "  24h: 0.439\n",
      "  wth: 0.428\n",
      "\n",
      "Ähnlich zu 'time':\n",
      "  hour: 0.682\n",
      "  countless: 0.633\n",
      "  dozen: 0.623\n",
      "  hrs: 0.587\n",
      "  numerous: 0.577\n",
      "  phone: 0.575\n",
      "  half: 0.572\n",
      "  multiple: 0.562\n",
      "  occasion: 0.557\n",
      "  course: 0.550\n"
     ]
    }
   ],
   "source": [
    "## Worteinbettung mit Word2Vec (engl. word embeddings)\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"Starte Word2Vec-Training...\")\n",
    "\n",
    "# 1. Vorbereitung: Tokenisierte Daten aus df[\"cleaned\"]\n",
    "sentences = df[\"cleaned\"].tolist()  # Liste von Listen mit Token\n",
    "\n",
    "# 2. Word2Vec-Modell trainieren\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=200,           # Dimensionalität der Wort-Vektoren\n",
    "    window=5,                  # Kontextfenster (±5 Wörter)\n",
    "    min_count=2,               # Wörter die weniger als 2x vorkommen ignorieren\n",
    "    workers=4,                 # Parallel-Verarbeitung\n",
    "    sg=0,                      # 0=CBOW, 1=Skip-gram\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"✓ Word2Vec-Modell trainiert!\")\n",
    "print(f\"  Vokabular-Größe: {len(w2v_model.wv)}\")\n",
    "print(f\"  Vektor-Dimension: {w2v_model.vector_size}\")\n",
    "\n",
    "# 3. Embedding-Matrix für das gesamte Vokabular erstellen\n",
    "embedding_matrix = np.zeros((len(vocab_df), 200))\n",
    "\n",
    "for idx, word in enumerate(vocab_df['word'].values):\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    else:\n",
    "        # Wort nicht im Modell: klein random Vektor\n",
    "        embedding_matrix[idx] = np.random.randn(200) * 0.01\n",
    "\n",
    "print(f\"✓ Embedding-Matrix erstellt: {embedding_matrix.shape}\")\n",
    "\n",
    "# 4. Dokumentenvektoren erstellen (Durchschnitt aller Wortvektoren)\n",
    "def get_document_vector(tokens, model, vector_size=200):\n",
    "    \"\"\"Berechnet den Durchschnittvektor eines Dokuments\"\"\"\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Dokumentenvektoren für alle Beschwerden\n",
    "document_embeddings = np.array([\n",
    "    get_document_vector(tokens, w2v_model, 200) \n",
    "    for tokens in df[\"cleaned\"]\n",
    "])\n",
    "\n",
    "print(f\"✓ Dokumentenvektoren erstellt: {document_embeddings.shape}\")\n",
    "\n",
    "# 5. Beispiele: Ähnliche Wörter finden\n",
    "print(\"\\n--- Ähnliche Wörter (Word2Vec) ---\")\n",
    "test_words = ['internet', 'comcast', 'time']\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        similar = w2v_model.wv.most_similar(word, topn=10)\n",
    "        print(f\"\\nÄhnlich zu '{word}':\")\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"  {similar_word}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{word}' nicht im Modell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5834439",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be303bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LATENT DIRICHLET ALLOCATION (LDA)\n",
      "================================================================================\n",
      "\n",
      "1. Überprüfe Datenverfügbarkeit...\n",
      "   ✓ 'cleaned' Spalte gefunden\n",
      "   ✓ 5627 Dokumente geladen\n",
      "   - Nicht-leere Dokumente: 5627\n",
      "   - Leere Dokumente: 0\n",
      "   - Beispiel-Tokens (erste 10): ['love', 'comcast', 'constant', 'update', 'internet', 'cable', 'crash', 'lot', 'night', 'day']\n",
      "   → Nach Filterung: 5627 nicht-leere Dokumente\n",
      "\n",
      "2. Erstelle Dictionary aus Token-Liste...\n",
      "   Wörter VOR Filter: 12628\n",
      "\n",
      "   Top 10 häufigste Wörter:\n",
      "     'comcast': 15215x\n",
      "     'service': 15169x\n",
      "     'tell': 7876x\n",
      "     'call': 7783x\n",
      "     'time': 6235x\n",
      "     'customer': 5732x\n",
      "     'bill': 5619x\n",
      "     'internet': 5247x\n",
      "     'say': 5219x\n",
      "     'month': 4956x\n",
      "\n",
      "   Wende MINIMALE Filter an:\n",
      "   - no_below=1 (Wort kommt mindestens 1x vor)\n",
      "   - no_above=0.99 (Wort kommt in max. 99% der Dokumente vor)\n",
      "   Wörter NACH Filter: 12628 (von 12628)\n",
      "\n",
      "3. Erstelle Corpus (Bag-of-Words)...\n",
      "   Corpus Größe: 5627 Dokumente\n",
      "   Nicht-leere Dokumente im Corpus: 5627\n",
      "   Durchschn. Terme pro Dokument: 56.7\n",
      "   Min/Max Terme pro Dokument: 3/333\n",
      "\n",
      "   Sample Corpus-Dokumente (erste 3):\n",
      "     Doc 0: 23 Terme - [('ago', 1), ('cable', 1), ('channel', 1), ('comcast', 1), ('constant', 1)]\n",
      "     Doc 1: 27 Terme - [('comcast', 2), ('internet', 3), ('switch', 1), ('area', 1), ('assignment', 1)]\n",
      "     Doc 2: 68 Terme - [('comcast', 3), ('think', 1), ('wish', 1), ('work', 1), ('bad', 1)]\n",
      "\n",
      "4. Trainiere LDA Modell...\n",
      "   Trainings-Daten: 5627 Dokumente mit 12628 Wörtern im Dictionary\n",
      "   Parameter:\n",
      "   - Topics: 10\n",
      "   - Passes: 20\n",
      "   - Iterations: 400\n",
      "   (Dies kann 5-15 Minuten dauern...)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ LDA MODELL ERFOLGREICH TRAINIERT!\n",
      "================================================================================\n",
      "\n",
      "Modell-Statistiken:\n",
      "  - Dictionary Größe: 12628 Wörter\n",
      "  - Corpus Größe: 5627 Dokumente\n",
      "  - Anzahl Topics: 10\n",
      "\n",
      "================================================================================\n",
      "TOP WORDS PER TOPIC (mit Gewichtungen)\n",
      "================================================================================\n",
      "\n",
      "Topic 0:\n",
      "  service                   0.0340\n",
      "  comcast                   0.0301\n",
      "  company                   0.0254\n",
      "  time                      0.0202\n",
      "  bad                       0.0183\n",
      "  customer                  0.0167\n",
      "  business                  0.0139\n",
      "  area                      0.0111\n",
      "  internet                  0.0107\n",
      "  like                      0.0106\n",
      "\n",
      "Topic 1:\n",
      "  call                      0.0369\n",
      "  comcast                   0.0266\n",
      "  tell                      0.0263\n",
      "  service                   0.0261\n",
      "  come                      0.0219\n",
      "  day                       0.0212\n",
      "  time                      0.0200\n",
      "  phone                     0.0198\n",
      "  say                       0.0184\n",
      "  work                      0.0146\n",
      "\n",
      "Topic 2:\n",
      "  box                       0.0328\n",
      "  comcast                   0.0270\n",
      "  channel                   0.0204\n",
      "  cable                     0.0194\n",
      "  pay                       0.0150\n",
      "  tell                      0.0137\n",
      "  customer                  0.0137\n",
      "  service                   0.0134\n",
      "  go                        0.0106\n",
      "  like                      0.0097\n",
      "\n",
      "Topic 3:\n",
      "  service                   0.0353\n",
      "  comcast                   0.0254\n",
      "  customer                  0.0237\n",
      "  time                      0.0212\n",
      "  phone                     0.0195\n",
      "  problem                   0.0162\n",
      "  hour                      0.0146\n",
      "  email                     0.0139\n",
      "  issue                     0.0134\n",
      "  try                       0.0120\n",
      "\n",
      "Topic 4:\n",
      "  service                   0.0414\n",
      "  month                     0.0409\n",
      "  internet                  0.0310\n",
      "  comcast                   0.0297\n",
      "  bill                      0.0259\n",
      "  pay                       0.0206\n",
      "  year                      0.0185\n",
      "  customer                  0.0173\n",
      "  charge                    0.0163\n",
      "  price                     0.0162\n",
      "\n",
      "Topic 5:\n",
      "  comcast                   0.0554\n",
      "  service                   0.0550\n",
      "  cable                     0.0248\n",
      "  problem                   0.0231\n",
      "  internet                  0.0175\n",
      "  technician                0.0167\n",
      "  work                      0.0108\n",
      "  home                      0.0089\n",
      "  time                      0.0083\n",
      "  customer                  0.0082\n",
      "\n",
      "Topic 6:\n",
      "  vcr                       0.0170\n",
      "  parent                    0.0134\n",
      "  winter                    0.0088\n",
      "  dark                      0.0086\n",
      "  practice                  0.0082\n",
      "  locked                    0.0082\n",
      "  outraged                  0.0080\n",
      "  i'm                       0.0077\n",
      "  inquiry                   0.0074\n",
      "  gerald                    0.0073\n",
      "\n",
      "Topic 7:\n",
      "  say                       0.0292\n",
      "  modem                     0.0268\n",
      "  tech                      0.0225\n",
      "  tell                      0.0221\n",
      "  comcast                   0.0198\n",
      "  come                      0.0186\n",
      "  call                      0.0180\n",
      "  internet                  0.0166\n",
      "  work                      0.0166\n",
      "  box                       0.0164\n",
      "\n",
      "Topic 8:\n",
      "  november                  0.0579\n",
      "  a.m.                      0.0362\n",
      "  house                     0.0328\n",
      "  rewire                    0.0308\n",
      "  p.m.                      0.0302\n",
      "  signal                    0.0215\n",
      "  say                       0.0209\n",
      "  voice                     0.0202\n",
      "  goodlow                   0.0189\n",
      "  supervisor                0.0177\n",
      "\n",
      "Topic 9:\n",
      "  comcast                   0.0396\n",
      "  bill                      0.0391\n",
      "  service                   0.0383\n",
      "  tell                      0.0249\n",
      "  account                   0.0225\n",
      "  pay                       0.0213\n",
      "  call                      0.0202\n",
      "  credit                    0.0175\n",
      "  charge                    0.0168\n",
      "  receive                   0.0152\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Latent Dirichlet Allocation (LDA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Überprüfe Datenverfügbarkeit...\")\n",
    "\n",
    "# Prüfe, ob 'cleaned' Spalte existiert\n",
    "if \"cleaned\" in df.columns:\n",
    "    print(\"   ✓ 'cleaned' Spalte gefunden\")\n",
    "    cleaned_tokens = df[\"cleaned\"].tolist()\n",
    "else:\n",
    "    print(\"   ✗ 'cleaned' Spalte nicht gefunden!\")\n",
    "    print(\"   Nutze 'text_cleaned' und konvertiere...\")\n",
    "    cleaned_tokens = [tokens.split() for tokens in df[\"text_cleaned\"]]\n",
    "\n",
    "print(f\"   ✓ {len(cleaned_tokens)} Dokumente geladen\")\n",
    "\n",
    "# Überprüfe auf leere Dokumente\n",
    "non_empty_count = sum(1 for doc in cleaned_tokens if len(doc) > 0)\n",
    "empty_count = len(cleaned_tokens) - non_empty_count\n",
    "print(f\"   - Nicht-leere Dokumente: {non_empty_count}\")\n",
    "print(f\"   - Leere Dokumente: {empty_count}\")\n",
    "\n",
    "# Debug: Zeige erste Tokens\n",
    "if non_empty_count > 0:\n",
    "    sample_doc = next((doc for doc in cleaned_tokens if len(doc) > 0), None)\n",
    "    if sample_doc:\n",
    "        print(f\"   - Beispiel-Tokens (erste 10): {sample_doc[:10]}\")\n",
    "\n",
    "if non_empty_count == 0:\n",
    "    print(\"   ✗ FEHLER: Alle Dokumente sind leer!\")\n",
    "else:\n",
    "    # Entferne leere Dokumente\n",
    "    cleaned_tokens = [doc for doc in cleaned_tokens if len(doc) > 0]\n",
    "    print(f\"   → Nach Filterung: {len(cleaned_tokens)} nicht-leere Dokumente\")\n",
    "    \n",
    "    print(f\"\\n2. Erstelle Dictionary aus Token-Liste...\")\n",
    "    dictionary = Dictionary(cleaned_tokens)\n",
    "    print(f\"   Wörter VOR Filter: {len(dictionary)}\")\n",
    "    \n",
    "    # Debug: Zeige häufigste Wörter VOR Filter\n",
    "    print(f\"\\n   Top 10 häufigste Wörter:\")\n",
    "    word_freq = {}\n",
    "    for doc in cleaned_tokens:\n",
    "        for word in doc:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for word, freq in sorted_words:\n",
    "        print(f\"     '{word}': {freq}x\")\n",
    "    \n",
    "    # MINIMALE FILTER: So mild wie möglich\n",
    "    print(f\"\\n   Wende MINIMALE Filter an:\")\n",
    "    print(f\"   - no_below=1 (Wort kommt mindestens 1x vor)\")\n",
    "    print(f\"   - no_above=0.99 (Wort kommt in max. 99% der Dokumente vor)\")\n",
    "    \n",
    "    initial_dict_size = len(dictionary)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.99, keep_n=100000)\n",
    "    print(f\"   Wörter NACH Filter: {len(dictionary)} (von {initial_dict_size})\")\n",
    "    \n",
    "    if len(dictionary) == 0:\n",
    "        print(f\"\\n   ⚠️ WARNUNG: Dictionary ist leer nach Filter!\")\n",
    "        print(f\"   → Verwende Dictionary OHNE Filter...\")\n",
    "        dictionary = Dictionary(cleaned_tokens)\n",
    "        print(f\"   Dictionary (kein Filter): {len(dictionary)} Wörter\")\n",
    "    \n",
    "    if len(dictionary) > 0:\n",
    "        print(f\"\\n3. Erstelle Corpus (Bag-of-Words)...\")\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in cleaned_tokens]\n",
    "        \n",
    "        # Debug: Corpus-Statistiken\n",
    "        corpus_lengths = [len(doc) for doc in corpus]\n",
    "        non_empty_corpus = sum(1 for doc in corpus if len(doc) > 0)\n",
    "        \n",
    "        print(f\"   Corpus Größe: {len(corpus)} Dokumente\")\n",
    "        print(f\"   Nicht-leere Dokumente im Corpus: {non_empty_corpus}\")\n",
    "        print(f\"   Durchschn. Terme pro Dokument: {sum(corpus_lengths)/len(corpus_lengths):.1f}\")\n",
    "        print(f\"   Min/Max Terme pro Dokument: {min(corpus_lengths)}/{max(corpus_lengths)}\")\n",
    "        \n",
    "        # Zeige Sample Corpus-Einträge\n",
    "        sample_corpus = [doc for doc in corpus if len(doc) > 0][:3]\n",
    "        print(f\"\\n   Sample Corpus-Dokumente (erste 3):\")\n",
    "        for i, doc in enumerate(sample_corpus):\n",
    "            terms = [(dictionary[term_id], freq) for term_id, freq in doc[:5]]\n",
    "            print(f\"     Doc {i}: {len(doc)} Terme - {terms}\")\n",
    "        \n",
    "        if non_empty_corpus > 0:\n",
    "            # Entferne Dokumente mit 0 Termen für LDA\n",
    "            filtered_corpus = [doc for doc in corpus if len(doc) > 0]\n",
    "            filtered_tokens = [tokens for tokens, doc in zip(cleaned_tokens, corpus) if len(doc) > 0]\n",
    "            \n",
    "            print(f\"\\n4. Trainiere LDA Modell...\")\n",
    "            print(f\"   Trainings-Daten: {len(filtered_corpus)} Dokumente mit {len(dictionary)} Wörtern im Dictionary\")\n",
    "            print(f\"   Parameter:\")\n",
    "            print(f\"   - Topics: 10\")\n",
    "            print(f\"   - Passes: 20\")\n",
    "            print(f\"   - Iterations: 400\")\n",
    "            print(f\"   (Dies kann 5-15 Minuten dauern...)\\n\")\n",
    "            \n",
    "            try:\n",
    "                model = LdaModel(\n",
    "                    corpus=filtered_corpus,\n",
    "                    id2word=dictionary.id2token,\n",
    "                    num_topics=10,\n",
    "                    random_state=42,\n",
    "                    chunksize=2000,\n",
    "                    passes=20,\n",
    "                    iterations=400,\n",
    "                    per_word_topics=True,\n",
    "                    minimum_probability=0.0,\n",
    "                    alpha='auto',\n",
    "                    eta='auto'\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"✓ LDA MODELL ERFOLGREICH TRAINIERT!\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"\\nModell-Statistiken:\")\n",
    "                print(f\"  - Dictionary Größe: {len(dictionary)} Wörter\")\n",
    "                print(f\"  - Corpus Größe: {len(filtered_corpus)} Dokumente\")\n",
    "                print(f\"  - Anzahl Topics: 10\")\n",
    "                \n",
    "                # Zeige Top Words pro Topic\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(\"TOP WORDS PER TOPIC (mit Gewichtungen)\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                for idx in range(10):\n",
    "                    terms = model.show_topic(idx, topn=10)\n",
    "                    print(f\"\\nTopic {idx}:\")\n",
    "                    for term, weight in terms:\n",
    "                        print(f\"  {term:25s} {weight:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"\\n✗ FEHLER beim LDA-Training:\")\n",
    "                print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"\\n✗ FEHLER: Alle Dokumente im Corpus sind leer!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ FEHLER: Dictionary ist leer und konnte nicht rekonstruiert werden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188216dc",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23031072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Projekt_Advanced-Data-Analysis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BERTOPIC - Topic Modeling\n",
      "================================================================================\n",
      "   ✓ 5627 Dokumente aus 'cleaned' erstellt\n",
      "\n",
      "✓ Daten vorbereitet:\n",
      "  Anzahl Dokumente (Beschwerden): 5627\n",
      "  Erste 10 Beschwerden:\n",
      "    1. love comcast constant update internet cable crash lot night day channel work dem...\n",
      "    2. comcast bad internet provider take online class multiple time late assignment po...\n",
      "    3. negative star star review work industry bad customer service comcast matter mone...\n",
      "    4. bad experience far install problem show schedule service appointment extreme dif...\n",
      "    5. check contract sign comcast advertised offer match contract issue sign 150mbps i...\n",
      "    6. thank god change dish give awesome pricing super people deal actually understand...\n",
      "    7. long time customer xfinity isp local walmart november customer representative xf...\n",
      "    8. malfunction dvr manager prevent add recording customer service fairly certain pr...\n",
      "    9. charge overwhelming comcast service rep ignorant rude resolve issue bill email t...\n",
      "    10. cable dish verse etc past know comcast take cake drive time day gripe online con...\n",
      "\n",
      "2. Starte BERTopic-Training...\n",
      "   (Dies kann einige Minuten dauern...)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1574.58it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BERTOPIC - Topic Modeling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Daten vorbereiten\n",
    "\n",
    "docs = [\" \".join(tokens) for tokens in df[\"cleaned\"]]               # Datenaufbereitung für BERTopic (bereinigte Beschwerden)\n",
    "print(f\"   ✓ {len(docs)} Dokumente aus 'cleaned' erstellt\")\n",
    "\n",
    "if docs is not None and len(docs) > 0:\n",
    "    print(f\"\\n✓ Daten vorbereitet:\")\n",
    "    print(f\"  Anzahl Dokumente (Beschwerden): {len(docs)}\")\n",
    "    print(f\"  Erste 10 Beschwerden:\")\n",
    "    for i, doc in enumerate(docs[:10]):\n",
    "        preview = doc[:80] + \"...\" if len(doc) > 80 else doc\n",
    "        print(f\"    {i+1}. {preview}\")\n",
    "\n",
    "    print(f\"\\n2. Starte BERTopic-Training...\")\n",
    "    print(\"   (Dies kann einige Minuten dauern...)\\n\")\n",
    "\n",
    "    topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"✓ BERTopic abgeschlossen!\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"  Topics gefunden: {len(set(topics)) - 1}\")  # -1 für Outlier\n",
    "    print(f\"  Dokumente verarbeitet: {len(topics)}\")\n",
    "else:\n",
    "    print(\"\\n✗ BERTopic konnte nicht trainiert werden (keine Daten)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3abb502d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtopic_model\u001b[49m.get_topic(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "topic_model.get_topic(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projekt_Advanced-Data-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
