# Aufgabe 1.1: NLP-Techniken anwenden, um eine Textsammlung zu analyieren
Die linguistische Datenverarbeitung (LDV, engl. natural language processing â€“ NLP) lÃ¤sst sich in drei zentrale Bereiche unterteilen: die automatische Spracherkennung (engl. automatic speech recognition â€“ ASR), das VerstÃ¤ndnis natÃ¼rlicher Sprache (engl. natural language understanding â€“ NLU) sowie die natÃ¼rliche Sprachgenerierung (engl. natural language generation â€“ NLG). Dieses Projekt fokussiert sich auf NLU, also die Analyse und Interpretation von Textinhalten.

Ziel der Aufgabe ist es NLP-Techniken auf einem organisch entstandenen Datensatz mit Beschwerden anzuwenden und so die am hÃ¤ufigsten angesprochenen Themen aus den Beschwerdetexten zu extrahieren. Die hierdurch gewonnenen Informationen sollen im Anschluss fÃ¼r EntscheidungstrÃ¤ger (einer Ã¶rtlichen Stadtverwaltung) aufbereitet werden.<br>

Das schriftliche Konzept hierzu soll die Schritte der NLP-Datenverarbeitung mit Python darlegen. Dabei sollen zwei Techniken zur Vektorisierung der Beschwerdetexte sowie zwei AnsÃ¤tze zur Extraktion von Themen aus dem Datensatz genannt und die verwendeten Python-Bibliotheken kurz aufgefÃ¼hrt werden.

Durch einen Klick auf â–º werden ErlÃ¤uterungen und Unterschritte sichtbar. Die verwendeten Softwarebibliotheken werden in folgender Form hervorgehoben: 

<div style="margin-left: 2em;">
  <code>Bibliothek 1</code>&nbsp;<code>Bibliotek 2</code><br>
</div>


`<paketname>.<funktionsname>(<funktionsargumente>)`


- Clustering von Beschwerden.

## Projektstruktur
### Ordnerstruktur
```markdown
â”œâ”€â”€ dataset/   # gewÃ¤hlter Datensatz
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / complaints_data.csv
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / complaints_data_cleaned.csv
â”œâ”€â”€ src/       # Python-Skripte
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / Datensatzvorverarbeitung (engl. dataset pipeline)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / Datenverarbeitung (engl. data processing)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / Datennachverarbeitung (engl. data post-processing)
â”œâ”€â”€ docs/      # Ãœbersichten (Arbeitsunterlagen, Datensatzauswertungen, Pipeline)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / 1 -
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / 2 - 
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / 3 - 
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ / X - 
â””â”€â”€ README.md
```

## Installation (engl. setup)
Als Programmiersprache wird Python in der Version 3.9 genutzt. Als Entwicklungsumgebung (engl. integrated development environment - IDE) wird Visual Studio Code (VSCode) genutzt.

### Vorbereitende Installation (engl. preparatory setup)

###### Laufzeitumgebung (engl. runtime environment)
Als virtuelle Umgebungen stehen in Python "venv" und "conda" zur VerfÃ¼gung. FÃ¼r die bestmÃ¶gliche KompatibilitÃ¤t habe ich mich fÃ¼r einen hybriden Virtualisierungsansatz entschieden. 
GroÃŸe wissenschaftliche Pakete wie PyTorch und spaCy werden dabei via conda, spezialisierte via pip installiert.

```console
conda env create -f environment.yml
conda activate ada-project
```


###### Datenvalidierung (engl. data validation)
Die Spaltenbeschriftung der textfÃ¼hrende Spalte des gewÃ¤hlten Datensatzes muss mit "text" benannt sein damit das Skript den Datensatz durch das externe Modul "AITextDetector.py" verarbeiten kann.
<ul>
<li><ins>KI Detektor</ins></li>
Python in einer Version 3.8+

```python
`pip install transformers`
`pip install torch`
```
Jai Soorya N, K. (2023). AI-Text-Detector-python [Software]. https://github.com/Kishanjaisoorya/AI-Text-Detector-python<br><br>

_________________________________________________________________________________________________________________________________________________________

## Konzeption
Die ausgearbeitete Konzeption lÃ¤sst sich grob in 3 Phasen einteilen. Datensatzvorverarbeitung (engl. dataset pipeline), Datenverarbeitung (engl. data processing) und Datennachverarbeitung (engl. data post-processing).

## âšª Datensatzverarbeitung (engl. dataset pipeline)
<img src="1 - Datensatzverarbeitung (engl. dataset pipeline).jpg" width="1200">

### Datensatzakquisition (engl. dataset acquisition)
In der Phase der Datenaquisition werden DatensÃ¤tze gesucht und bewertungsbasiert ausgewÃ¤hlt. Hierzu wird ein trichterfÃ¶rmiger, vier stufiger Prozess bestehend aus Datensatzrecherche, â€“sammlung, â€“prÃ¼fung und â€“auswahl durchlaufen, um dem Korpus fÃ¼r die NLP-Pipeline zu bestimmen.<br>
<ol>
    <details>
      <summary>âšª Datensatzrecherche (engl. dataset research)</b></summary>
      <i>Es wird eine Onlinerecherche auf verschiedenen Datenportalen (Kaggle, GitHub, GovData, MendeleyData, u.A.) durchgefÃ¼hrt und nach geeigneten deutschen und englischen DatensÃ¤tzen gesucht.</i><br>
    </details>
</ol>
<ol>
    <details>
      <summary>âšª Datensatzsammlung (engl. dataset collection)</summary>
      <i>Offensichtlich synthetisch erzeugte DatensÃ¤tze werden ignoriert. Datenquellen mit vermutetem organischen Ursprung werden im CSV-Datenformat manuell oder per API heruntergeladen und lokal gespeichert.</i><br>
    </details>
</ol>
<ol>    
    <details>
      <summary>âšª DatensatzprÃ¼fung (engl. dataset check)</summary>
      <i>Die gesammelten DatensÃ¤tze werden anhand deines vortrainierten, extern entwickelten [KI-Detektors](https://github.com/Kishanjaisoorya/AI-Text-Detector-python) auf synthetisch erzeugte Instanzen (engl. samples) geprÃ¼ft und mit Labels (REAL / FAKE / ERROR) getaggt. [genutze Technik - BERT ? BeschrÃ¤nkung der Zeichen] BERT-base
      </i><br><br>
      <div style="margin-left: 2em;">
       <code>transformers</code>&nbsp;<code>torch</code><br><br>
    </details>
    <details>
      <summary>âšª Datensatzauswahl (engl. dataset selection)</summary>
      <i>Durch eine HÃ¤ufigkeitsauswertung der Label wird der Datensatz mit dem prozentual hÃ¶chsten Anteil an organischen (REAL-Label) Instanzen die die Formel:</i><br>
      <br>
      $$\%\text{ organisch} = \left(\frac{REAL}{REAL + FAKE + ERROR}\right) \cdot100$$
      <br>
      <br><i>ausgewertet. Die Wahrscheinlichkeit eines organischen Ursprungs erscheint hÃ¶her, je hÃ¶her der Prozentsatz organisch identifizierter Instanzen im VerhÃ¤ltnis zum Gesamtdatensatz ist. Kann ein Datensatz nicht in angemessener Zeit (30 min.) durch das Modell verarbeitet werden, wird die PrÃ¼fung abgebrochen und die Bewertung als n/a markiert. Der Datensatz flieÃŸt dann nicht in den Ergebnisvergleich ein. Der Datensatz mit der prozentualen hÃ¶chsten Bewertung wird als Korpus fÃ¼r die nachfolgenden Schritte genutzt. Ãœbersteigen seine Instanzen die Schwelle von 2000, wird der Datensatz fÃ¼r die folgenden Verarbeitungschritte darauf begrenzt.<br><br></i>

| Nr.| Bezeichnung                        | Bewertung | GrÃ¶ÃŸe     |Quelle                     |
|----|------------------------------------|-----------|-----------|---------------------------|
| 01 | Consumer_Complaints.csv            | n/a       | 59,40  MB |[^01] &nbsp; Kaggle        |
| 02 | rows.csv                           | n/a       | 176    MB |[^02] &nbsp; Kaggle        |
| 03 | Consumer_Complaints.csv            | 13,5 %    | 107,0  MB |[^03] &nbsp; Kaggle/GovData|
| 04 | complaints_processed.csv           | 64,72 %   | 19,8   MB |[^04] &nbsp; Kaggle        |
| 05 | complaints_data.csv                | 82 %      | 7,20   MB |[^05] &nbsp; GitHub        |
| 06 | user_complaints                    | 0,69 %    | 229,0  kB |[^06] &nbsp; GitHub        |
| 07 | consumer_complaints.csv            | n/a       | 175,39 MB |[^07] &nbsp; Kaggle        |
| 08 | Complaints_Reports_Data.sql        | n/a       | 3,28   MB |[^08] &nbsp; MendeleyData  |
| 09 | chatgpt_reviews.csv                | 35,03 %   | 119,9  MB |[^09] &nbsp; GitHub        |
| 10 | dataset-tickets-multi-lang3-4k.csv | n/a       | 6,87   MB |[^10] Kaggle               |

Es wird Datensatz Nr. 05[^05] *"complaints_data.csv"* gewÃ¤hlt da dieser ein Scoring von 82 % erreicht.

  </details>
</ol>

### Datensatzsichtung (engl. dataset inspection)
In der Phase der Datensatzsichtung wird eine Explorative Datenanalyse (engl. exploratory data analysis) durchgefÃ¼hrt, um Muster, QualitÃ¤tsprobleme und Strukturen des Datensatzes zu erkennen, damit diese zur Datensatzaufbereitung (engl. dataset preparation) und in den anschlieÃŸenden Phasen berÃ¼cksichtigt werden kÃ¶nnen. Die Datensatzsichtung wurde mittels des selbstgeschrieben Python-Skripts "Datensatzsichtung (engl. dataset inspection).ipynb" durchgefÃ¼hrt, um den gewÃ¤hlten Datensatz zur eine Datentrukturanalyse, sowie der Analyse der strukutierten und unstrukturierten besser zu verstehen.

<ol>   
  <details>
  <summary>âšª Datenstrukturanalyse (engl. data structure analysis)</summary>
  <i>Die Datenstruktur des gewÃ¤hlten Datensatzes ist ein Spezialfall einer
  "Delimiter Separated Value"-Datei welche als Trennzeichen Komma (engl. comma)
  nutzt (Klein, 2023, p. 261-262).</i>
  <sup id="ref-12"><a href="#fn-12">[12]</a></sup><i> Diese sog. CSV-Datei verfÃ¼gt im vorliegenden Fall Ã¼ber eine Header und
  5659 Zeilen, welche in 4 Spalten organisiert wurden.</i><br><br>

|author                             |posted_on                 |rating |text              |
|-----------------------------------|--------------------------|-------|------------------|
|`<Benutzername>`of`<US-Ortsangabe>`|`<Monat>`.`<Tag>`,`<Jahr>`|`<0-5>`|`<Beschwerdetext>`|

<i>Die in der Datei enthaltenen Daten lassen sich in <ins>struktierte Daten</ins> und <ins>unstrukturierte Daten</ins> unterteilen, wobei letztere als Input fÃ¼r die NLP-Pipeline genutzt wird.<br></i>
</ol>
  </details>
<ol>   
  <details>
  <summary>âšª Explorative Datenanalyse (engl. exploratory data analysis)</summary>
  <i>In der EDA werden Textdaten untersucht, um Muster, QualitÃ¤tsprobleme und Strukturen zu erkennen.</i><br><br>
  <ul>
    <li><ins>EDA der struktierte Daten</ins></li>
    In strukturierter Form liegen die Spalten "author", "posted_on" und "rating" vor. Diesen Informationen ist gemein, dass sie ohne grÃ¶ÃŸere Vorverarbeitung direkt weiterverarbeitet werden kÃ¶nnen, da die Informationen meist in einheitlicher (normalisierter Form) vorliegen.<br><br>
    <ul>
        <li><ins> "author"</ins>

  > Die Zeilen der Spalte enthalten jeweils den alphanumerischen`<Benutzernamen>` des Beschwerdeverfassers sowie eine, durch ein "of" getrente, US-Ortsangabe welche im Format `<Ortsname "of" US-Bundesstaat>` vorliegt. 
  

  Die Datenexploration durch eine Ortsdatenanalyse zeigte, dass 3 US-Ortsangaben ['BC', 'ON', 'PE'] ungÃ¼ltig sind, aus 17 Bundesstaaten eine dreistellige Anzahl an Beschwerden zu verzeichnen ist, von deren auf ['FL: 778', 'CA: 554', 'GA: 414'] entfallen und aus 5 Bundesstaaten ['IA', 'MT', 'OK', 'RI', 'SD'] keine Beschwerden erfasst wurden.<br>
        <li><ins>"posted_on"</ins><br>

  > Die Zeilen der Spalte "posted_on" enhalten Datumsangaben mit alphabetisch abgekÃ¼rzer Monatsangabe Ã¼ber einen Zeitraum von 16 Jahren im amerikanischem Format `<Monat>`.`<Tag>`,`<Jahr>`. 
  
   Im Rahmen der Datenexploration wurde eine Zeitdatenanlyse durchgefÃ¼hrt welche Muster in der (jÃ¤hrlichen, monatlichen, wÃ¶chentlichen) Verteilung der Beschwerden im Datensatz Ã¼ber den Zeitraum vom 31.07.2000 bis 22.11.2016 zeigte.

   <ins>jÃ¤hrliche Verteilung:</ins> Die EDA zeigt, dass die meisten Beschwerden im Jahr 2015 erfolgten sind. 
   
   <ins>monatliche Verteilung:</ins> Die EDA zeigte weiter, dass die meisten Beschwerden im August (540) und wie wenigsten Beschwerden im April (369) abgesetzt wurden, wobei der Datensatz ein saisonales Muster zeigt. 
   
   <ins>wÃ¶chentliche Verteilung:</ins> Die Verteilung der Beschwerden aufgeschlÃ¼sselt nach Wochentagen zeigt, dass die meisten Beschwerden mittwochs (993), dienstags (960) und donnerstags (861), gefolgt von montags (820) und freitags (802) abgesezt wurden, wohingegen an den Tagen der Wochenenden Samstag (659) und Sonntags (564) weniger Beschwerden zu verzeichnen sind. Dieses Muster deutet ebenfalls auf einen organischen Ursprung des Datensatzes hin.

  <li><ins>"rating"</ins><br>
  
  > Die Zeilen der Spalte "rating" enthalten Bewertungen auf einer Skala `<0-5>`.<br>

  Die Anzahl der Bewertungen nach "rating" ist wie folgt verteilt [rating: 0=1560 (27.57%); 1=3734 (65.98%); 2=260 (4.59%); 3=54 (0.95%); 4=19 (0.34%); 5=32 (0.57%)] was einen Ãœberhang niedriger Bewertungen zeigt. Dies weist ebenfalls auf einen organischen Datensatz hin, da Beschwerden grundsÃ¤tzlich negativ sind. 

  </ul>
</ul>

<ul>
  <li><ins>EDA der unstruktuierte Daten</ins></li>
  Unstrukturierte Daten sind Informationen, die in einer nicht identifizierbaren Datenstruktur vorliegen. Ein typisches Beispiel dafÃ¼r sind natÃ¼rlichsprachliche Texte wie sie in der Spalte "text" vorhanden sind.<br><br>
  <ul>
  <li><ins>"text"</ins></li>
  
  > In den Zeilen der Spalte "text" befindet sich ein englischer `<Beschwerdetext>`. Er besteht aus WÃ¶rtern (Zeichenketten, sprich Folgen von Buchstaben, Ziffern, Satzzeichen, ect.) die konkateniert SÃ¤tze bilden die Zeit- und Datumsangaben in unterschiedlichen Formatierungen, GroÃŸschreibungen, AufzÃ¤hlungen und Sonderzeichen enthalten was bei der Sprachverarbeitung zu beachten ist.<br>  
  
  Die EDA zeigte dass die Texte im Median aus 1.239,94 Zeichen bestehen.
  </ul>
</ul>

<ul>
  <li><ins>fehlerhafte Daten</ins></li>
  Im Zuge der Datenexploration ist aufgefallen, dass sich sowohl in den unstrukturiert vorliegenden Daten 30 NaNs, nicht vorhandene Informationen befanden. die bereinigt werden mÃ¼ssen, um Verzerrungen in der spÃ¤teren Modellbildung zu vermeiden. Es wurden 30 NaNs in der Spalte 'text' und fehlende Daten und (XXX) Duplikate gefunden<br>
  <ul>
  <li><ins>XXXXX</ins></li>
  
  > Die Analyse fehlender Daten zeigte, dass der Datensatz <br>  
  </ul>
</ul>
</ol>
  </details>

## Datensatzaufbereitung (engl. dataset preparation)
In der Phase der Datensatzbereinigung werden die in der EDA gewonnen Erkenntnisse genutzt, um den Datensatz fÃ¼r den Anwendungsfall vorzubereiten. Hierzu wird eine Datenbereinigung sowie eine Datenvalidierung durchgefÃ¼hrt, wodurch diejenigen Daten bestimmt werden, die weiter verarbeitet werden.
<ol>
    <details>
      <summary>âšª Datensatzbereinigung (engl. dataset cleaning)</b></summary>
      <i>
      
###### Fehlwertbehandlung
Die Behandlung von Fehlwerten wie NaNs (Not a Number) oder NaTs (Not a Text) kann durch listenweisen Fallausschluss, durch welchen Zeilen ohne Text oder Text unter einer MindeslÃ¤nge entfernt wird oder Imputation, das AuffÃ¼llen oder Ersetzen fehlender oder unvollstÃ¤ndiger Textelemente durch geschÃ¤tzte Werte, damit der Datensatz fÃ¼r Modelltraining oder Analyse vollstÃ¤ndig nutzbar bleibt.

###### Duplikatentfernung
Durch die Duplikatentfernung werden doppelte Zeilen im Datensatz entfernt, um Verzerrungen des NLP-Models zu vermeiden. </i><br>
    </details>
</ol>
<ol>
    <details>
      <summary>âšª Datensatzvalidierung (engl. dataset validation)</b></summary>
      <i>Im Rahmen der Datensatzvalisierung werden fehlerhafte Daten korrigiert, verworfen oder speziell behandelt um DatenqualitÃ¤t und Aussagekraft zu sichern.</i><br>
    </details>
</ol>

_________________________________________________________________________________________________________________________________________________________

## Datenverarbeitung (engl. data processing)
Im maschinellen Lernen stellen Merkmale (engl. features) kategorielle oder numerische GrÃ¶ÃŸen dar, anhand derer Algorithmen oder neuronale Netze Texte klassifizieren oder clustern kÃ¶nnen.[^16] Innerhalb von NLU dienen die Features als BrÃ¼cke zwischen rohem Text und algorithmischer Verarbeitung: Sie extrahieren relevante linguistische Informationen auf lexikalischer, syntaktischer oder semantischer Ebene. 

###### Pipeline Eingabe (engl. pipeline input)
Die Sprachverarbeitung beginnt mit dem Import des aufbereiteten Datensatz *"complaints_data_cleaned.csv"*, genauer dem Import der Spalte `<text>` welche als als Korpus fÃ¼r die folgenden NLP-Schritte genutzt wird. Die Zeilen des Datensatzes werden auch als Dokumente bezeichnet. 

<div style="margin-left: 2em;">
  <code>pandas</code>&nbsp;<code>????</code><br>
</div>

### Datenvorverarbeiten (engl. data pre-processing)
> WÃ¤hrend der Datenvorverarbeitung erfolgt die *Merkmalsvorbereitung (engl. feature preparation)* fÃ¼r nachfolgende Schritte in einem mehrstufigen Prozess, welcher sich grob in Textbereinigung (engl. text cleaning) und Merkmalsextraktion unterteilen lÃ¤sst. 
<div style="margin-left: 2em;">
  <code>spaCy</code>&nbsp;<code>NLTK</code><br>
</div>

#### Textbereinigung (engl. text cleaning)
Im Rahmen der Textbereinigung werden Texte von Rauschen befreit und standardisiert.<br>
<img src="docs/2 - Textbereinigung (engl. text cleaning).jpg">

<ol type="1">
  <details>
    <summary>ğŸ”´ Rauschentfernung (engl. noise reduction)</summary>
    <p><i>Ziel der Rauschentfernung ist es irrelevante Token (Zeichen und Zeichenketten) fÃ¼r nachfolgende Prozesse zu identifizieren und zu lÃ¶schen.</i></p>
    <ol type="1">
      </li>
      <li>Wortbereinigung (engl. word cleaning)</li>
        <i>XXX</i>
            <div style="margin-left: 2em;">
              <code>spaCy</code>&nbsp;<code>NLTK(stopwords)</code><br><br>
            </div>
            <ol type="2">
              </div>
      <li>Zeichenbereinigung ()</li>
            <li><ins>Satzzeichen (engl. punctuation marks)</ins></li>
              <div style="margin-left: 2em;">
                <code>spaCy</code>&nbsp;<code>regex</code><br><br>
              </div>
            <li><ins>Leerzeichen (engl. white space)</ins></li>
              <div style="margin-left: 2em;">
                <code>spaCy</code>&nbsp;<code>regex</code><br><br>
              </div>
            <li><ins>Sonderzeichen (engl. special character)</ins></li>
              <div style="margin-left: 2em;">
                <code>spaCy</code>&nbsp;<code>textnorm</code><br><br>
              </div>
        <li>Nummernbereinigung (engl. numbers cleaning)</li>
            <li><ins>Nummern (engl. removing numbers)</ins></li>
              <div style="margin-left: 2em;">
                <code>spaCy</code>&nbsp;<code>regex</code><br><br>
              </div>
      </ol>
  </details>
  <details>
    <summary>ğŸ”´ Standardisierung (engl. standardisation)</summary>
    <p><i>Durch Standardisierung werden relevante Token vereinheitlicht. Hierdurch wird vermieden, dass gleiche Inhalte nicht in mehreren leicht unterschiedlichen Varianten auftreten.</i></p>
          <div style="margin-left: 2em;">
            <code>???</code>&nbsp;<code>????</code><br><br>
          </div>
      <ol type="2">
            <li><ins>Normalisierung (engl. normalisation)</ins></li>
            Durch die Normalisierung wird Text in ein einheitliches Format, besser zu analysierendes Format gebracht.
            </div>
            <ul>
              <li>Kasusumwandlung (engl. case conversion)</li>
              In diesem Schritt erfolgt eine konsequente Kleinschreibung (engl. lowercasing) aller WÃ¶rter.
              <div style="margin-left: 2em;">
                <code>spaCy</code>&nbsp;<code>stdlib(.lower)</code><br><br>
              <li>Formatnormalisierungen (engl. format normalisations)</li>
              In der Formatnormalisierung erfolgt die Normalisierung von Schreibweisen (Datenformate oder Zahlenformaten) und Sonderformen (Emojis).
              <div style="margin-left: 2em;">
                <code>datetime</code><br><br>
              </div>
            </ul>
            <li><ins>Rechtschreibfehlerkorrektur (engl. spelling correction)</ins></li>
            Durch Rechtschreibkorrekrur werden Schreib- und Tippfehler korrigiert, um eine linguistisch korrekte Analyse gewÃ¤hrleisten.
              <div style="margin-left: 2em;">
                <code>pyspellchecker</code>&nbsp;<code>contextualSpellCheck</code>&nbsp;<code>PyEnchant</code><br><br>
              </div>
    </details>
      </ol>
    </ol>



#### Linguistische Analyse
Im Rahmen der lingusitischen Analyse erfolgt, je nach Anwendungsfall neben einer lexikalischen, eine syntaktische und/oder semantische Verarbeitung von bereinigten Texten zur Merkmalsvorbereitung.
<ol type="1">
    <details>
      <summary>ğŸ”´ lexikalische Verarbeitung (engl. lexical processing)</summary>
      <p><i>Im Rahmen der lexikalischen Analyse werden Texte tokenisiert, Token auf ihre Grundform reduziert und ein Vokabular aufgebaut.</i></p>
      <ol type="1">
        <li>Tokenisierung (engl. tokenization)<br>
          <i>Durch Tokenisierung wird der vorbereitete Text in Einzeltoken (Worte) oder N-Gramme (Phrasen) wie z.B. SÃ¤tze zerlegt. Tokenisierung zerlegt Text in Token (WÃ¶rter, SubwÃ¶rter oder Zeichen), aus denen das Vokabular als Menge eindeutiger Token-IDs entsteht.</i><br>
          <div style="margin-left: 2em;">
            <code>SpaCy</code>&nbsp;<code>NLTK(word_tokenize; sent_tokenize)</code><br><br>
          </div>
        </li>
        <li>Grundformreduktion (engl. inflection reduction)<br>
          <i>Durch Grundformreduktion werden WÃ¶rter auf ihre Grundformen reduziert. Da Lemmatisierung (engl. lemmatization) genauer als Stammformreduktion (engl. stemming) ist, wird diese eingesetzt.</i><br>
          <div style="margin-left: 2em;">
            <code>spaCy</code>&nbsp;<code>NLTK (WordNetLemmatizer)</code><br><br>
          </div>
        </li>
        <li>Vokabularerstellung/Wortschatzaufbau (engl. vocabulary construction)<br>
          <i>Im Schritt des Wortschatzaufbaus wird aus dem tokenisierten Textkorpus ein endliches Vokabular erstellt, das allen Token eine eindeutige Token-ID zuweist. Das Vokabular stellt eine Menge eindeutiger Token-IDs dar.</i><br>
          <div style="margin-left: 2em;">
            <code>sklearn(CountVectorizer)</code><br><br>
          </div>
        </li>
        <li>lexikalisches POS-Tagging<br>
          <i>Durch lexikalisches Part-of-Speech Tagging kÃ¶nnen mittels Lookup-Tabellen grammatikalische Wortfunktionen und Kategorien zu einem gegebenen Text hinzugefÃ¼gt werden. Das Vokabular wird dabei mit Sprachdatenannotationen (engl. linguistic annotations) versehen.</i><br>
          <div style="margin-left: 2em;">
            <code>XXXX</code><br><br>
          </div>
        </li>
      </ol>
    </details>
    <details>
      <summary>ğŸ”´ syntaktische Verarbeitung (engl. syntactic processing)</summary>
      <p><i>Im Rahmen der syntaktischen Analyse werden Satzstrukturen und grammatikalische Funktionen analysiert.</i></p>
      <ol type="1">
        <li>syntaktisches POS-Tagging<br>
          <i>Durch Wortart-Tagging (engl. Part-of-Speech Tagging) kÃ¶nnen durch Modelle (Hidden Markov Models - HMM) grammatikalische Wortfunktionen und Kategorien durch Tags-Sets zu einem gegebenen Text hinzugefÃ¼gt werden und so das Vokabular sprach-/domÃ¤nenspezifisch annotieren.</i><br>
          <div style="margin-left: 2em;">
            <code>spaCy</code>&nbsp;<code>NLTK</code><br><br>
          </div>
        </li>
        <li>syntaktisches Parsen (engl. syntax parsing)<br>
          <ol type="1">
            <li>Flaches Parsen (engl. shallow parsing/chunking)<br>
              <i>Beim flachen Parsen werden aufeinanderfolgende WÃ¶rter zu Satzgliedern (engl. chunks) gruppiert, ohne eine vollstÃ¤ndige Satzstruktur zu analysieren. Dies ermÃ¶glicht eine schnelle und effiziente Extraktion von Nominalphrasen und Verbalphrasen.</i><br>
              <div style="margin-left: 2em;">
                <code>NLTK (ne_chunk)</code>&nbsp;<code>spaCy</code><br><br>
              </div>
            </li>
            <li>Tiefes Parsen (engl. deep parsing/dependency parsing)<br>
              <i>Beim tiefen Parsen wird die vollstÃ¤ndige Satzstruktur durch AbhÃ¤ngigkeitsrelationen zwischen WÃ¶rtern analysiert. Dies ermÃ¶glicht ein tieferes VerstÃ¤ndnis von Satzbeziehungen und grammatikalischen Strukturen.</i><br>
              <div style="margin-left: 2em;">
                <code>NLTK</code>&nbsp;<code>spaCy (dependency parser)</code><br><br>
              </div>
            </li>
          </ol>
        </li>
      </ol>
    </details>
    <details>
      <summary>ğŸ”´ semantische Verarbeitung (engl. semantic processing)</summary>
      <p><i>Im Rahmen der semantischen Verarbeitung werden Bedeutungen und ZusammenhÃ¤nge im Text analysiert.</i></p>
      <ol type="1">
        <li>semantisches Parsen (engl. semantic parsing)<br>
          <i>Durch semantisches Parsen werden Bedeutungsstrukturen extrahiert, um tieferes TextverstÃ¤ndnis zu ermÃ¶glichen.</i><br>
          <div style="margin-left: 2em;">
            <code>spaCy</code><br><br>
          </div>
          <ol type="1">
            <li>Eigennamenerkennung (engl. Named Entity Recognition - NER)<br>
              <i>Bei der der Erkennung benannter EntitÃ¤ten werden WÃ¶rter in einem unstrukturierten Text als Kategorien klassifiziert (Namen, Orte, Zeit, Datum, Organisationen, Mengen).</i><br>
              <div style="margin-left: 2em;">
                <code>spaCy</code><br><br>
              </div>
            </li>
            <li>KoreferenzauflÃ¶sung (engl. Coreference Resolution - CR)<br>
              <i>KoreferenzauflÃ¶sung ist eine Methode die es Modellen ermÃ¶glicht, Referenzen auf dieselbe EntitÃ¤t oder dasselbe Konzept innerhalb eines Textes zu erkennen. Durch diese Technik kÃ¶nnen KI-Systeme besser verstehen, auf wen oder was sich Pronomen, Namen oder Nominalphrasen in einem Satz oder Absatz beziehen.</i><br>
              <div style="margin-left: 2em;">
                <code>Library1</code>&nbsp;<code>Library2</code><br><br>
              </div>
            </li>
            <li>Beziehungsextraktion (engl. Relationship Extraction - RE)<br>
              <i>Die Beziehungsextraktion identifiziert Beziehungen zwischen benannten Objekten in einem Text, beispielsweise zwischen Vater und Sohn oder zwischen Mutter und Tochter, ect.</i><br>
              <div style="margin-left: 2em;">
                <code>Library1</code>&nbsp;<code>Library2</code><br><br>
              </div>
            </li>
          </ol>
        </li>
      </ol>
    </details>
</ol>

### Datenvorbereitung (engl. data preparation)
> Im Rahmen der Datenverarbeitung werden Merkmale (engl. features) erzeugt und ausgewÃ¤hlt. Dies erfolgt durch  Merkmalsgenerierung (engl. feature generation/featurization) und Merkmalsauswahl (engl. feature selection). <br> Merkmalsgenerierung (engl. feature generation) bezeichnet in der NLP-Pipeline den Prozess, aus rohem oder vorverarbeitetem Text neue, informative Merkmale zu erzeugen, die Machine-Learning-Modelle effizient nutzen kÃ¶nnen. Sie wandelt unstrukturierte Daten in numerische oder kategorische ReprÃ¤sentationen um, die syntaktische, semantische oder kontextuelle Aspekte einfangen. Dabei werden Attribute/Features in eine fÃ¼r die Modellierung adÃ¤quate Form Ã¼berfÃ¼hrt, weshalb von Merkmalsaufbereitung (engl. feature engineering) gesprochen wird (Baars und Kemper, 2021, p. 159). Dies kann mittels Merkmalskonstruktion, Merkmalsextraktion oder Merkmalsumwandlung erfolgen oder automatisch durch trainierte Modelle vorgenommen werden. In diesem Fall spricht man von Merkmalslernen (engl. feature learning / representation learning), wobei Merkmale direkt aus Rohtexten gewonnen werden. <br> Merkmalsauswahl (engl. feature selection) ist ein komplementÃ¤rer Prozess, der aus einer groÃŸen Menge von erzeugten Merkmalen die relevantesten auswÃ¤hlt. Dies reduziert DimensionalitÃ¤t, verbessert Modellperformance und verringert Rechenaufwand, indem irrelevante oder redundante Merkmale entfernt werden. <br>

feature - explizite / Abstrakte

<div style="margin-left: 2em;">
  <code>???</code>&nbsp;<code>???</code><br><br>
</div>

#### Merkmalsgenerierung (engl. feature generation/featurization)
Merkmalsgenerierung bezeichnet den Prozess, aus rohem oder vorverarbeitetem Text neue, informative Merkmale zu erzeugen. Unstrukturierte Daten werden durch Merkmalskodierung (engl. feature encoding) in numerische oder kategorische ReprÃ¤sentationen Ã¼berfÃ¼hrt, die Machine-Learning-Modelle nutzen kÃ¶nnen.

<ol type="1">
  <details>
    <summary>ğŸŸ¡ Vektorisierung (engl. vectorization)</summary>
    <p><i>Als Vektorisierung wird die Merkmalskodierung (engl. feature encoding) von Textdaten bezeichnet. Die Token (WÃ¶rter, SubwÃ¶rter oder Zeichen) aus dem Vokabular werden durch Vektorisierungstechniken in numerische ReprÃ¤sentationen Ã¼berfÃ¼hrt, die als Merkmalsvektoren in einem nâ€‘dimensionalen Merkmalsraum (engl. feature space) dargestellt und zu Merkmalsmatrizen zusammengefasst werden. Vektorisierungstechniken nutzen Merkmalsextraktion, um Texte je nach Anwendungsfall auf Silben,- Wort-, Satz-, Segmentâ€‘ oder Dokumentenâ€‘Ebene fÃ¼r Modelle aufzubereiten, um lexikalische, syntaktische oder kontextuelle Aspekte eines Textes einzufangen. - explizite Features</i></p>
    <ol type="1">
        <details>
          <summary>ğŸŸ¡ Merkmalsvektoren (engl. feature vectors)</summary>
          <p><i>Spannen keinen semantischen Merkmalsraum auf, sondern erzeugen dÃ¼nn besetzte Vektoren (engl. sparse vectors) auf Basis von Tokenfrequenzen, was Modellen eine algebraische bzw. statistische Auswertung ermÃ¶glicht. Teils werden die Merkmalsvektoren auch als unsemantische oder hÃ¤ufigkeitsbasierte Embeddings (engl. frequency based embeddings) bezeichnet. Diese frequenzbasierten Methoden erzeugen dÃ¼nn besetzte Merkmalsvektoren basierend auf Vokabularpositionen, wobei zwischen Methoden mit und ohne Informationsgewichtung diffrenziert wird.</i></p>
          <ul>
          <details>
            <summary>ğŸŸ¡ <b>BoX (Bag-of-X)</b></summary>
            <p><i>Bei den Bag-of-X-Methoden erfolgt keine Informationsgewichtung, Token oder Tokensequenzen wird eine eigene Dimension zugewiesen.</i></p>
          <ul>
            <li><ins>BoX auf Einzeltoken</ins><br>
            Wird die Methode auf Wortebene durchgefÃ¼hrt, wird sie als Bag-of-Words (BoW) bezeichnet. Der â€Bag-of-Words-Vektor hat fÃ¼r jedes Wort eine eigene Dimension. Wenn das Vokabular n WÃ¶rter umfasst, wird ein Dokument zu einem Punkt (Dokumentenvektor) in einem n-dimensionalen Raum.â€œ (Zheng und Casari, 2019, p. 41)
            <div style="margin-left: 2em;">
              <code>sklearn (CountVectorizer)</code>
            </div>
            <li><ins>BoX auf Tokensequenzen</ins><br>
            Wird die Methode mit einer Folge von n-Token durchgefÃ¼hrt, wird sie als Bag-of-N-Grams (BoN) bezeichnet, was eine lokal auf die Tokensequenz begrenzte Kontexterfassung ermÃ¶glicht. â€Je grÃ¶ÃŸer n ist, desto reicher ist der Informationsgehalt und desto hÃ¶her die Kostenâ€œ fÃ¼r Berechnung, Speicherung und Modellierung (Zheng und Casari, 2019, p. 44). Was bedeutet, dass sich bei BoN ein viel grÃ¶ÃŸerer und dÃ¼nner besetzten Merkmalsraum ergibt.
            <div style="margin-left: 2em;">
              <code>sklearn (CountVectorizer(ngram_range))</code><br><br>
            </div>
            </ul>
          </details>
          <details>
            <summary>ğŸŸ¡ <b>TF-IDF (term frequency times inverse document frequency)</b></summary>
            <p><i>Bei der TF-IDF-Methode handelt es sich um eine statistische Erweiterung von BoX, durch welche eine Informationsgewichtung der Token bzw. Tokensequenzen vorgenommen wird.</i></p>
              <ul>
              <li><ins>TF-IDF auf Einzeltoken</ins><br>
              Wird die TF-IDF-Methode auf Wortebene durchgefÃ¼hrt, werden EinzelwÃ¶rter gewichtet, um ihre Relevanz im Dokument und im Korpus auszudrÃ¼cken.
              <div style="margin-left: 2em;">
                <code>sklearn (TfidfVectorizer)</code>
              </div>
              <li><ins>TF-IDF auf Tokensequenzen</ins><br>
              Wird die TF-IDF-Methode mit einer Folge von n-Token durchgefÃ¼hrt, werden Wort-Paare oder lÃ¤ngere Phrasen gewichtet, um ihre Relevanz auszudrÃ¼cken.
              <div style="margin-left: 2em;">
                <code>sklearn (TfidfVectorizer(ngram_range))</code><br><br>
              </div>
              </ul>
          </details>
</ol> 
      <ol type="1">
        <details>
          <summary>ğŸŸ¡ Merkmalseinbettungen (engl. feature embeddings)</summary>
          <p><i>Spannen einen semantischen Merkmalsraum auf und liefern dichtbesetzte Vektoren (engl. dense vectors), was Modellen eine Auswertung von semantischen Ã„hnlichkeiten Ã¼ber AbstÃ¤nde bzw. Ã„hnlichkeitsmaÃŸe ermÃ¶glicht. Einbettungen (engl. embeddings) weisen jedem Merkmal einen dichten Vektor im semantischen Raum zu und erfassen so statisch oder dynamisch die Bedeutungsdimension mittels vorhersage- oder kontextbasierter Verfahren anhand vortrainierter Modelle auf Wort-, Satz-, Segmentâ€‘ oder Dokumentenâ€‘Ebene. Hierdurch werden semi-explizite Merkmale generiert, welche zwischen expliziten und latenten Merkmalen einzuordnen sind. Kontextmodelle: 
          <p><i>Worteinbettungen sind dichte Vektoren, die semantische Bedeutungen von WÃ¶rtern oder SÃ¤tzen reprÃ¤sentieren. Sie entstehen durch das Training von Modellen auf Textdaten und erfassen semantische und syntaktische Beziehungen.</i></p>
          </i></p>
          <ol type="1">
              <details>
                <summary>ğŸŸ¡ <b>Worteinbettungen </b>(engl. word embeddings)</summary>
                <p><i>Worteinbettungen weisen jedem Wort einen dichten Vektor im semantischen Raum zu und ermÃ¶glichen hierdurch Modellen eine Auswertung von semantischen Ã„hnlichkeiten zwischen WÃ¶rtern.</i></p>
                <ul>
                <li><ins>vorhersagebasierte Wort-Einbettungen (engl. prediction based word embeddings)</ins></li>
                Diese Embeddings werden durch Vorhersage von WÃ¶rtern basierend auf ihrem Kontext trainiert.
              Word2Vec (Skip-gram, CBOW); GloVe (Global Vectors for Word Representation), FastText
                vorhersagebasierte Wort-Einbettungen sind statische Einbettungen, die jedes Wort zu einem festen Vektor Ã¼bersetzen â€“ unabhÃ¤ngig vom Kontext, in dem es steht. Ihr Training erfolgt auf riesigen Textkorpora, dabei lernen die Modelle, WÃ¶rter mit Ã¤hnlichen Kontexten auch im Vektorraum zusammenzubringen. Sprachliche Vieldeutigkeiten (Polysemie) und KontextÃ¤nderungen werden dabei jedoch nicht abgebildet (Wehner, 2026).
              <ul>
                <li>GloVe (Global Vectors for Word Representation)</li>
                  <div style="margin-left: 2em;">
                    <code>gensim</code>&nbsp;<code>glove-python</code><br><br>
                  </div>
                <li>Word2Vec</li>
                  <div style="margin-left: 2em;">
                    <code>gensim</code><br><br>
                  </div>
                <li>FastText
                  <div style="margin-left: 2em;">
                    <code>gensim</code><br><br>
                  </div>
                </li>
              </ul>
                <li><ins>kontextbasierte Wort-Einbettungen (engl. contextualized word embeddings)</ins></li>
                Bei kontextbasierte Wort-Einbettungen handelt es sich um dynamische Worteinbettungen bei denen jedes â€Wort, jede Phrase, jeder Satz [...] situativ angepasste Embedding-Vektoren â€“ in AbhÃ¤ngigkeit vom umgebenden Kontext [bekommen]." Semantische und syntaktische Unterschiede kÃ¶nnen so erstmals maschinell berÃ¼cksichtigt werden. (Wehner, 2026)<br>
                <ul>
                <li>bidirektionale kontextbasierte Wort-Einbettungen</li>
                Diese Embeddings werden durch groÃŸe Sprachmodelle erzeugt, die Kontext bidirektional nutzen. ---- Bidirektionale Kontextmodelle:</b> ELMo, BERT
                <ul>
                <li>BERT (Bidirectional Encoder Representations from Transformers)
                  <div style="margin-left: 2em;">
                    <code>XXX</code>&nbsp;<code>XXX</code><br><br>
                  </div>
                </li>
                <li>ELMo (Embeddings from Language Models)</li>
                bi-directional LSTM Network.
                  <div style="margin-left: 2em;">
                    <code>tensorflow</code>&nbsp;<code>tensorflow_hub</code><br><br>
                  </div>
                </li>
                </ul>
                <li>unidirektionale kontextbasierte Wort-Einbettungen</li>
                Diese Embeddings werden durch generative Sprachmodelle erzeugt, die Kontext von links nach rechts (unidirektional) nutzen.
                <ul>
                <li>GPT (Generative Pre-trained Transformer)</li>
                  <div style="margin-left: 2em;">
                    <code>transformers</code>&nbsp;<code>sentence-transformers</code><br><br>
                  </div>
                </li>
                </ul>
                </ul>
              </details>
              <details>
                <summary>ğŸŸ¡ <b>Satzeinbettungen </b>(engl. sentence embeddings)</summary>
                <p><i>Satzeinbettungen weisen jedem Satz einen dichten Vektor im semantischen Raum zu und ermÃ¶glichen hierdurch Modellen eine Auswertung von semantischen Ã„hnlichkeiten zwischen SÃ¤tzen.</i></p>
                <ul>
                <li><ins>vorhersagebasierte Satzeinbettungen (engl. prediction based sentence embeddings)</ins></li>
                <i>vorhersagebasierte Satzeinbettungen sind statische Einbettungen auf Satzebene und nutzen Encoder-Decoder-Architekturen oder Ã¤hnliche Verfahren, um SÃ¤tze in feste Vektoren zu Ã¼bersetzen.</i>
                <ul>
                <li>SkipThought Embeddings
                Als vorhersagebasierte Satzeinbettung mit Encoder-Decoder-Architektur verarbeiten SkipThought Embeddings die Eingabesequenzen sequenziell. Das ursprÃ¼ngliche SkipThought-Modell basiert tyischerweise auf RNNs/LSTMs, die Sequenzen Token fÃ¼r Token(Wort fÃ¼r Wort) verarbeiten. Bei der klassischen SkipThought-Architektur mit RNN/LSTM-Encoder-Decoder verarbeitet der Encoder die Eingabesequenz in einer Richtung (von links nach rechts). Das Modell nutzt dabei nur Informationen aus den vorangegangenen Tokens, um zukÃ¼nftige Tokens vorherzusagen.
                  <div style="margin-left: 2em;">
                    <code>XXX</code>&nbsp;<code>gensim</code><br><br>
                  </div>
                </li>
                </ul>
                <li><ins>kontextbasierte Satzeinbettungen (engl. contextualized sentence embeddings)</ins></li>
                <i>kontextbasierte Satzeinbettungen werden durch Transformer-basierte oder RNN-basierte Modelle erzeugt und erfassen Satzebenen-Semantik.</i>
                <ul>
                <li><ins>Unidirektionale Satzeinbettungen (unidirektional)</ins></li>
                <i>GrundsÃ¤tzlich wÃ¤ren auch unidirektionale Kontextmodelle auf Satzebene mÃ¶glich â€“ beispielsweise indem man GPT-basierte Modelle fÃ¼r SatzreprÃ¤sentationen nutzt. Solche Modelle wÃ¼rden den Kontext von links nach rechts verarbeiten und SÃ¤tze als ganze Einheiten einbetten kÃ¶nnen.</i></li>
                <li><ins>Bidirektionale Kontextmodelle (bidirektional)</ins></li>
                <ul>
                  <li>SBERT (Sentence-BERT)
                    <div style="margin-left: 2em;">
                      <code>sentence-transformers</code>&nbsp;<code>XXX</code><br><br>
                    </div>
                  </li>
                  <li>USE Embedding (Universal Sentence Encoder)
                    <div style="margin-left: 2em;">
                      <code>tensorflow-hub</code><br><br>
                    </div>
                  </li>
                </ul>
                </li>
                </ul>
              </details>
            </ol>
          </ol>
    </ul>
      </li>
        </details>
    </ol>
  </details>
</ol>



#### Merkmalsauswahl (engl. feature selection)
<p><i>Merkmalsauswahl ist ein komplementÃ¤rer Prozess zur Merkmalsgenerierung. Nach der Erzeugung von Features werden die relevantesten Merkmale aus dem bestehenden Merkmalsraum ausgewÃ¤hlt, um Redundanz zu reduzieren, Overfitting zu vermeiden und die Modellleistung zu optimieren.</i></p>
      <ol type="1">
        <details>
          <summary>ğŸŸ¡ Filtermethoden</summary>
          <p><i>wÃ¤hlen Features basierend auf statistischen Eigenschaften (z.B. Korrelation, Chi-Quadrat)</i></p>
        </details>
        <details>
          <summary>ğŸŸ¡ Wrapper-Methoden</summary>
          <p><i>evaluieren Feature-Subsets durch Modelltraining</i></p>
        </details>
        <details>
          <summary>ğŸŸ¡ Eingebettete Methoden</summary>
          <p><i>wÃ¤hlen Features wÃ¤hrend des Modelltrainings aus (z.B. Lasso, Tree-based)</i></p>
        </details>
        <div style="margin-left: 2em;">
          <code>sklearn (SelectKBest, SelectPercentile, RFE)</code><br><br>
        </div> 
      </ol>


### ModellbildungÂ (engl. model training/building)
<p><i>Modellbildung ist der Prozess, in dem die Merkmalslernen-Modelle mit den vorbereiteten Daten trainiert werden. Dabei werden die Modellparameter iterativ optimiert, um optimale Features zu lernen. Der Trainingsprozess umfasst die Festlegung von Hyperparametern (z.B. Anzahl der Themen K, Lernrate, Iterationen), die Initialisierung des Modells und die iterative Anpassung der Parameter bis zur Konvergenz oder zum Erreichen einer maximalen Anzahl von Iterationen.</i></p>

Modellbildung (Modeling): Hier werden lineare Modelle oder neuronale Modelle trainiert, unter Nutzung von Modellparametern (Gewichte) und Hyperparametern (Lernrate, BatchgrÃ¶ÃŸe)

Modellparameter / Hyperparameter.
Hyperparameter bestimmen das Trainingsverhalten und beeinflussen die QualitÃ¤t der gelernten Features. Ihre Optimierung erfolgt typischerweise durch iterative Verfahren wie Grid Search oder Random Search.


â€Die Hyperparameter eines Modells liegen auÃŸerhalb des Modells und werden vor dem Training durch die Abstimmung der Hyperparameter festgelegt. Einige Hyperparameter bestimmen das Verhalten des Modells wÃ¤hrend des Trainingsâ€œ (IBM Deutschland GmbH, 2025)

Overfitting
Um overfitting zu vermeiden wird der Datensatz fÃ¼r das Training auf xxx Zeilen begrenzt.

#### Merkmalslernen (engl. feature learning / representation learning)
<p><i>Merkmalslernen ist ein automatisierter Prozess, bei dem ein Modell selbst neue informative Merkmale aus den vorhandenen oder rohen Features lernt und entdeckt. Im Gegensatz zu manuellem Feature Engineering werden die Merkmale nicht von Menschen definiert, sondern vom Modell wÃ¤hrend des Trainings durch Algorithmen erlernt. Dabei wird eine Merkmalsumwandlung (engl. feature transformation) durch Modelle durchgefÃ¼hrt. Neuen Features entstehen so entweder durch semantische Abstraktion (neue interpretierbare Konzepte), Merkmalsabstraktion (engl. feature abstraction) oder mathematische Projektion (neue Achsen), Merkmalsprojektion (engl. feature projection).</i></p>
unÃ¼berwacht, Clustering.

<ol type="1">
  <details>
      <summary>ğŸŸ¡ Themenmodellemodelle/Themenmodellierung (engl. topic modeling)</summary>
        <p><i>Themenmodellierung identifiziert unÃ¼berwacht latente abstrakte Themen in Textsammlungen. Diese neuen Merkmale (Themen) sind nicht explizit im Text vorhanden, sondern werden durch mathematische Modelle aus den bestehenden Merkmalen automatisch extrahiert oder transformiert. Topic-Modelle unterscheiden sich je nachdem, ob sie auf Merkmalsabstraktion oder Merkmalsprojektion basieren.</i></p>
    <ol type="1">
      <details>
        <summary>ğŸŸ¡ Merkmalsabstraktion (engl. feature abstraction)</summary>
        <p><i>Merkmalsabstraktion bedeutet, dass Modelle neue Konzepte oder Bedeutungen direkt aus den Daten herausfinden und als neue Features reprÃ¤sentieren. Die neuen Features sind semantisch interpretierbar und nicht nur mathematische Transformationen.
        <b>Merkmalsabstraktion:</b> Modelle extrahieren neue interpretierbare Konzepte aus Features (z.B. Themen, Embeddings).
        Abstraktion-basiert (semantische Konzepte); Abstraktion-basierte Topic-Modelle nutzen probabilistische oder Embedding-basierte Verfahren, um neue interpretierbare Konzepte direkt aus Daten zu extrahieren. Sie modellieren semantische Themen durch komplexe statistische oder neuronale Prozesse.</i></p>
            <ol type="1">
              <details>
                <summary>ğŸŸ¡ wortraumbasierte Topicmodelle</summary>
                <p><i> Diskreter Wort-Feature-Raum. Jedes Wort ist eine Dimension, aufgerufen durch einen Merkmalsvektor (engl. feature vector), siehe oben.</i></p>
                <ul>
                <li><ins>LDA (Latent Dirichlet Allocation)</li></ins>
                <p><i>Latent Dirichlet Allocation (LDA) ist ein probabilistisches Modell, das latente Themen aus der Merkmalsmatrix durch wahrscheinlichkeitsbasierte Themen-Wort-Verteilungen identifiziert. LDA erzeugt interpretierbare Themen mit probabilistischen Zuordnungen zu Dokumenten und WÃ¶rtern.</i></p>
                <div style="margin-left: 2em;">
                  <code>gensim</code>&nbsp;<code>sklearn (LatentDirichletAllocation)</code><br><br>
                </div>
                <p><b>Output:</b> Themenmischung pro Dokument (Î±), Wort-Gewichte pro Thema (Î²), K latente Themen</p>
                </ul>
              </details>
              <details>
                <summary>ğŸŸ¡ kontextraumbasierte Themenmodelle </summary>
                <p><i>Kontinuierlicher semantischer Raum. Jedes Wort ist , aufgerufen durch Merkmalseinbettung (engl. feature embedding), siehe oben. </i></p>
                <ul>
                <li><ins>BERTopic</li></ins>
                <p><i>BERTopic ist eine moderne Erweiterung klassischer Topic-Modeling-Methoden, die vortrainierte BERT-Embeddings mit Dimensionsreduktion (UMAP) und Clustering (HDBSCAN) kombiniert. Sie erzeugt interpretierbare und semantisch kohÃ¤rente Themen direkt aus Embeddings, ohne dass eine separate Merkmalsmatrix nÃ¶tig ist, und ist besonders effektiv bei groÃŸen Textsammlungen.</i></p>
                <div style="margin-left: 2em;">
                  <code>bertopic</code>&nbsp;<code>sentence-transformers</code>&nbsp;<code>umap-learn</code><br><br>
                </div>
                <p><b>Output:</b> Topic-Label pro Dokument, Wort-Gewichte pro Topic, Cluster-Visualisierung</p>
                </ul>
              </details>
            </ol>
          </details>
          <details>
            <summary>ğŸŸ¡ Merkmalsprojektion (engl. feature projection)</summary>
            <p><i>Projektion-basierte Topic-Modelle nutzen algebraische Matrixfaktorisierungstechniken, um die Merkmalsmatrix in Faktoren zu zerlegen. Obwohl sie mathematische Transformationen verwenden, erzeugen sie dennoch interpretierbare latente Konzepte, die als Themen fungieren.</i></p>
            <ol type="1">
              <details>
                <summary>ğŸŸ¡ wortraumbasierte Topicmodelle</summary>
                <p><i>Diskreter Wort-Feature-Raum. Merkmalsprojektion basiert auf algebraischen Verfahren der Matrixfaktorisierung auf HÃ¤ufigkeitsvektoren.</i></p>
                <ul>
                <li><ins>NMF (Non-Negative Matrix Factorization)</ins></li>
                <p><i>Non-Negative Matrix Factorization (NMF) ist ein algebraisches Verfahren, das die Merkmalsmatrix in zwei Faktormatrizen mit nicht-negativen Werten zerlegt. Im Gegensatz zu probabilistischen Modellen wie LDA erzeugt NMF deterministische Topic-Zuordnungen, die direkt aus der Matrixfaktorisierung hervorgehen.</i></p>
                <div style="margin-left: 2em;">
                  <code>sklearn (NMF)</code><br><br>
                </div>
                <p><b>Output:</b> Topic-Gewichte pro Dokument, Wort-Gewichte pro Topic, K Themen</p>
                </ul>
                <ul>
                <li><ins>LSA (Latent Semantic Analysis)</ins></li>
                <p><i>Latent Semantic Analysis (LSA) nutzt SingulÃ¤rwertzerlegung (SVD), um latente semantische Dimensionen aus der Merkmalsmatrix zu extrahieren. LSA ist ein algebraisches Verfahren der Matrixfaktorisierung, das effektiv und effizient interpretierbare Themen fÃ¼r Topic Modeling erzeugt.</i></p>
                <div style="margin-left: 2em;">
                  <code>sklearn (TruncatedSVD)</code><br><br>
                </div>
                <p><b>Output:</b> k latente Dimensionen, Singular Values, LSA-Komponenten</p>
                </ul>
              </details>
              <details>
                <summary>ğŸŸ¡ kontextraumbasierte Topicmodelle</summary>
                <p><i>Kontextraumbasierte Modelle arbeiten typischerweise mit Merkmalsabstraktion, nicht mit Merkmalsprojektion. Die Matrixfaktorisierung ist fÃ¼r wortraumbasierte Verfahren charakteristisch.</i></p>
              </details>
            </ol>
          </details>
      Output: semantisch interpretierbare Themen 
  </details>
  <details>
    <summary>ğŸŸ¡ Dimensionsreduktion (engl. dimensionality reduction)</summary>
    <p><i>Transformiert hochdimensionale Features auf neue mathematische Achsen fÃ¼r Visualisierung und Datenanalyse. Die neuen Dimensionen sind nicht semantisch interpretierbar, aber nÃ¼tzlich zur Strukturerkennung.</i></p>
    <ol type="1">
      <details>
        <summary>ğŸŸ¡ Lineare Projektionen (engl. linear projections)</summary>
        <p><i>Lineare Projektionen reduzieren Dimensionen durch orthogonale Transformationen, die Varianzrichtungen im Datenraum erfassen.</i></p>
        <ol type="1">
          <details>
            <summary>ğŸŸ¡ PCA (Principal Component Analysis)</summary>
          <p><i>Findet Hauptkomponenten (Richtungen maximaler Varianz) und projiziert Features darauf.</i></p>
          <div style="margin-left: 2em;">
            <code>sklearn (PCA)</code><br>
            <b>Output:</b> K Komponenten, Varianzanteil pro Komponente
          </div>
        </details>
      </details>
      <details>
        <summary>ğŸŸ¡ Nichtlineare Projektionen (engl. non-linear projections)</summary>
        <p><i>Nichtlineare Projektionen bewahren lokale oder globale Strukturen in den Daten besser, sind aber rechnerisch aufwendiger. Sie werden hauptsÃ¤chlich fÃ¼r Visualisierung verwendet.
        </i></p>
        <ol type="1">
          <details>
            <summary>ğŸŸ¡ t-SNE (t-distributed Stochastic Neighbor Embedding)</summary>
            <p><i>Projiziert auf 2-3 Dimensionen, bewahrt lokale Nachbarschaften. Ideal fÃ¼r Cluster-Visualisierung.</i></p>
            T-SNE (t-distributed Stochastic Neighbor Embedding) â€Die t-SNE- Projektion (Mitte) zeigt eine Verbesserung mit gut voneinander getrennten Clustern. Jede Farbe (die fÃ¼r eine andere Person steht) bildet eine eigene, kompakte Gruppe mit klaren Grenzen zwischen den verschiedenen Personen. Schau mal, wie t-SNE fast perfekte lokale Gruppierungen macht, bei denen Gesichter derselben Person ganz nah beieinander liegen und von anderen Gruppen weggeschoben werden. Das ist die StÃ¤rke von t-SNE: Es ist super darin, lokale Nachbarschaften zu erhalten und visuell unterschiedliche Cluster zu erstellen.â€œ (Thevapalan, 2025)
            nichtlineare probabilistische Technik zur DimensionalitÃ¤tsreduzierung â€Eigene Embedding Spaces erstellen & visualisieren
            <div style="margin-left: 2em;">
              <code>sklearn (TSNE)</code><br>
              <b>Output:</b> 2-3D Koordinaten
            </div>
          </details>
          <details>
            <summary>ğŸŸ¡ UMAP (Uniform Manifold Approximation and Projection)</summary>
            <p><i>Moderne Alternative zu t-SNE, schneller und skalierbarer. Bewahrt lokale und globale Strukturen.</i></p>
            UMAP (Uniform Manifold Approximation and Projection) â€NatÃ¼rliche Sprachverarbeitung: Textdaten kÃ¶nnen, wenn sie in hochdimensionale Einbettungen umgewandelt werden, mit UMAP visualisiert werden, um semantische Beziehungen zu verstehen. Es wird oft benutzt, um Wort-Embeddings und Dokument-Cluster zu zeigen und Sprachmodelle zu debuggen, indem es zeigt, wie verschiedene Konzepte im Embedding-Raum miteinander zusammenhÃ¤ngen.â€œ (Thevapalan, 2025)
            <div style="margin-left: 2em;">
              <code>umap-learn</code><br>
              <b>Output:</b> 2-3D Koordinaten
            </div>
            Output: Visualisierbar, aber nicht semantisch interpretierbar
          </details>
        </ol>
      </details>
    </ol>
  </details>
</ol>

### Modellabstimung (engl. model calibration)
Anpassung der Modellbildung

##### Modellbewertung (engl. model evaluation)
AbhÃ¤ngig vom ML-Aufgabentyp erfolgt eine Modellbewertungen entweder anhand intrinsische oder extrinsische Metriken. Extrinsische Metriken werden bei Ã¼berwachten Lernaufgaben intrinsische Metriken bei unÃ¼berwachten Lernaufgaben verwendet.

<ol type="1">
  <details>
    <summary>ğŸŸ¡ Intrinsische Metriken (engl. intrinsic metrics)</summary>
    <p><i>Bewerten die QualitÃ¤t gelernter Features basierend auf innerer Struktur, ohne externe Referenzen zu benÃ¶tigen.</i></p>
    <ol type="1">
      <details>
        <summary>ğŸŸ¡ KohÃ¤renz (engl. coherence)</summary>
        <p><i>Misst semantische Konsistenz der Top-WÃ¶rter pro Thema. Ein hÃ¶herer Wert deutet auf kohÃ¤rente, interpretierbare Themen hin.</i></p>
        <ul>
          <li><b>u_mass</b>: Interne KohÃ¤renz</li>
          <li><b>c_v</b>: Externe Konsistenz (empfohlen)</li>
          <li><b>c_uci, c_npmi</b>: Alternative Berechnungsvarianten</li>
        </ul>
        <div style="margin-left: 2em;">
          <code>gensim.models.CoherenceModel</code><br>
          <b>Bereich:</b> -1 bis 1 (hÃ¶her = besser)
        </div>
      </details>
      <details>
        <summary>ğŸŸ¡ Verwirrung (engl. perplexity)</summary>
        <p><i>Misst die durchschnittliche Vorhersageunsicherheit des Modells auf ungesehenen Daten. Niedrigere Werte deuten auf bessere Generalisierung hin.</i></p>
        <div style="margin-left: 2em;">
          <code>gensim</code>&nbsp;<code>torchmetrics</code><br>
          <b>Bereich:</b> 0 bis âˆ (niedriger = besser)
        </div>
      </details>
      <details>
        <summary>ğŸŸ¡ Themenvielfalt (engl. topic diversity)</summary>
        <p><i>Misst, inwieweit sich die Top-WÃ¶rter verschiedener Themen unterscheiden â€“ verhindert redundante Themen.</i></p>
      </details>
    </ol>
  </details>
  <details>
    <summary>ğŸŸ¡ Extrinsische Metriken (engl. extrinsic metrics)</summary>
    <p><i>Bewerten Modellleistung durch Vergleich mit bekannten Labels in Downstream-Tasks (z.B. Klassifikation, Named Entity Recognition).</i></p>
    <ul>
      <li>Accuracy, Precision, Recall, F1-Score</li>
      <li>GLUE/SuperGLUE Benchmarks (fÃ¼r Sprachmodelle)</li>
    </ul>
  </details>
</ol>

###### Pipeline Ausgabe (engl. pipeline output)

Die durch das finale Modell verarbeiteten Daten flieÃŸen in Form von Scores, Labels oder Logits die Datennachverarbeitung (engl. data post-processing) ein.
______________
### Datennachverarbeitung (engl. data post-processing)
Post-Processing macht Daten nutzbar.

Datennachverarbeitung (engl. post-processing) erfolgt nach der ModellausfÃ¼hrung (Inference), um rohe Modellausgaben nutzbar zu machen.
<img src="3 - Datennachverarbeitung (engl. data post-processing).jpg" width="1200">

#### ğŸ”µ Merkmalszusammenfassung (engl. (feature) aggregation)
Im Rahmen der Merkmalszusammenfassung erfolgt eine Konsolidierung der Modellausgaben, in der Merkmalsanalysen (engl. feature analysis) durchgefÃ¼hrt und letztlich als DatenprÃ¤sentation (engl. data presentation) aufbereitet werden.

alphanumerische Darstellungen - Aggregation reduziert die Datenmenge durch mathematische Operationen wie Summe, Mittelwert, ZÃ¤hlung oder Maximum Ã¼ber Gruppierungen (z. B. nach Token-Typ, Dokument oder Zeitraum). In NLP kÃ¶nnte dies die HÃ¤ufigkeitsverteilung von n-Grammen pro DomÃ¤ne oder die durchschnittliche Embedding-Distanz pro Klasse bedeuten. Sie erfolgt vor der Visualisierung, um Ãœberladung zu vermeiden, und ist rein datenverarbeitend ohne grafische Elemente. Aggregation fasst Rohdaten zu kompakteren Zusammenfassungen zusammen.


#### ğŸ”µ Merkmalsanalyse (engl. feature analysis)
<p><i>Merkmalsanalyse ist der analytische Prozess, bei dem bereits erstellte, ausgewÃ¤hlte oder gelernte Merkmale untersucht, beschrieben und interpretiert werden. Dies erfolgt durch Merkmalserkennung, um spezifische Muster und Strukturen in den Daten zu identifizieren.</i></p>

**Merkmalsanalyse** untersucht und beschreibt die in der Merkmalsaufbereitung erstellten Merkmale. WÃ¤hrend die **Merkmalsaufbereitung** (Phase Datenverarbeitung) konstruktiv arbeitet und Merkmale schafft, arbeitet die **Merkmalsanalyse** (Phase Datennachverarbeitung) analytisch und interpretiert diese Merkmale fÃ¼r EntscheidungstrÃ¤ger.

- **Merkmalsanalyse (engl. feature analysis)**: Der analytische Prozess, bei dem bereits erstellte oder vorhandene Merkmale untersucht, beschrieben und bewertet werden. Merkmalsanalyse basiert auf Merkmalsbeschreibungen und erfolgt typischerweise in der Datennachverarbeitung.


<ol type="1">
  <details>
    <summary>ğŸ”µ Merkmalserkennung (engl. feature recognition) â€“ Pattern-Erkennung</summary>
    <p><i>Merkmalserkennung identifiziert spezifische Muster, Anomalien oder Strukturen in bereits erzeugten oder gelernten Features durch regelbasierte oder lernbasierte Verfahren.</i></p>
    <ol type="1">
      <li><ins>Regelbasierte Erkennung</ins><br>
      <i>Verwendung von vordefinierten Regeln und Heuristiken zur Mustererkennung.</i>
        <div style="margin-left: 2em;">
          <code>NLTK (pattern matching)</code>&nbsp;<code>regex</code><br><br>
        </div>
      </li>
      <li><ins>Lernbasierte Erkennung</ins><br>
      <i>Verwendung von trainierten Modellen zur automatischen Mustererkennung und Klassifikation.</i>
        <ol type="1">
          <details>
            <summary>ğŸŸ¡ KeyBERT (Keyword/Term Extraction)</summary>
            <p><i>KeyBERT extrahiert automatisch interpretierbare SchlÃ¼sselwÃ¶rter aus Dokumenten durch semantische Ã„hnlichkeit von BERT-Embeddings und ermÃ¶glicht so die schnelle Identifikation dominanter Begriffe in Textsammlungen.</i></p>
            <div style="margin-left: 2em;">
              <code>keybert</code>&nbsp;<code>sentence-transformers</code><br><br>
            </div>
          </details>
        </ol>
        <div style="margin-left: 2em;">
          <code>sklearn (Clustering, Classification)</code>&nbsp;<code>K-Means, DBSCAN</code><br><br>
        </div>
      </li>
    </ol>
  </details>
</ol>


Datenauswertung (engl. data analysis)
<div style="margin-left: 2em;">
  <code>???</code>&nbsp;<code>???</code><br>
</div>

#### ğŸ”µ Merkmalsauswertungen (engl. feature )
DatenprÃ¤sentation  (engl. data presentation)
ğŸ”µ Visualisierung (engl. visualization)
      <p><i>grafische Darstellung - Visualisierung stellt die aggregierten Daten grafisch dar, um Muster erkennbar zu machen.</i></p>
      Themenverteilungen; Top-WÃ¶rter pro Thema
      BERTopic-Integrierte Visualisierung
      <div style="margin-left: 2em;">
        <code>PyLDAvis</code>&nbsp;<code>BERTopic</code>&nbsp;<code>ploty</code><br><br>
      </div>


### DatenverstÃ¤ndnis (engl. data understanding)
Dateninterpretation / domÃ¤nenspezifische Interpretation

______________

</ul>

## AbhÃ¤ngigkeiten (engl. dependencies)
Ein Python Skript mit der Endung ".py" wird als Modul bezeichnet. Eine Sammlung von Modulen in einem Ordner, wird Paket (engl. package) genannt. Eine Sammlung von Paketen innerhalb eines grÃ¶ÃŸeren Projekts wird Bibliothek (engl. librarys) genannt. Als Rahmenwerk (engl. framework) werden groÃŸe, grundlegende Bibliotheken mit vielen aufeinander aufbauenden oder voneinander abhÃ¤ngenden Paketen bezeichnet. Viele Funktionen sind Bestandteil von Bibliotheken und kÃ¶nnen Ã¼ber: 

```python
<paketname>.<funktionsname>(<funktionsargumente>)
```
aufgerufen werden.[^15]<br>

###### Standardbibliothek

  Dokumentation: https://docs.python.org/3.9/py-modindex.html

  |`stdlib`         | Dokumentation                                                      |Verwendung      |
  |-----------------|--------------------------------------------------------------------|----------------|
  |[`re`]           |https://docs.python.org/3.9/library/re.html#module-re               |NLP             |
  |[`csv`]          |https://docs.python.org/3.9/library/csv.html#module-csv             |Datahandling    |
  |[`venv`]         |https://docs.python.org/3.9/library/venv.html#module-venv           |Laufzeitumgebung|

###### externe Bibliothek

  | Bibliothek           | Website                                                                                                                                           |Verwendung              |
  |--------------------- |---------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|
  |`torch`               |Website: https://pypi.org/project/torch/                                  <br>Dokumentation: https://docs.pytorch.org/docs/stable/index.html       |KI-Detektor             |
  |`transformers`        |Website: https://pypi.org/project/transformers/                           <br>Dokumentation:                                                       |KI-Detektor             |
  |`pandas`              |Website: https://pandas.pydata.org                                        <br>Dokumentation: https://pandas.pydata.org/docs/                       |Datahandling            |
  |`spacy`               |Website: https://spacy.io                                                 <br>Dokumentation:                                                       |NLP                     |
  |`nltk`                |Website: https://github.com/nltk                                          <br>Dokumentation: https://www.nltk.org                                  |NLP                     |
  |`pyspellchecker`      |Website: https://pypi.org/project/pyspellchecker/                         <br>Dokumentation: https://pyspellchecker.readthedocs.io/en/latest/      |NLP                     |
  |`gensim`              |Website: https://pypi.org/project/gensim/                                 <br>Dokumentation:                                                       |NLP - Themenmodellierung|
  |`numpy`               |Website: https://numpy.org                                                <br>Dokumentation: https://numpy.org/doc/stable/.                        |                        |
  |`sklearn`             |Website: https://scikit-learn.org/stable/index.html                       <br>Dokumentation:                                                       |                        |
  |`torchmetrics`        |Website: https://pypi.org/project/torchmetrics/                           <br>Dokumentation: https://lightning.ai/docs/torchmetrics/stable/.       |Evaluation              |
  |`matplotlib`          |Website: https://matplotlib.org                                           <br>Dokumentation: https://matplotlib.org/stable/index.html              |Visualisierung          |
  |`seaborn`             |Website: https://seaborn.pydata.org                                       <br>Dokumentation: https://seaborn.pydata.org/tutorial.html              |Visualisierung          |
  |`PyLDAvis`            |Website: https://pypi.org/project/pyLDAvis/                               <br>Dokumentation: https://pyldavis.readthedocs.io/en/latest/            |Visualisierung          |
  |`cartopy`             |Website: https://github.com/SciTools                                      <br>Dokumentation: https://cartopy.readthedocs.io/stable/                |Visualisierung          |
  |`wordcloud`           |Website: https://pypi.org/project/wordcloud/                              <br>Dokumentation: https://amueller.github.io/word_cloud/                |Visualisierung          |
  |`Top2Vec`             |Website: https://pypi.org/project/top2vec/                                <br>Dokumentation: https://top2vec.readthedocs.io/en/stable/Top2Vec.html |Visualisierung          |
  |`stanza `             |Website: https://stanfordnlp.github.io/stanza/                            <br>Dokumentation:                                                       |NLP                     |




## Referenzen

###### Skripte
IU Internationale Hochschule. (2024). Advanced Data Analysis (DLBDSEDA01_D) [Lernskript]. 001-2024-1210.<br>
<br>IU Internationale Hochschule. (2023). Artificial Intelligence (K. Schaaff, Ãœbers.; DLBDSEAIS01_D) [Studienskript]. 001-2023-1213.<br>
<br>IU Internationale Hochschule. (2025). Data Analytics und Big Data (DLBINGDABD01) [Lernskript]. 002-2025-0108.

###### Abschlussarbeiten
Kruse, C. (2022). Vergleichende Evaluation von  Topic-Modellen fÃ¼r die  Analyse von  Softwareinzidenztickets [Masterarbeit, Technische Hochschule Ingolstadt]. https://opus4.kobv.de/opus4-haw/frontdoor/deliver/index/docId/3478/file/I001169705Abschlussarbeit.pdf<br>
Steiner, D., & Zeneli, G. (2019). Texploration: Automatische Analyse von grossen Textsammlungen [Bachelorarbeit, ZÃ¼rcher Hochschule fÃ¼r Angewandte Wissenschaften]. https://www.zhaw.ch/storage/engineering/institute-zentren/cai/BA19_Texploration_Steiner_Zeneli.pdf<br>

###### BÃ¼cher
Lane, H., Howard, C, & Hapke, H. M. (2019). Natural language processing in action: Understanding, analyzing, and generating text with Python. Manning.<br>
<br>Abbott, D., Kommer, I., & Kommer, C. (2025). Datenvisualisierung im praktischen Einsatz: Ansprechende Diagramme und Dashboards gestalte (1. Auflage). dpunkt.verlag.<br>
<br>Alpar, P., Alt, R., Bensberg, F., & Czarnecki, C. (2023). Anwendungsorientierte Wirtschaftsinformatik: Strategische Planung, Entwicklung und Nutzung von Informationssystemen (10. Auflage). Springer Vieweg. https://doi.org/10.1007/978-3-658-40352-2<br>

###### Fachzeitschriften
Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.: Latent dirichlet allocation. In: The Journal of Machine Learning Research. Nr. 3, 3. Januar 2003, S. 993â€“1022<br>
<br>Blum et al., 2020 ?<br>

###### Websites
Freitas, G. & Lily Hulatt. (2025). Feature Selection: Methoden & Techniken [Bildungsplattform]. StudySmarter. https://www.studysmarter.de/schule/informatik/computerlinguistik-theorie/feature-selection/<br>
<br>Helm, C. (2025, Mai 8). spaCy vs NLTK â€“ Was ist die bessere Wahl fÃ¼r NLP? Konfuzio. https://konfuzio.com/de/spacy-vs-nltk/<br>
<br>Bonart, M., & FÃ¶rstner, K. (o.Â J.). Python Pakete und Bibliothekten: Data librarianâ€”Modul 3â€”Daten analysieren und darstellen. Data Librarian. Abgerufen 11. Oktober 2025, von https://bonartm.github.io/data-librarian/organisation/packages/

###### Anleitungen/Tutorials
`gensim` Å˜ehÅ¯Å™ek, R. (2024, August 10). LDA Model. Gensim. https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html<br>
<br>`gensim` Å˜ehÅ¯Å™ek, R. (2025). Gensim: Topic modelling for humans. Gemsim. https://radimrehurek.com/gensim/<br>
<br>`gensim` Å˜ehÅ¯Å™ek, R. (2024, August 10). LDA Model. Gensim. https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html<br>
<br>Sanchhaya Education Private Ltd. (2025, September 3). NLP Gensim Tutorialâ€”Complete Guide For Beginners [Bildungsplattform]. GeeksforGeeks. https://www.geeksforgeeks.org/nlp/nlp-gensim-tutorial-complete-guide-for-beginners/<br>
<br>Sanchhaya Education Private Ltd. (2025, Juli 23). Normalizing Textual Data with Python [Bildungsplattform]. GeeksforGeeks. https://www.geeksforgeeks.org/python/normalizing-textual-data-with-python/<br>






Formatierung
ğŸŸ¥ğŸŸ¨ğŸŸ¦ğŸŸ«â¬œğŸŸ§ğŸŸ©ğŸŸªâ—¼ï¸â—»ï¸ğŸ”¶ğŸ”¸ğŸ”˜
:red_square:
<br>GitHub - https://docs.github.com/de/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax<br>

ğŸŸ£ğŸŸ¢ğŸŸ ğŸ”´ğŸ”µğŸŸ¡ğŸŸ¤âš«


[^01]: [Datensatz01] (https://www.kaggle.com/datasets/ashwinik/consumer-complaints-financial-products)
[^02]: [Datensatz02] (https://www.kaggle.com/datasets/selener/consumer-complaint-database)
[^03]: [Datensatz03] (https://www.kaggle.com/code/saurabhsawhney/nlp-complaints-classification)
[^04]: [Datensatz04] (https://www.kaggle.com/datasets/shashwatwork/consume-complaints-dataset-fo-nlp)
[^05]: [Datensatz05] (https://github.com/sanax-997/nlp_customer_complaints_analysis/blob/main/Data/complaints_data.csv)
[^06]: [Datensatz07] (https://github.com/gurneetjuneja/NLP-Problem-Solving/blob/main/user_complaints.csv)
[^07]: [Datensatz08] (https://www.kaggle.com/code/mchirico/analyzing-text-in-consumer-complaints)
[^08]: [Datensatz09] (https://data.mendeley.com/datasets/w2cp7h53s5/1)
[^09]: [Datensatz10] (https://github.com/Schossi2908/DLBDSEDA02_D)
[^10]: [Datensatz11] (https://www.kaggle.com/datasets/tobiasbueck/multilingual-customer-support-tickets)
[^11]: [Webseite] (Helm, C. (2025, Mai 8). spaCy vs NLTK â€“ Was ist die bessere Wahl fÃ¼r NLP? Konfuzio. https://konfuzio.com/de/spacy-vs-nltk/)
[^16]: [Webseite] (Timmermann, T. (2019, MÃ¤rz 7). Natural Language Processingâ€”Einsteigen und loslegen! [Unternehmenswebseite]. codecentric AG. https://www.codecentric.de/wissens-hub/blog/natural-language-processing-basics)

[^12]: [Buch] Klein, B. (2023). Numerisches Python: Arbeiten mit NumPy, Matplotlib und Pandas (2., aktualisierte u. erweiterte Auflage). Hanser.
[^13]: `seaborn` Waskom, M. (2021). seaborn: Statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.03021<br>
[^14]: Elson, P., Andrade, E. S. de, Lucas, G., May, R., Hattersley, R., Campbell, E., Comer, R., Dawson, A., Little, B., Raynaud, S., scmc72, Snow, A. D., lgolston, Blay, B., Killick, P., lbdreyer, Peglar, P., Wilson, N., Andrew, â€¦ Kirkham, D. (2024). SciTools/cartopy: REL: v0.24.1 (Version v0.24.1) [Software]. Zenodo. https://doi.org/10.5281/ZENODO.1182735
[^15]: Bonart, M., & FÃ¶rstner, K. (o.Â J.). Python Pakete und Bibliothekten: Data librarianâ€”Modul 3â€”Daten analysieren und darstellen. Data Librarian. Abgerufen 11. Oktober 2025, von https://bonartm.github.io/data-librarian/organisation/packages/

